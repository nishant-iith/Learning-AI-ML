# Advance NLP Series-Bidirectional LSTM Intuition And Implementation Deep Learning

hello guys i hope everybody is able to hear me out can i get a quick yes if you're able to hear me out i'll just start the session in some time but i hope everybody's doing fine what's up what's all how's everything going on after a long time uh we are back again live with some amazing community sessions like nlp and all so we will complete the nlp advanced nlp series which had been pending from past many many days right so i hope everybody is able to hear me out yeah can i get a quick yes yeah okay perfect hit like if you're new please make sure that you subscribe the channel uh press the bell notification icon uh please give me a confirmation once you press the bell notification icon because this live session will happen from 8 pm ist in india and you know i don't want you all to miss any such opportunities of learning with me okay okay perfect so another one minute and then we will probably start today uh in this session we are going to discuss about bi-directional lstm okay so the nlp advanced nlp series we're going to just complete it okay so bi-directional okay perfect i'm just writing you know so hello hello guys hello everyone i've recently brought eye neuron course well it worth it yes obviously it will worth it um the tech neuron is available now for lifetime so please make sure that you realize that particular opportunity and go ahead with it you know so it is just going to be for lifetime any life classes that are going to come up in the further time um yeah materials are always available now let me just share my screen so that we can start the session i hope everybody is able to see the screen also right so yes uh so if you remember guys what all things we have completed uh with respect to the previous session anybody whoever has been attending my previous classes with respect to nlp right we completed rnn we completed lstmrn right we completed from machine learning uh like bag of words then we also completed tf idf uh we completed so tech neuron that i purchased is two-year validation mantras and just contact the support team they'll help you out with that okay we also completed something like what to wack or we completed average word to work in what to work we discussed both skip gram and cd cbo ow lstm also completed okay now today uh we'll start what are the pending topics that we really need to complete i'm just going to write down okay so today we are going to finish up bidirectional lstm rnn with practicals okay so here we are also going to do practicals okay then uh probably the next topic that we are going to see after this is something called as encoder decoder this will basically happen in the next class okay we'll also do a project with respect to language translation then we'll go with transformers transformers bert and gpt we'll understand this all architecture in depth architecture and then finally we will do some practical applications on all these things okay and if you are probably good with all these things i think they'll know there will not be anyone who can stop you from getting jobs okay so we will be completing all these things as we go ahead okay perfect uh so shall i start the session everyone with respect to bi-directional lstm rnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn says with the help of you krish i got data scientist in tata alexey congratulations yeah john when best platform to practice my sequel you don't require any platform you basically needs to practice through say in i neuron also there is a mysql course very nice my sequel course everything is available okay so shall we go ahead and start guys yeah so this will probably take two weeks time to complete everything so we are going to take two week timeline uh with respect to making you understand everything okay so two weeks timeline yeah yeah accessibility you'll also be able to see from mobile handset and download the materials everything you can basically do so let's start guys uh yeah whoever has bought in the earlier also we'll get lifetime so you can just go ahead and ask it okay uh so hit like guys let's start and uh we will be starting with bi-directional lstm rna okay so let's go ahead okay bi-directional lstm rnn and i hope everybody knows about lstmrn we have already done the practical implementation okay so to begin with uh first of all i will first of all what i will do is that with respect to bi-directional lstm rnn first we will try to create a simple rnn okay so so we will try to create a very simple rnn first of all so let's go ahead okay how does the rnn look like you will be seeing this kind of boxes okay so everybody remembers how the rnn architecture looks like yeah it looks something like this right with respect to time stamps we will be giving our inputs right right so over here let's say i'm doing a task predicting the next word okay predicting the next word so the task is very simple over here i have a sentence which says like chris likes to eat dash in bangalore okay something like this some statements are there okay and probably whatever output that i have over here this is basically my training data set whatever output i'll be having that will be my output value okay now let's consider in this specific way so i really want to train this entire sentences with respect to this and i want to do the task of predicting the next word okay now first of all most like with respect to any uh architecture of rnn you know that if i have this kind of values then i will probably have words like x11 x12 okay so let's say i have some words over here okay so let's say i have some words like x11 two x one three x one four x one five like this i have a lot of words okay x one six now let's say that i'm giving my first word over here so this will be my x one word okay now this x one one word what it'll do at time is equal to one we are going to send this particular word right and as you know let's say this lstm over here uh rnn will take x0 initially and this will be the x1 which we will basically supply and then from here we will be forward this over here but in the next time is equal to 2 we will be having x12 right x12 is my word that is going to come again i'm going to have over here y 1 1 y 1 2 and over here i will be having y 1 3 right and then probably this is my input that will go over here then let's set t is equal to 3 what will happen at t is equal to 3 we will be sending our next input so here i will basically be having x 1 3 at t is equal to 3 and here we are just going to supply x14 at t is equal to four right everybody remembers this simple architecture with respect to timestamp yeah with respect to timestamp everybody remembers this yeah very simple right so what is this this is nothing but a normal rnn right so here i will be having my neurons right here neurons will be there your neurons will be there here neurons will be there okay everybody clear with this simple diagram what i have actually done yeah and whatever x 1 2 x 1 3 x 1 4 this we are giving in the form of embedding layers what is embedding layer if you remember embedding layer embedding layer make sure that whatever text we are giving it converts this into vectors in short this is a many to many rnn okay you can also say this as many too many rna yeah perfect many too many rnn and here whatever values you are giving x 1 2 x 1 x 1 1 x 1 2 x 1 3 x 1 4 whatever words you are given you are giving in the form of vectors ok now understand what is the difference between bidirectional and normal lstm rna okay that is what we are going to discuss now let us say the word that i really want to predict is this specific word okay now tell me over here in this kind of sentences like krish likes to eat something it can be apple chocolate it can be uh anything right it can be any food items over here right now in order to determine this obviously you know y 1 3 when we are trying to find out the output you know that y 1 3 will have the information of x 1 3 right it will have this specific information it will make sure that it will also have information of x 1 2 and it will also have the information of x 1 1 do you agree with this everyone yes do you agree with me that it will have the information of all these particular words why because with respect to y 1 3 you know that from this at every time stamp this words are getting added so tell me one thing whether they will be knowing this specific information or not whether y one three will know the output will have some context with respect to these three words it will definitely have they will y 1 3 will have the context of all these previous words that are there but always remember if i really want to do this kind of predictions this prediction needs to also be dependent on the forward words right all the operations are same like how rnn over here we will be assigning some different weights here we'll be assigning some different weights let's say over here i will be assigning w1 w1 w1 right over here this will be my o1 output this will be my o2 output this will be my o3 output here also i will be having different uh weights like w dash w dash w dash right everything will be almost same like how we discussed in the rna but and understand that for this specific word it will have obviously have the context of the previous word like x 1 3 x 1 to x 1 1 but this will not have the context of the forward words right it will not have the context of the forwards but if it has the context of the forward words then probably this blank that you see right it will be filled with what is famous in bangalore like i i like to see that krish likes to eat sharma in bangalore because in bangalore probably you may get a good shawarma something like that right so based on this forward words right this words can also change because instead of bangalore if i probably say some other city in up or lucknow right in lucknow what is so best tunde kebab have you heard of tunde kabab tunde kebab right this is the most amazing food item over there right so this specific word may change based on this for the words that that is going to come in the sentence okay so what i want is that whenever we are doing this prediction this should also have the further words context i hope everybody is able to understand guys give me a quick yes if you are able to understand or hit like do something you know tell something whether you are able to understand this thing or not yeah everybody is able to understand this thing what i'm actually saying trying to say the blank space whatever word we are trying to predict should have the context of the forward words but in this particular case i don't have the context this will not y 1 3 will not know that what is x 1 4 because here we are basically using a simple rnn i'm just considering this as a simple rnn let's say i don't know what will be the further words obviously over here x14 i will definitely not know what is the context of this with respect to y 1 3 it will not have any idea now what else we can do is that we can make sure that we can provide the context to y 1 3 about the further words and how we can basically do that now what i'll do is that i will introduce you to something called as bi-directional lhtm rnn okay let me just make my screen size a little bit smaller so that you'll be able to see this okay now here i'm just made my screen smaller now here what i will do is that in order to make the context reach y one three okay here i will create an same neural network let us say in this color let us say in this color i will connect i i'll create the same neural network like this one more neural network i will connect it to something like this one more neural network i'll connect it to like this one more neural network i'll connect it over here okay and why i am connecting this because i want all this information to go in the reverse manner and make sure that y1 3 also will come to know about this right so what i'll do now here this x14 will get connected to this okay will get connected to this whatever output that you are going to give over here this will get connected to this specific output okay similarly what will happen is that this layer to this layer will get connected right then whatever output again is coming from here it will get connected to this to this similarly from here to here right here to here this will get connected whatever output you are going to come up with this this will get connected to this and from here to here it will go over here and whatever output is coming over here it will get connected to this is this making sense everyone so this is the idea about bi-directional neural network now see by this connection what is basically happening the context of x 1 4 because why because x 1 4 is basically getting passed over here and i will also add to some lines over here with respect to this also so this will also go as an input this will also go as an input and this will also go as an input does this make sense everyone are you able to understand this part or not everyone say this as both emphasis combined it is a bi-directional stream rnn okay so along with the input data that you basically have it is also going to make sure that you you will be passing let's say from the reverse order all these words to this neural network so when we have y one three now it will have the context information of the further words and it will also have the context information of the previous words and now if we train this model in proper way you will be able to probably accurate predict the accurate result over here simple so in short if i say in short i can say if i have this see and now i will draw it in a very shorter manner okay bi-directional htmlrn looks something like this okay the size will be almost same okay so here we are passing these words this will also go over here okay uh with respect to this specific word then the second word will get passed over here this will also go over here third world will get passed away this will also go over here each and every one will be having its output this output will be combined to this this output will be combined to this this output will be combined to this right and this all neural network will be combined to each other that's it how do we decide that can be a hyper parameter tuning right and these are all my neurons that's it so simple it is now we are you you can use lstmrn you can use rnn any one right but at the end of the day what you are doing is that in the forward direction also you are passing all the words in the backward direction also you are passing all the words so with respect to every output here it is getting joined right with respect to every output here you will be able to see that it will also have the context of the previous word in context of the forward words and that how cool this bi-directional html rna is and over here in the output layer you'll be seeing that we can either use sigmoid or soft max based on the classification problem right if it is a classification problem you can use either sigmoid or soft max here also you can use the same thing here also you can use in case of regression you can use something else right so this is all together a bi-directional lstm rnn how many of you were able to understand this because these all are important guys see if i'm able to make you understand this then you'll also be able to understand encoder decoder right because the idea of encoder decoder is somewhat related to this you should know how things are basically connected to each other yeah so everybody clear with respect to understanding this yes shall we do a practical application yeah everyone shall we do a practical application quickly shall we do a practical application guys yes tell me yes or no so i will just open yeah everything will make sense now uh i'll show you an example of how to pass a sentence and all that everything okay so um there is something called as fake news data set toggle now everything will make sense since that is the reason why i am doing this practical for you all will take up this data set and we'll try to just try to implement this uh what is the data so data you can download it over here so here you have your trained data so download the strain data if you want okay so this is your trained data that is basically coming open google collab everyone just go and search for this fake news uh like kaggle data set okay fake news kaggle data set and uh download it i'll just go and open my new file and also make sure that i'll share you this link so that you can also see the track of everything that i do okay okay so let this and here i'll just change the runtime to gpu so that i can use the gpu and i'll connect this okay and now uh i will just open this so whatever things you are doing please make sure that you download the data set and upload the data set over here so that you can basically work okay so here now you can see guys here i have extracted my data set folder now all i'll do is that i will try to upload this data set over here okay so this is 96 kb i know how much time it will take but let's see it'll get uploaded yeah so now this data set will get uploaded over here and we will start the work with respect to the specific data set and if you want this i will share this link with you so that you can see the track of it okay copy link and i will give you in the link in the chat okay so i think everybody has got the link just open that and i will start now here you can see i have the train data set now let's start the coding part as usual uh what what problem statement we are trying to solve we are trying to solve fake news classified data set right so we are solving a pick news classifier using bi directional lstm right at the end of the day the information that you will be passing will be in the form of vectors okay so data set i have basically taken it from here so i'm also going to give you the link data set okay and uh as usual we'll start with import pandas as pd uh dfpd.read underscore csv and this will basically be my train.csv okay so i'm getting an error not a problem let's see why we are getting an error on error tokenizing end of string at line two one eight zero let's see are we getting some problem with respect to this so everybody uh i hope uh just to open this great data set and let's see i'm just opening this data set let's see how it looks like let's do one thing uh whenever we get this kind of errors there is a parameter which is called as parses let's see how to solve this okay whenever you get this kind of things guys please make sure that you watch this how to fix this issue so here you can see why you're getting this issue you can use this engine engine is equal to python okay let's see whether this will work or not unexpected end of data okay we are getting some errors we change this to c once and of string starting at so guys anybody who is able to read this data set i'll use one more error to it let's see something will be able to get it to use coding encoding yeah there you go we're getting this error from this big news classifier data set and we're starting at 547 let's see ignore errors reading data set from pandas we give an error underscore bad lines is equal to false guys are you able to read this data set anyone perfect finally we can see this tf.head i was able to get something okay so this is how the data set looks like okay uh there was one error somewhere so i've just used error underscore bad underscore line just remove that wherever we are trying to get an error okay just to make my work a little bit easier uh probably i'll just click this now here you can see this entire data set oh my god this looks this looks way way way bad so you can see this entire data set over here let me just execute it again so this is how your data set looks like everybody is able to see the data set now what we are going to do let's take this title column and let's try to do this sentiment analysis prediction with the help of bi-directional lst okay now first of all we'll go and see whether there is any null values so here you can see that we have some null values like 195 674 now with respect to label we don't have some some values of features are basically null but if you probably just write df dot shape uh i think we will be able to get 7000 records so there are 7000 records uh i probably feel like if you try to remove the null values also it won't impact that much so what i'm actually going to do i'm just going to say drop an a and with respect to text right you can't do it much you know so you probably have to drop it okay so df.drop any i have specifically done the drop statement and then if i probably go and see the head here you will be able to see the head okay now out of this i don't require id column or this label column is basically my output feature and i'll take this as my independent feature so uh let's go ahead and write get the independent features okay so here let me write x is equal to df.drop label label is my feature and this is my output feature and i'll write x is equal to 1 okay so here it is uh then i can also write df dot df of label right so this becomes my dependent feature right so here is my x and here is my y okay now let's see whether uh data set is balanced imbalanced or not check whether data set is balanced or not okay so here i can basically write y dot value underscore counts and here you can see that whatever values that i can basically see okay so here it is here you can see that okay it is not completely balanced because we have good number of data with respect to positive and negative values right uh so what we're going to do now is that let's go ahead and import tensorflow and before that let's see whether we have tensorflow or not tensorflow as tf and before that also you know because still there are so many things that we really need to do but i really want to import all the libraries in tensorflow okay so if i probably cite tf dot underscore underscore version by default now google collab has this so here you'll be able to see 2.2.2.8.2 right now i'm going to import some important libraries of tensorflow so here i'll save tensorflow.keras dot layers i'm going to import embedding i'm going to import embedding from tensorflow dot keras dot pre-processing from tensorflow dot keras dot pre-processing dot sequence i'm going to import padding sequence this is just for the pad sequence which we have also discussed in our previous tutorials right and then from tensorflow dot keras dot models import sequential and then from tensorflow dot keras dot p processing dot text import one hot because before applying to one uh you know before before basically applying to uh embedding or before giving the data to embedding layer we also need to do one not encoding and then from tensorflow dot keras dot layers i'm going to import lstm lstm is definitely required over here lstm okay and then from tensorflow dot keras dot layers i'm also going to import dense dense for creating the neurons and from tensor flow dot keros dot layers import bi-directional bi-directional is required because we are creating bi-directional lstm so all these things we are going to basically use it so all these libraries you are going to use it over here and uh what we are also going to do over here is that uh first of all we'll we'll fix some vocabulary size okay so it is always a good habit that we try to fix a good vocabulary size so that we can play with it so let's go ahead and write vocabulary size right so here this is my vocabulary size let's say i'm taking 5000 words in my dictionary okay 5000 words in my dictionary so that i can actually create my indexes in short right so let me first of all make i need to do some processing before i send okay so i will probably write messages dot x dot copy so this x is my entire data set right so i'm just making a copy of this over here and storing it in messages so if you probably see messages these are my entire data frame with respect to all the input features okay so so here is my input feature now uh the first thing that i am actually going to do over here is that uh this id column is also not required uh you know what we'll do we have to play with this title column or you can play with a text column but here i'm going to basically play with title column okay now in order to play with title column the first thing that i am going to do is that just import uh nltk let's see whether nltk is installed or not i don't know whether it is installed or not okay it is perfectly installed i think by default it is going to give this and here i am also going to install re regular expression and from nltk dot corpus i'm just going to import stop words also stop words uh the reason i'll be using stopwatch to remove unnecessary word over there and here i can basically write an ltk dot download i will try to download all the stop words also so that i can play with it an ltk and ltk okay so this will get downloaded so here you have the complete downloading process now i'm just going to copy some important code over here and here you can see that what exactly i am doing i am using porter stemmer this porter stemmer is for the stemming purpose and why i am using stemming because this is a kind of problem statement where you focus more on implementing uh you know implementing sentiment analysis so i will probably be using this so here you can have see that what i'm doing i'm using portrait stemmer portastema for i in range of zero comma length of messages i am first of all removing all the special characters other than small a to z and capital a to z on every title that i have i am lowering it splitting it and then over here i'm applying it to the stop word and then stemming it and finally i'll be able to get my entire data in corpus so here if i execute it um what is the error with respect to key so let's see this what is the error message start head if i probably see the 10 okay i have removed some of the data right so let me do one thing here i'm just going to write messages dot i'm just going to make one more code scale because i have to remove the nand values right so here what i'm actually going to do is that i'm just going to say messages dot reset index reset underscore index okay no it is not a syntax issue we are just not getting this key value okay so if i probably reset index and say in place underscore true now you will not find any gap okay and place is equal to true so here i'm just going to execute it now i think it will start working no need to print i over here otherwise it will take let me just print i so that you'll be able to understand how much time it is taking so here everything is happening and slowly for all every every column you'll be able to see this process happening entirely for all the title message okay so we'll wait for some time till this entire thing happens and here you can see that it is getting updated 2075 yes perfectly it costs executed now if i probably go and see my corpus here are all the corpus after applying stemming and all okay um can we apply multi-thread of process yes you can also apply that but you really need to follow a different approach now this is my corpus this becomes my input feature i have my output feature as you already know okay so this is my entire corpus okay now the next thing that i'm going to do with this specific corpus is that uh you know i am just going to apply my technique of uh seeing after seeing this corpus i just need to apply my embedding and before i'm by applying the embedding also right you have to also make sure that you apply one hot encodings on that before applying embedding right so for doing that uh what i'm actually going to do is that or obviously you know how to do it uh for one hot encoding one hot encoding is basically done from one hot or the library that you have imported uh now let us go ahead and try to basically use that one hot on this specific corpus okay so let's quickly go ahead and write our code so here i'm basically going to say one hot underscore representation and remember this one hot representation will be based on 500 vocabulary size okay so here i'm just going to write one underscore hot and here i will just give my words and my vocabulary size vocabulary size and i will i have to provide this words right so i'll write for words and corpus so i'm just going to go ahead with each and every words in corpus and probably we can print the one hot representation this one hot representation it will give us all the indexes okay so here you will be able to see all the indexes with respect to every sentences so this is my first sentence these are the indexes that i am getting these are this is my second sentence this is my third sentence so all the sprint statement you will be able to see if i probably don't print this like this then you will be able to see in the longer manners now here you can see for every sentences you have this is what you get this is the vocab based on the vocabulary size this is the indexes that you are actually getting it okay so here is what you are able to see the entire data with respect to the same corpus data now after converting this into indexes the next step uh that we really want to specifically focus on is how we can pass this to an embedding layer and then bi-directional lstm layer to basically do the prediction okay so that will be our next step uh because this is the step that we usually follow with respect to first we do one hot encoding then now all we do is that we try to apply we try to apply an embedding representation embedding representation because at the end of the day i have to basically convert these words into vectors so here first of all i will take the maximum sentence length probably over here i have already verified it the maximum sentence length that can occur is 20. so i'm going to basically put up 20 and then now i'm going to apply the embedding layer now before applying the embedding layer i need to also use the padding sequence which you probably saw right the padding sequence should need to be applied just to make sure that all the sentences are of this particular sentence length that is 20 so for that i will use pad underscore sequences which i had already imported uh on the top okay so just a second okay here i'll write sentence on the scroll line 20 and then here i'll write pad underscore sequence sequences okay and here i need to provide all my one hot representation so here you have one hot representation comma the next thing is that whether you want what kind of padding whether you want pre-padding or post padding so in this case i will say pre-padding you can also use post padding i don't think so it will make much difference the max length that i am going to basically do is my sentence length okay and this entire thing i am going to basically show it in my embedded docs in a new variable okay and here is what i'm going to do i'm just going to show my embedded underscore docs now here you will be able to see this is how my padding will basically occur and this is my entire one hot representation now let's go ahead and create the model now to create the model i have used a very simple process okay so here it is what it is creating a model first of all with respect to embedding right now here i have got this all documents after pre-padding right now this entire indexes will get converted into vectors vectors of the size of 40 why i have taken 40 because sentence length was 20 i thought 40 would make more sense and we will be able to get good accuracy over here okay so here i have made it as 40 then we used sequential layer then here i've used embedding layer with the same vocabulary size embedding vector features from where i'm getting embedding vector features uh over here so embedding vector feature is nothing but 40 input length sentence length should be the same like 20 here which i have actually initialized and then i've used an lstm layer over here and then here you can see dense layer okay but still you may be thinking krish where is bi-directional then guys like this how we did it for lstm right for bi-directional all we have to do is that just add here bi-directional on top of it so once you add like this bi-directional and probably close this brackets then this will basically become a bi-directional lstm right on top of the lhtm i've just noted down this as bi-directional that basically means 100 neurons forward 100 neurons backward you know with respect to that lhtm architecture that we have in mind and since uh this is a binary classifier i've used the activation function as sigmoid then the loss function i have used binary cross entropy optimizer adam matrix accuracy and here you can basically see the model summary so i'll just go and execute this after executing it you'll be able to see the output okay yeah so this is how many number of parameters we have created with respect to weights and bias uh i've used embedding layer one bi-directional lstm layer and dense layer and finally i have this dense layer as my output okay guys is it making sense everyone max sentence length is 20 then we can have 100 lst now you have just given a number as 100 you can try with multiple different numbers also okay so till here i think we are we have done it in a better way now all i'll do is that let's do one thing train test split also i need to do first of all i'll convert this entire embedded docs that i basically have right over here into an array so that i can supply it now i will draw a train test split so trained test split okay for train test split obviously you know how to do it train test split it's very simple we'll just use the strain transplant of a scale on and just execute it with x final y final i'll get my x 2 and y test and this one now finally i will go ahead and do my model training okay and then i will write model dot fit okay on my x string comma y train comma i can also give my validation data my validation underscore data is equal to uh that is my x test comma y test right these are my validation data number of epochs that i'm just going to run for just checking it out is 10 and my best size i'm just going to keep it as 32. i think in that much ram i have okay so let's go ahead and execute it hope so everything works fine epoch 1 has started i don't know how much time it is going to take but let's see it is going to take definitely some amount of time because i have also taken 10 epochs back size is 32. here it is so we have started the training over here perfectly all right training is going on well the validation loss uh validation accuracy this accuracy this loss is also getting decreased you know so here you can see good amount of information that we are able to see right yeah so just with 10 epochs i'm able to get training accuracy this much validation accuracy this much okay which is good you can still uh train it for any number of epochs that you basically want now what i will do is that i will probably use the confusion matrix and do the prediction so here i'm just going to write y prime is equal to model dot predict on my extras data okay and probably this will be my wipe right let's see so if i probably see why pred i think it will get in form of probability so what you can do is that you can basically use another classes which is like underscore classes to get what is the exact output okay predict underscore classes is not there let's see model dot predict okay predict i think predict underscore classes has been removed now here all i can say that greater than or equal to 0.5 okay just execute this then you will be getting true or false and probably wherever it is true greater than equal to this uh wherever it is true we will try to or let me just write with numpy okay import numpy as numpy as np and here i will say np dot where this is this i'm going to replace it one otherwise zero so here we can use np dot numpy yeah perfect so this i will save it in my y and scope right okay i've just kept the threshold as point five okay then from sk learn dot metrics import confusion matrix okay and then i can probably use this confusion underscore matrix with y test comma y prime should be metrics okay so yes this much accuracy at least we are able to get with respect to the right prediction we can also use accuracy score okay and here you'll also be able to see the accuracy score okay now probably you can print this accuracy underscore score with y underscore test comma windows corporate so this is how much i am able to get somewhere on 90 accuracy with respect to the training data set so this was how we basically implemented a bi-directional lstm rnn you can do with other examples also at the end of the day you will be creating a bi-directional listim rnn to do proper predictions okay model will not have any data leakage problem because i have made sure that the train test split is done in better way okay guys so how was the session everyone was that worth did you were able to understand were you able to understand all these things whatever was discussed we covered both hairy part so here you could see some theory i'm using theory over here in short what we did is that we tried to use both rnn from the forward layer and from the backward layer right over here two sets and how it knows that it has this information over here right so this kind of neuron has only got created now we are actually trying to find out the output after taking the entire sentences but if we also try to do it i think it will make sense right so in short this is how things work and probably in the future uh we're also going to do uh language translation we're going to do encoder decoder and then we are going to understand all these things right so i hope everybody got the materials i hope everybody was able to try it out and probably check it out now the next session we will probably have on friday okay on friday we will start with encoders decoders so i hope uh you like this one quick announcement guys uh just go ahead and visit i neuron okay we are we have launched this tech neuron one neuron platform this will be available for lifetime at ten thousand rupees here you have 200 plus courses and if you feel 10 000 is also costly you can use my code that is krish 20 okay so if you are probably buying this with the help of my code chris 20 you will be able to get 20 off okay so but this has amazing things because all the courses we have included over here even uh the live courses that has are going to come in the future will be included over here so you'll be able to attend any live closes over here also okay so and this is lifetime is only for this uh more four days are left till sunday so please utilize this opportunity all the information regarding this is available in the description of this particular video yeah uh if you also don't want to buy it uh you tell it to your friends whoever require it because not everybody cannot afford a very high cost fees and we are always working towards affordability right so yes support us to help us in this noble course and i think uh this session was good all the links everything will be provided with respect to this specific session and uh yeah advanced you know already i've given the course link niche down in the community session or communication link so over there only we are going to update all these things along with the materials okay so thank you guys this was it uh anything you have to talk please do let me know if you have any type of queries you can let me know any concerns you can let me know and also make sure that you subscribe krishna hindi channel please i have i'm also going to start krishna in krishnak in the channel some interview playlist that i'm going to start i'm going to discuss every day two to three couple of questions with respect to interviews so please make sure that you join that or subscribe the channel okay yeah and anything that you have other than that i can definitely help you out if you have any questions okay uh when are you starting projects which projects so project nlp projects end to end that will start once we complete all these things right okay i'll also give you an idea with respect to real world projects how we can basically do it and guys if you really want to support me please make sure that provide some donations join membership support me and somehow you know it becomes difficult you know to devote much time so i really want to want some support and obviously can't earn much money from youtube also you know specifically tech channel like hours you know teaching and none of the people are interested in teaching you know today also hardly 87 to 100 people have joined you know so this is really bad can't help but definitely make sure that try to support the channel by joining membership uh making sure that i provide you the best quality thing you know so that is what we are trying to do okay that is only the thing that i can request other than nothing as such but i will keep on uploading videos please make sure that you subscribe krishnak uh hindi channel also and there i'm going to basically start post this one interview preparation batch very soon um i think from friday that is going to come up okay let's make a video on college droppers someday sure i will do that and kate solo says thank you because you're not consistent last class there were three thousand viewers no last class every time there are a hundred years only i took three months live session classes and people are not at all interested they told uh they'll come at eight pm but still people are not there multi-processing for a loop can you get use uh das das library is there try to use that it will do the parallel purposing for for loops also okay tk other says namaskar sir with your communities and god blessing i got offered from cognizant 18.70 lakhs per annum please suggest me how much is optimized it is your five years from deloitte i think that is fine for five years 18.70 is amazing right i think you should go ahead with it is more than three lakhs three times you know so that is absolutely good congratulations dk other this is amazing perfect this is this is quite amazing guys i i like people see people say trust sometime you know you see always understand uh you all need to understand that whenever uh these things that what we are going to do right this tech neuron this entire technical things has a lot of things over here you know you have a lot of opportunities with respect to jobs with respect to multiple things as such right so this is super fun you should just devote time there's so many openings but people are after things where there are very less number of openings and a lot of competitions yeah thank you sir this is my second transition from data engineer awesome awesome okay any more question guys that you have please do let me know i'm working as a data analyst for blue dot and your videos are really helpful amazing congratulations guys congratulations thank you sometimes this this success story is right this looks amazingly well i feel really happy this gives us motivation guys uh is this time fine uh 8 pm to 9 pm i guess with respect to think please correct the pin message okay it's fine not a problem and make sure that you join the telegram channel uh i know code but in front of the interview i'm completely blank so you have you have to talk you know you have to talk right unless and you don't talk it'll not work out okay donation was for the love i got from your streams thank you nihal thank you thank you dk because sometimes right donations is required you know like do you know how many how many videos i have uploaded in my channel around 1500 just imagine i still think that how is that possible really in just two and a half years it's very difficult really really very difficult i'm getting a problem on solving end-to-end advance in lp don't worry i will take couple of problem statements and projects on that sir please review cdx dbda it's a government post graduation course uh i'm not sure about this okay bala more only says that telegram link is not working no it should work telegram link should work try to yeah i can see that it is working it's working fine i watched your old videos every day it's really awesome thank you thank you thank you thank you guys i'm interviewing the jungle company for applied scientist role can give you some tips it's based on an nlp playlist make sure that your basics are strong that is that is super important your basics are strong you don't have to go and talk about bert architecture and all but understand what nlp things you can basically do that is very important okay that is super important so unless and until you don't do that uh you will not be able to you have to be confident you you should know how to talk i've seen people who don't have much knowledge in nlp then also they crack interview jobs because the basics are strong right okay i wonder why the upsc video is still there playlist and that is how i started my youtube career in short i uploaded a video at that point of time just to see that how good i am is there a separate nlp master course in eye neuron yes in the fhds only we have included all everything so guys fhds batch is also getting launched from september 17th uh fsds batch so you can also join that if you want job guaranteed program uh everything is there but at the end of the day telegram links you see in the chat pinned comment can we have a session for implementing or something like jarvis sure i can do that see something that is already there why you want to again implement it what we choose between strong basics or all things known you see there is a point till that specific point you need to know right if you don't say some advanced things also no it is fine but basics if you don't say no that will hurt the interviewer just qualified as a data scientist struggling to secure a job work on your resume work on your projects work on your talking skills that's it big love foreign content thank you so much thank you for the amazing series send super chat and then us probably he'll do it don't come for free stuff no no it's okay if whatever is there i will try to get should i give uh tensorflow developer certificate exam is it worth it if you feel it is worth it just go ahead and write it you know can you please list out some resources for data analyst opening just go and check uh i neuron job portal i think there are a lot of data analyst openings that are currently going on content on net about targeted sentiment analysis is very less or will you be taking niche area some time yeah sure i'll do that okay now right now more complex projects is going to come when you understand birds and transformers because we are going to do the end-to-end application with hugging face library it will be quite amazing chris it says there is a problem in purchasing it should not happen try to contact the support team yeah i checked your pin comment but there is no telegram link no telegram link is there no https pinned comment in this chat uh as fresher where should i apply for data engineering position there are many companies first of all don't apply for a full-time job and instead apply for internship internship will be available more when compared to full-time jobs okay this bow be power bi certification of microsoft word yes this kind of tools i think it will be of some worth because at the end of the day if you say i am a microsoft certification of the product that own microsoft has created then it looks good uh sir please create an interview series again it helps a lot for beginners or two years experience sure ratio okay so yes guys uh this was it from my side i hope you liked this session uh keep on rocking keep on learning uh please keep on supporting and please make sure that you press the bell notification icon uh next session we will basically be having on friday where we'll discuss about encoders and decoders and we'll also be doing a practical implementation okay so yes this was it from my side i will see you all in the next session uh yeah you can contact me through linkedin you can join the telegram link that i've given in the chat uh yeah this was it for my site bye bye take care rock everybody rock bye good night guys yep thank