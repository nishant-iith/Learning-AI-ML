# ackPropogration In Recurrent Neural Network And NLP Application

hello guys i hope everybody is doing well i'm fine and today we are basically going to just you know just continue the nlp session and today is the seventh day okay so we'll just start in some time till then we'll wait for everybody to join okay thank you okay i'm good i'm good we'll just wait for some time so that everybody joins just let me know whether you can hear me or not quickly i hope everybody is able to hear me out so i hope everybody is doing fine i get a quick yes if okay perfect perfect perfect so just let us wait for another couple of minutes till then i hope everybody is doing fine uh to begin with one amazing announcements everyone like uh now uh you know all the eye neuron tech or live classes will also be available in the tech neuron part okay so this is one amazing announcement that we are going to have this is quite amazing because like we got many people request saying that krish can you bring all the live classes in tech neuron we thought of it and finally we are there you know so everybody will be able to find even that too yeah so so let's start in some time till then how are you all i hope everybody is doing fine yeah i hope everybody is doing fine hi chris can it be possible to get the problem statement of hackathon of mla sure i will do that and i'll share all those things probably in the next class okay do you have fhds no job guaranteed program will not be there apart from that all the other courses will be there okay okay are we going to do chat bots in community session sure we can do that i will be showing you rasa and lu and we can basically see this where can i find all the notes just go in the description you'll be able to see all the community dashboard link and uh you know you'll be able to understand from there designer on skype id is deactivated now every support is basically happening from the website itself uh so for in the website itself when you log in you'll be able to see one chat bot in the left hand side so from there all the support will happen hikers can we become analytics manager by learning yes obviously okay yeah don't worry we'll learn about rasa i know you everything and all uh we'll i'll try to show you how to create an intent and all so yes if you are new to this channel please make sure that you hit like so that will just start in some time okay so till then i'll just write d7 of nlp so how many people have attended the yesterday's session we were day 7 natural language processing okay i will write and i'll share you my screen okay just before that a quick announcement i really want to give it to everyone okay so here it is just go to i neuron dot a i neuron.ai and here you will be able to see one neuron course and going ahead guys what we are doing in this one neuron course all the live sessions that are going to come up in uh in the live itself like suppose if i give go over here if i go over here to one neuron right here you'll be able to see all the live sessions will be available now you can just click on this on every saturday and sunday everything the calendar is scheduled for all of you you know whoever are in one neuron so one neuron already has 200 plus courses all the information regarding that is given in the description of this particular video and along with that every youtubers live sessions will also get uploaded like system design patterns cyber security digital marketing full stack data analytics so everything will be uploaded like if i say full stack data analytics this is a new batch that we have already started all the classes i'm currently taking all these classes you'll be able to see all those live classes also over there okay and in the upcoming days you'll also be having courses like enterprise java with springboot you'll also be having full stack data analytics you'll also be having a job automation tester with javascript so all the live classes will be available to you all over here in just one platform and along with all the other courses that you can see over here in recorded mode right so both these things we are like we thought of giving it to the maximum people so if you really are interested and it usually comes with two year subscription plan all the information regarding that will be given in the description of this particular video okay so you can just go and check out the video okay ah fine let's take some simple interview questions uh now in natural language processing or whatever things we have discussed in the previous classes right like rnn right so in rnn we have specifically focused on forward propagation okay so one simple question to you is from stats this is the first interview question first interview question first of all what is this kind of distribution of data known as okay let's talk about this what is this distribution of data called as uh just let me know what is the answer for this if i have this kind of distribution of data what it is called as okay this is the first question obviously you know this right you know this you know this right what about this right tell me this can be an interview question for you what is this distribution of data this is obviously called as normal distribution this is called as log normal distribution but what about this what is this distribution called as yes it uses power law distribution but this distribution is basically called as pareto distribution okay how do we convert a pareto distribution into a gaussian normal distribution okay just let me know so this is this can be one interview question what is the technique that you probably use to convert a pareto distribution into a normal distribution this is basically called as box cox transform okay box cox transform okay so this can be a very important interview question suppose if i have any kind of distribution the next question can be how do i know what is the technique that i may probably use to make sure that to understand whether this is a gaussian or normal distribution so this can also be another question to you what technique do we apply in order to check whether this distribution follows normal or gaussian distribution so this can be another quick question to you tell me the technique guys and i've already taken all these things right what is the what is the technique called as this technique is basically called as q q plots right so people are not answering okay so we can definitely use q q plots and um you know with respect to this everything will be there okay tell me what is standard normal distribution this can be another question to you standard normal distribution what is standard normal distribution okay what is standard normal distribution okay so this can be another question to you one more is that suppose i have this distribution and i have this distribution okay what is the what is the relationship between mean median and mode in the a diagram and in the b diagram you can also tell me over here what will be this specific answer because this quickly you have to set say in front of the if this is left skew this is right skewed so what is the relationship between mean median mode okay next one next question basically is what is the difference between fit underscore transform and transform this can be another amazing interview question for you all okay a mean is greater than median greater than mode true here mean is greater than median greater than mode okay here mode is greater than median is greater than mode is greater here mode is greater than mean right so what is the difference between fit underscore transform and transform okay there may also be a question that when do we use fit underscore predict method okay when do you use this specific method okay so there are some amazing questions in this right so tell me what will be the answer for this too i'm not not writing okay so this can be a very good uh interview questions for you just to make sure that you're confused then the next question can be what is the difference between normalization and standardization this can be another interview question for all of you okay what is the difference between normal ideation and standardization fine this many questions and i'll again come back with new questions as we go ahead okay so if you like this all questions please make sure that you hit like and yes always make sure that you keep yourself prepared and once you are able to keep yourself prepared it is always good that you will be able to see all those things right so perfect uh i hope you like this question shall i start the session now shall i start the session everyone here will start the session okay now let's go towards rnn okay what we saw in rnn so basically we create this kind of diagram and this is the generic notation of rna expand this with respect to time series or sequences so it will look something like this okay go to the next step it will be something like this then again go to the next step something like this again go to the next step okay and finally we'll be seeing that this output will be taken it will give in to a function can which can be a sigmoid or activation function based on sigmoid or softmax activation function and finally i get my output which is nothing but y hat then we calculate loss and then the back propagation will happen right so here i'm going to give my word x 1 1 x 1 2 x 1 3 x 1 4. here weights will get initialized and finally i will be getting my output one your w dash will get initialized output two output three the blue dash output 4 w dash and finally i get my output right everybody clear yes now with respect to this uh i told you uh how you are going to write the forward propagation uh technique for this so over here uh when i'm giving the specific words right like if i'm giving my x 1 1 x 1 2 x 1 3 x one four and suppose this is my output zero or one right the first thing uh with respect to any kind of output that i am going to be one right this o one will nothing be it will be this multiplied by this right in the form of vectors x 1 1 w okay and in the neural network we will having some function x 1 1 multiplied by w and then when we go with respect to o2 right in the o2 we basically calculate what we basically write a function f and first of all we multiply x 1 2 multiplied by w okay x one two multiplied by w uh plus then we also have to be dependent on this so it is nothing but o one w one right one multiplied by w dash right so this is my first output with respect to o2 i hope i have made this very much clear okay in forward propagation proportion of output weight is fit to the next time stem please solve this confusion since the same weights right it needs to continued in that next time stamp only right whatever is initialized it will continue in the back propagation how many number of cycle it will get changed that we have to have a look okay and similarly if i go and probably write o3 then it will be f multiplied by x13 multiplied by w so i'm just going to write like this x13 multiplied by w plus o2 multiplied by w1 right so this will basically be my okay and then o four then i have my function and over here it will be o four will be dependent on x one four so x one four multiplied by w plus uh o3 multiplied by w1 right w dash okay so like this will go till this end and finally we will calculate loss y of y hat and then we'll back propagate now everybody see this with respect to back propagation so i hope is there or same weights at the initial stages yes all these weights will be same at the initial stages but in the back propagation we will try to update them right we'll try to update them immediate [Music] you know independently now see once we do the forward propagation if i really want to do the backward propagation that basically means i have to go in this way i have to make sure that this weight gets updated i have to make sure this weights also will get updated for that many number of time stamps each and every weight should get updated i hope everybody agrees with me right so all these weights needs to get updated as we go ahead okay and this is super important and i've already shown you in a n how does the backward propagation actually happen right i hope you have got some idea regarding that if you know what is chain rule of differentiation and all so based on that you will be able to see how the back propagation will happen okay now let's go ahead and let's see that how do we update this w dash for the first instance after calculating loss okay so here i will uh definitely be able to write something like this derivative of loss with respect to derivative of w dash so i need to update this weight how do i update this weight i can use a weight updation formula what is the weight updation formula what is the weight updation formula i hope everybody remembers this weight updation formula or let me do one thing let me just take up this entire thing and let me write it down once again okay okay i'm just going to take this i hope everybody understood about forward propagation now we are focusing on backward propagation okay so we'll focus on backward propagation in backward propagation we need to update all the weights okay we need to update all the weights now in backward propagation the first thing that we are basically going to update let's say we are going to update this w dash this one this weight first of all because w dash in the back propagation is coming first okay so how do i update it will be with the help of weight weight come on this is i am not writing the color today huh so this is weight updation formula right what is the weight updation formula in this so here you can basically see that the weight updation formula is nothing but derivative of sorry w dash is equal to and this will be new w dash old minus some learning rate or derivative of loss with respect to derivative w dash right so i hope everybody knows this formula we have done this entirely right and uh you'll be able to see over here that how this updation will happen now but the main factor is that when we learn about chain rule of differentiation we require we really need to find out this right this learning rate will be initialized w dash old will already be available so how do i find out derivative of loss with respect to derivative of w dash right so how do i specifically find out this specific thing right derivative of loss with respect to derivative or w dash right and you know that first of all on before this loss you calculate w uh y hat right so i can basically use a simple chain rule which will be like derivative loss with respect to derivative y hat multiplied or dot operation derivative of y hat divided by derivative of w dash right so i hope everybody remembers this so say this chain rule of derivative we will be able to find out derivative of loss with respect to derivative of w dash right now once this gets updated after applying this formula we have to update this w also because back propagation will also happen over here this updation then here also it will happen here also it will happen right now in order to calculate this w so i will again write the formula which looks like w new is equal to w w old minus learning rate of derivative of loss with respect to derivative of w old right now how do i calculate this derivative of loss with respect to derivative of w right right this one i need to calculate now see where is w w is present over here now if i do back propagation first i have to be dependent on y hat then i have to be dependent on o four then o four to this right so this way we have to use the chain rule so first thing first what i will write if i really want to write this it will be derivative of loss with respect to derivative y hat then derivative of y hat will be dependent on o four so derivative of o four right derivative of o four and multiplied by derivative of o4 divided by derivative of w so this is specifically chain rule right this is also another chain rule so by this we will be able to understand how things will be going to go right chain rule i hope everybody's clear yes yeah everybody clear with this i hope you're able to understand till here how did we apply the chain rule if you are liking it please make sure that you hit like okay now again if i really want to update this w dash then what will happen i have to be dependent on loss loss to y hat y hat to o4 o4 to w hat right so something like this so suppose if i really want to update again derivative of loss with respect to derivative of w dash at time stamp what time stamp so this is t is equal to one t is equal to 2 t is equal to 3 t is equal to 4 right at time time t is equal to 3 this w dash i'm actually updating right at ti times time 3 so in order to update this i have to be dependent on o 4 o four to w dash okay so how i'll write i will write basically derivative of loss with respect to derivative of y hat multiplied by derivative of y hat with respect to derivative of o4 and derivative of o4 with respect to derivative of w dash right so this will be another change so this in short we are using what kind of chain rules okay over here in order to find out and like this all the back propagation will happen and all the weights will get updated over here okay uh i did not use w y hat also i can directly start with derivative of loss with o4 and then derivative of o4 with this then this and this okay no need to write y hat y hat also i've just written it like this but internally it will be able to understand like this from here to here like understand this will be dependent on this this will be dependent on this this internally will be dependent on this then later on this like this all the back propagation will happen okay and this back forward and backward propagation will happen for some number of epochs and we can definitely use something called as early stopping which i have already shown you in my deep learning live sessions whenever the loss is almost stagnant then what will happen we will make sure that the model training is completed okay so this is super super important with respect to the backward propagation but now there is one problem guys just understand this over here i have just kept with timestamp is equal to 4 let's say that we have a very big sentence and probably the timestamp over there will be around 100 200 300. now in that particular case when we keep on updating weights what will happen think over it guys because internally over here in every neuron right some activation function will be used right activation function will be used in the back propagation what kind of issue we may face quickly what kind of issue we may face tell me in back propagation if my rnn is completely very depth right it's very long it's very long and it's like you know like let's say that this is very deep neural network deep rnn neural network deep rnn neural network yes many people are saying it right it is vanishing gradient problem why vanishing gradient problem let's say inside this neuron if we use something called a sigmoid activation function sigmoid activation function actual output is between 0 to 1 but if we calculate the derivative of sigmoid derivative of sigmoid its output will be ranging between 0 to 0.25 right so its output will be ranging between 0 to 0.25 now when your output is ranging between 0 to 0.425 as we go in this side again in the back propagation there are chances that this value will keep on getting smaller and smaller and if it is getting smaller and smaller what will happen over here the weight updation will be like negligible right it will be negligible so all the time those there will be the same weights so in order to fix this issue you know we have to specifically use another kind of neural network which is called as lstm rnn okay now lstm rnn is nothing but it is basically called as long short term memory and inside this long shop term memory this will be a uh recurrent neural network right so this is the entire full form of um you know lstm rnn and this is superbly amazing neural network because this entire neural network will be able to remember the context context of the words that are getting created context of the words that are available over there in the sentences and called as in input cell right we we will learn all these things in detail uh but before that we really need to understand the architecture you know so with the help of this kind of cells you know so over here what will happen is that the neural network will actually get changed okay just to give you a brief overview about how does an lstm recurrent neural network work or uh you know how how the entire architecture is i will just copy one image and show it to you just a second let me just see some better image i hope this is right okay so this is one of the image okay i will just scroll down this image okay just a second guys i'm just going to take this image uh i'll take some bigger image so that you'll be able to understand just a second i'm just searching in the internet okay this image looks good okay this image looks good okay so yes in the internet again attack reddit goes to the internet okay it's okay relu can actually cause dead neurons during back propagation of ecs you're absolutely right so that is the reason we cannot just be dependent on relu uh over here also okay there may be chances that you will probably create many dead neurons okay now uh the the other reason in this rn and you know see uh suppose i have some sentences right let's say there is a sentence over here uh like my name is krish and i want to eat pizza okay suppose this is the sentence right now when i'm probably using this rnn what rna will try to do is that it will try to make sure that each and every word it will try to probably read convert into vectors make sure that just consider that each and every word is important right now some things that we will be probably missing suppose let's say pizzazz dependent on some word like it now in this case you know that it is just before pizza so it is being able to capture the context by with the help of rn and also it will be able to capture the context but over here you can see another word like i i is definitely related to krish right his eye is basically giving the context to krish over here now because of this you know there is one word distance over here but if you have a log long sentence you know this i and my can also be related you know there should be some kind of context an important context but here you can see the distance is quite big right many numbers are then in between so it may be that this output three may be sometime dependent on one this output four may be sometime dependent on one right and this because of the length right over here since this new deep neural uh rnn is very very big it will not be able to capture the context properly right so that is the reason what we do is that we specifically use something called as rna lstm rna and this is specifically called as long short-term memory okay lstm rn and this is how the entire architecture looks like okay just understand that this is one neuron input is going over here this is the output from the previous cell this is the output from the previous cell okay and then we are just sending this information don't get confused we have so many different different operations over here okay i will try to make you understand okay everybody clear shall i continue if you are able to understand please do hit like and i hope everybody is getting it how happy you are please give something yes yes i hope everybody is able to understand till here good enough right okay perfect lstm rnn don't worry we have to learn it properly here i'm just giving you or i'll just give you a brief idea about uh you know the network itself but in the next session i will completely elaborate and open lstm okay okay okay perfect so hit like again if you want to donate join membership support my channel share with all your friends you know that all are like generic things that we usually talk okay this is entirely an amazing altogether uh you know and there is also an amazing blog you know uh that has been written by cohab cohab something link is there uh i will share you the link probably if you want but uh just to understand and make sure that how things will basically there they have written some amazing blog regarding it how does lstmrn work but uh let me open the blog if you want okay just a second cola lstm okay so so this is the vlog if you just search for cola lhtm here you'll be able to see this okay so this blog is quite amazing see uh over here it says that what is the problem with respect to an unrolled recurrent neural network see if this is dependent on this two right it is not being able to see this this statement is there the problem of long-term dependencies one of the appeals of rnn is the idea that they made they might be able to connect previous information to the present task such as using the previous video might not inform the understanding of the present if rnn could do this they would be extremely useful but can they it depends sometimes we only need to look at the recent information to perform the present task for example consider a language model trying to predict the next word based on the previous one if we are trying to predict the last word the clouds are in the sky just just read this you will get a brief idea about it but let me just try to quickly talk about like what all the different different things over here and what all things are not there in the next class i will go with detailed architecture point of view you know what all things are there and all will try to understand okay so let's go ahead and let's quickly uh finish this up okay so the first thing is that this region first of all i told you this is the output of the previous layer output of the previous cell okay this operation this operation where you can probably see cross right like like a simple cross that you can actually see over here this operation is basically called as i'll just show you this operation this is basically called as when whenever there is a cross sign okay this cross sign okay or whether this is plus right this is specifically called as pointwise operation okay so we are going to take each and every point and do some kind of operation if if there is a cross that basically is point twice cross operation right if there is plus point wise addition okay so here you can see plus is the here you can see cross is there okay this cell is specifically called as memory cell okay memory cell basically means what does memory cell indicate see memory basically indicates you need to remember some of the things and you need to forget some of the things okay so first thing memory cell will exist over here okay the core idea just just understand the core idea then everything you'll be able to understand it okay so first of all over here you will be able to see that i have basically discussed about something called as memory cell uh sorry memory cell and here specifically some point wise operation will come this point wise operation will just say that how many things you need to remember and how many things you need to forget okay how many things you need to remember i'm just giving you a brief idea about it okay i'm not explaining any maths okay how many things you need to remember and how many things you need to forget that's it this is what this is the information that is portrayed from this uh pointwise operation which is multiplication over here okay then this operation whenever you see two lines are joining together this is basically called if suppose two lines are joining like this this operation is basically called concatenation okay concatenation why it will do i'll just specifically talk about it okay and if and again understand this is called as memory cell right when we are over here we are we are learning some information we are forgetting some information but here in the next operation which is basically written plus over here here we are adding more information here we are adding more info okay here we are basically adding more info okay here we are forgetting some info and here we are adding more info which type of info we adding any new context info any new context info okay like i may give you an example krish likes data science okay and probably you know i may take someone else yan lakul likes cnn okay now here you see krish and this sentence is basically one context and then as we go to the next line here the context is changing now we are talking about some other person so let's say in our previous cell we have passed this information in the next cell again the context is changing so my neural network should make sure that because now if i want my neural network to predict the future words it should be something related to this right it it should not remember this previous word right now okay so this word will be forgotten over here and the new context will get added related to yandiku i hope everybody is able to understand yes what is this operation don't worry right now i'm just giving you one basic phenomena what things are actually happening here we here this neural network forgot something here they are trying to add info which info new context info okay new context info which is coming after we combine this both the data from the previous cell and the new data that is coming up is this clear or not everyone everybody is it clear or not i hope you are getting some idea regarding this can i get a quick yes don't worry don't worry about the maths i will teach you the maths i will make sure that each and everything is basically media understood right so can i get a quick yes if you are able to understand till here yeah okay so some info is getting passed new context information is getting passed right right so once this all info is basically done okay then what we do we normalize all those information and then we send it again as a new input over here and again this new memory whatever new context memory is there this is memory info this layer is basically called as input layer okay i'll talk about it more as we go ahead input layer okay this layer is basically called as forget layer forget cell forgets it because here we forget some information okay this entire thing okay and this after forgetting this information we basically add this up okay so right now just remember till here but anyhow tomorrow i will take in depth i'll make you understand each and every operation how it is basically happening with various examples and all but just get some idea about memory cell forget cell and input cell okay now i hope you understood this is a very important interview question can anybody tell me why lstm rnn instead of rna what all problems we found out okay don't worry if you are not able to understand forget and remember is confusing don't worry i will teach you tomorrow again okay i will repeat all these things but here i just want to give you an idea this thing forgetting is happening here this is called as memory cell okay this this entire thing is basically called as a memory cell over here don't worry i will make sure that i will explain you all those things okay why lstm rnn instead of rn so the first thing is that don't worry if you are not clear with lstmrn at least you are able to you are able to understand rnn right yes or no you are able to understand rna okay so first of all there will be a vanishing gradient problem vanishing gradient or dead neuron okay the second thing that we actually focused on is that the context right context info let me just open that cola block so that you will be able to see this and it is very clearly very nicely written just give me a second i will just open the cola block okay just a second so if you just search for cola lstm right so here you see the problem of long term dependency everybody will will read this okay we'll take a five minutes time and we'll read this one of the appeals of rnn is the idea that they might be able to connect to the previous information as i said context information right previous information if rnn could do this they would be extremely useful but can they it depends sometime we need to look at the recent information to perform the present task for example consider a language model trying to predict the next word based on the previous one if we are trying to predict the last word in the clouds in this sentence that is clouds are in the sky we don't need further context it's pretty obvious the next word is going to be sky like suppose this is the sentence and i need to predict the next word obviously in this we don't need any further context so what we do we our rna will be able to predict it in such cases with the gap between the relative information and the place that it is needed is small rnn can learn to use the past information see if the gap is small obviously the previous context it will be able to understand but what if this output is dependent on the first word this becomes a long gap right but they are called they are also cases where we need more context consider trying to predict the last word i grew up in french france and i sp i speak fluent french so in between i need to predict all the words recent suggestions suggest that that the next word is probably the name of a language but if we want to narrow down which language we need to context of france right so for this whatever i need to find out in between i really need the context of ranch from further back it is entirely possible for the gap between the relevant information and a point where it is needed to become very large okay unfortunately as the gap grows the rna becomes unable to learn to connect the information so when this gap is continuously growing there will be higher dependency of the current output with some of the inputs that are always there in the back right so that is the reason over here the context info because of in specifically in deep rnn they will be huge right those information gap will be huge this information gap will be huge so i hope everybody is able to understand so if someone asks you what is the main issue why specifically we use uh lstm rna you just say that let's say if we have a very deep neural network and some of the output is really dependent on the first word right so this context cannot be easily captured with the help of rna clear everyone yes yes everybody i hope everybody i hope everybody is clear with this right yeah guys so that is the reason why we specifically use lstmrl okay so tomorrow what we are going to do today i will probably end the session in another 10 minutes but today tomorrow we will understand this detailed architecture right we will understand this detailed architecture we'll try to find out what exactly are the cells which i'm saying right forget cell memory cell why how it is going to get used everything input layer and what is the main aim of all this kind of lstm rna and then after that we will move towards practical applications right practical application using python so we will try to do that okay it's is it only because of the number of words between or more where we will get gap not only getting gap it's all about like how easily the context information will be remaining right and with the help of lstm i just want to pick up some context rnn cannot just remember each and every word that are there so if i'm probably talking about myself this memory cell will make sure that i remember only some specific information that are related to me okay so i hope everybody is clear with this and i hope you like this specific session uh i will continue tomorrow guys today i wanted to cover till here tomorrow we will go towards the detailed architecture of this okay lstm rnn and then we'll also try to do some uh you know practical application is it possible to get the notes of any important context suppose i say krish likes to eat pizza and he also likes to go to movies so he should have a context for krish and what lstmrn does is that it makes sure that that context is captured and how it is because of this architecture which we are going to discuss tomorrow okay so all the information these materials will get uploaded in the description let me have a look whether it has got uploaded with respect to rna or not and will continue our discussion tomorrow this was it for my side and don't worry if notes is not there i'll make sure that it gets uploaded till today evening okay and i think it within one hour it will get uploaded i will anyhow tell you uh regarding that okay so thank you all this was it for my side today i have some other work also so i did not continue the session for much more time but yes please make sure that you subscribe the channel uh go ahead and subscribe krishnak in the channel and uh we'll be doing a lot of things many things are going to come up there is also something called as embedding layer which we are going to see as we go ahead okay so yes hit like and subscribe and yes please do support buy some donations or join the memberships so that it will help me to keep this kind of sessions again and again okay and this is the plan in the main main main krishna channel i'll focus on putting more live sessions and learn new things thank you guys have a great day bye bye take care