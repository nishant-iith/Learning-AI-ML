# LSTM Practical Implementation In NLP Application

hello guys so we'll just wait for some time so that everybody joins and then we will start the session okay yes we are live I hope everybody is able to hear me out please can you give me a quick yes if you are able to hear me out in short you'll wait for some time so that everybody will join the notification is being sent hello hello everyone just a quick notification will wait for some time just give me a quick yes if you have got the notification or not because I don't know whether YouTube is sending the notification quickly or not okay so we'll just start the session in some time till then can you give me a confirmation whether YouTube is sending you the notification or not yeah so some people are saying yes so we will wait for some time so that everybody joins uh I'm just waiting for all the notification to go to everyone you know and it's been many days since I've not taken the session I was just not bit well okay yes okay so yes people are getting the notification now so guys uh I hope everybody is getting the notification yes or no so hit like uh today we are going to do some amazing NLP practical implementation today is the day 10th of the NLP Community session or and transfer learning will try to learn when we are learning Transformers and bird guys so Transformers and bird after this is left so we'll try to see this you know so um if you are not probably getting the notification please make sure that when you subscribe the channel also make sure that you click on the Bell notification icon right how are you now yes I am good uh still recovering uh it is going to take some amount of time you know but anyhow I will be taking up the sessions let me just have a look whether everybody is getting okay perfect so the course uh the materials that is going to getting used is given in the description of this particular video please make sure that you use that particular material and we are going to do that specific thing today in this session okay love your content can you make a linear algebra calculus course content sure I'll do that okay what happened to you I was not well but unwell you know had some had some I like had some viral going on you know so it's fine okay so we'll just wait for another two couple of minutes so that everybody joins till then hit like and yes after long days I'm taking some Community sessions over there and probably will be having fun now by going and implementing some amazing things right so before we go ahead please make sure that you download the information that is given in the description of this particular video so we will be using those materials as we go ahead you know yeah so if you probably go and see in the description of this particular video I have actually given us the Google collab link and you can download all the materials from there okay now let me go ahead and share the screen with you so that we will be able to see extremely sorry okay just just a second just a second I'm just going to set my display to one okay now I guess everybody can see my screen uh can you give me a quick confirmation if you're able to see my screen and yeah please just give me a confirmation if you're able to see my screen and yeah we will be starting right just in one minute I've already given the Google collab Link in the description of this particular video please you can find out all from there and we'll try to solve a problem which is called as fake news classifier using lstm okay so this is what we are going to solve today and we're going to solve make sure that we solve this in a better way so that everybody goes and understand all these things okay but I hope everybody is doing fine and you're quite excited with respect to this particular session right okay so please go ahead and download all the information these materials and let me know once you have downloaded it okay foreign we also going to have live project sessions once we complete NLP we will be able to take this up also so the problem that we are basically going to solve is with respect to this fake news classifier and this we are going to use lstm to solve this particular problem right so yeah so just make sure that give me a confirmation once you download this particular file or are you able to see this particular file from the Google collab link that I've actually shared in the description okay okay and just give me a confirmation once you have this and then we'll start the session okay so the problem statement that we specifically have is that we have a data set from kaggle and here it is basically having some information like title text and all and we really need to predict whether this news is fake or not okay that is what we are going to basically solve in this particular session okay and whatever we have learned from our previous session you can definitely check out the NLP playlist uh live NLP playlist and from there we'll try to solve all these things okay so just give me a quick confirmation everyone before I start are you able to see the information that is available do you see everything in the description of this particular video and just download it and then we'll start the session everyone right and just uh once you download it and once you're ready to use it see all you have to do is that download this particular data set like train.csv login into a kaggle website the link is given over here okay once you download it here you will be able to see the train.csv file and this train dot CSV file I have already downloaded in my folder so here you can see this I have just uploaded over here just drag and drop it over here it will be available in the in the Google collab okay don't worry we'll do the live projects I'll be using Dockers and GitHub actions okay okay so let's go ahead uh without wasting any time and the first thing is that as I said let's see our data set what exactly it is so I'm just going to import pandas SPD so I'll be executing over here we are reading this particular data set train.csv this data set had some um you know there are some uh I think encoding of the characters some problems is there with respect to this particular data set so I have actually used this see if I probably just run this particular code DF Dot DF dot read underscore CSV and if I just try to read this train dot CSV here you will be able to see that I will be getting some error okay the error is nothing but the data okay PD dot reads okay PD dot read C underscore CSV when I'm reading this particular data I will be get you can see over here I'm getting this particular entire data set right so if I probably read this and if I save this in DF here you will be able to see that I will be able to see this particular data so this is nothing but DF dot head right so definitely I have title I have text and I really need to predict whether this is a fake news classifier or not right if the label is one it is basically a fake news classifier if it is 0 it is not a fake news classifier right so this is my data frame that I'm trying to read along with this no need to use this particular code because I thought there were some errors with respect to this particular code okay now when we read this you know I have text and title so for this particular problem statement what I'm actually going to do is that I'm going to take the title of this particular article or news that I'm actually getting and we are going to train a lstm neural network to basically determine whether the label is one or zeros okay and we are going to see how this can basically be done and how we are going to perform this kind of sentiment analysis of fake news classifier with the help of uh with the help of lstm okay so this is my DF dot head as said okay you can also now the first thing after this is that let's go and check whether I have any null data set or not so I'm just going to execute DF dot is null okay and we can also do the summation so when I probably execute this here you can see title has somewhere around 558 null information 197 text is 39 label is equal to 0 right so definitely some amount of null values are definitely there now in this kind of case what should I do tell me whether we should drop the values it is a better way or should we try to replace something right since this is a text data we definitely cannot replace some data over here so what we will do is that we'll try to drop this nand values okay so wherever its title nand values are there and all and here probably you can also see that if I write DF dot shape since this is a data frame let's see whether this will get executed or not I have so many records 20 800 records right so if I delete around 2000 records it's fine we don't have to probably work or worry about it okay so definitely 1957 records if you're trying to just remove that is fine so I'm just going to drop the nand values because just understand we cannot replace the title with something else right we cannot replace the author with something else in this particular scenario and I usually have a very huge data somewhere around 20 800 records so it is fine I can definitely drop all the nand values that I have okay so that is the reason I'm actually trying to say okay so here we are going to basically execute DF dot drop n a okay now let me just go and see df.head okay now when I execute DF dot head over here you will be able to see that I have some records like title author text and label okay now obviously label is my dependent feature okay so what I'm actually going to do I'm going to basically drop this label and put make sure that all in my x axis right I can I can use all this as my independent feature but my problem is that I really want to work with only title right because this title only will try to implement an lstm okay so in this title we are going to implement an lstm we are going to use this we are going to use word embedding over here which I have already shown you in my previous session okay so here this title column we are going to take first of all let's get all the independent feature in order to get the independent Venture I will drop this label column and this label column once it is dropped with X is equal to 1 I will be getting all my independent feature so these are all my independent feature in X and DF of label will basically give me the dependent feature which is my output column okay so I hope everybody is clear till here it is super super easy I did not do much over here and now if I probably go and see my x dot shape here you'll be able to see that I'm having so many records right with four independent feature and similarly y dot shape it should be same 18285 okay now uh the next thing that we are actually going to do is that let us go ahead and import this tensorflow and we'll try to see which version of tensorflow we specifically have so if I go and see TF dot underscore underscore version underscore underscore we are having 2.8.2 okay now here you can definitely see that the version of the tensorflow that we are specifically using is 2.8.2 now if I really want to import lstm quickly let me know guys because from the previous session whatever we have actually discussed you know that we use something called as embedding layer you know why we specifically use pad sequences we know why we use sequential y one hot by lstm and white dense so these are some of the quick libraries that we are going to import one is the embedding layer one is the pad sequences one is sequential one is one hot lstm and dense okay so I'm just going to execute all these things so till here can I get a quick confirmation I hope everybody is able to understand till here okay embedding layer basically helps us to implement that word to work implementation uh we also say it as an embedding layer altogether we have something called as Pat sequences pad sequences is for a padding it can be pre-padding and post padding which I have also discussed in my previous class similarly I'm going to use sequential one hot encoding lstm and this to start with you know that we will be using a vocabulary size and in this particular case the vocabulary size that we are going to use is 5000 this basically indicates that let's consider in my dictionary I have 5000 words okay what is vocabulary vocabulary basically means that how many unique words I specifically have so in this particular case I have somewhere around 5000 words okay so yes 5000 words are there which I have already explained in my previous session also so here I'm just going to consider 5000 you can also consider 10 000 but always understand the more you consider this the one hot representation will be quite big with respect to this particular vocabulary okay now let's go ahead and focus on show seeing that how we can use this title column and probably create a one hot representation in that okay and I have already told you with respect to OneNote representation what will happen okay what will happen with respect to one hot representation if anybody can see me quickly yeah one hot representation one hotter presentation will be giving me all the specific words and it will be giving up representation of this at which Index this particular word basically exists on that 5000 words itself right so that is what we are going to do over here so first of all I'm just going to make a copy of this and I will be I will not work on X because this actually has my entire independent feature right now if I write message of title of one so title only I am going to consider because this is what is the feature I am going to use for training our model right title I'm not going to use text text has a huge data set so it will probably take time okay now if I probably go and see what is there in my messages okay so messages if you go and see here you will be able to see that I have so many records and I also have ID and this specific value like 0 1 2 3 4 5 right so first thing first let me reset the index so once I reset the index here you will be able to see how my messages will look like okay so this is what is my messages looks like right so I have just reset the index with respect to all these values you know so just to make you understand that these two column are not that important index and ID but anyhow you need not even need to implement this so I'm just going to comment it down okay comment it down over here okay now let's go and focus on title because we really need to perform one hot encoding on this right and for that we really need to perform some of the things what all things we need to perform we can apply stop words you know we can apply uh limitization or stemming you know we can also apply stemming uh over here you can see pH dot stem so two things we are basically going to apply one is stop words from this particular word I don't want all the stop words over here and the second thing is that I am going to do the stemming for all these words that are present inside title since this is a sentiment analysis problem statement if we just focus on doing or implementing the stopwards and stemming that will be more than sufficient for us okay so here is my messages quickly and now I will first of all download all the stop words so for that I have to import nltk re basically says regular expression we basically need to use some regular expression over here and then from nltk dot Corpus I'm going to input stop words now from this top what I am going to download all the stop words with respect to English and the other language that we really want to apply okay now from here already what we are actually doing over here is that we are basically pre-processing the data set okay pre-processing the data set basically means first of all I will be importing the porter stemmer this is specifically used for stemming purpose okay stemming purpose right so if I probably open my Notepad so in this problem statement what all things we are doing okay I'm just going to write the first step is that I have my data set uh the second step is that I divided this into um independent and dependent feature the third step that what I have actually performed is cleaning the data right and when we really need to cleaning the clean the data the first step that we specifically apply over here is something called as stemming because this is a sentiment analysis along with stemming the Second Step that I am going to apply something called a stopwards with the help of stop words I am going to remove all the all the you know the keywords that are not at all required like the he she and all right so this is what we are going to do with respect to cleaning the data now if I go over here the cleaning of the data is basically happening over here so I have defined a list which is called as Corpus and inside this Corpus I am trans I'm I'm basically going with respect to each and every data points inside this messages right each and every data points and then I'll pick up this particular color I'll apply stemming in each and every word and then I will probably apply after apply stemming I will also be applying this stopwards over here right so that is what we are going to do over here so first of all I am writing a for Loop for I in range 0 comma length of messages uh no need to print I it will just be printing my rows which row number it is and then here you can see re dot sub right here what I am doing is that apart from small A to Z and capital A to Z I am removing all the necessary all the necessary special characters from here okay so what I'm actually doing over here is that apart from small A to Z and capital A to Z I am removing all the special characters and I am replacing it with blank right that is what I'm actually doing next step is that all my review that I'm actually going to get which is coming from this title column I'm going to lower it because there will be chances that one word will be in a capital letter and one word will be in a small letter and then finally I'm actually doing the split when I do the split I will be getting words by words now when I get words by word I am just going to Traverse this by using a for Loop and this is entirely a list comprehension okay so when I write for word in review so this review will be having a list of words and then I am saying that if that word is not present in stopwatch dot words English then you have to probably stem it okay and if that particular word is present in the stop words dot words I have to ignore it okay so finally I will be applying over here stopwards and stemming and finally I will join all the review and combine that and finally all the Corpus will be created right so if I execute this this will probably take more amount of time since they are around 18 000 records so till then can you give me a confirmation if you are able to understand till here or not right so I hope it is quite simple we are able to understand each and everything and we are able to understand in a better way right so here it is so you can see that it is still executing and yes finally you can see that it has got executed right so can I get a confirmation if you are able to understand till here so once I will go and see in the corpus now here you will be able to see all the title after applying stemming and stop words right you can also apply what is you can also apply limitization to this but I would suggest see limitization will definitely take more time when compared to stomach okay so these are my all the things and all these are my clean text with respect to all the titles okay so clear now let's go ahead and let's see how we can provide the one hot representation okay why should we not limitize instead of stemming you can definitely do the limitization but it is definitely going to take more time in lematization we will be able to see that it will take more time when compared to stemming okay okay now if I probably go and see the purpose of one okay GPU is on guys see see the change run time GPU is on so no need to worry so let's say this is my Corpus of Flynn Hillary Clinton big women Camp RedBot okay now with respect to this okay what we are actually going to do is that you can see clearly over here is that if I really want to apply the one hot encoding okay now with respect to the one hot encoding obviously we have imported something called as one hot if you see over here uh where it is okay GPU part C I have encoded I have imported something called as one hot and this one hot is basically used to create one hot encoding so if I go down and see what does one hot basically require here we specify the words and the vocabulary size when we specify the words and the vocabulary size so whatever word will be there in this vocabulary size right like let's say this vocabulary size is 5000. and each and every word let's say Flynn in this particular vocabulary size in what index it will be that specific word that will basically give me that specific Index right so I am going to go from words in Corpus I'm going to Traverse for each and every word in Corpus and I am going to apply the one hot encoding on that particular word and finally I'll be able to get that in the entire list which is this one hot representation so if I execute this you will be seeing that it will take time now these are all my one hot representation that I am getting now see this if I write Corpus of one so this Corpus of one will basically give me the first text and if I really want to find out one hot one underscore hot one hot representation of one so here you will be able to see if you really want to find out the one hot representation for this words you will be able to see in this index you are going to get that particular value as 1 okay so Flynn word has two eight six one index you are getting going to get value this as one and out of this now see one interesting thing is that obviously this size will be almost same will be equal to this side right so here you have one two three four five six seven here also you have one two three four five six seven right so definitely you are able to see all the representation over here right now what we need to do is that we need to take this one hot representation and we really need to apply embedding layer or we can also say word to vac representation right now before that you know that each and every sentence that is present in the Corpus is of different different length okay so what we need to do is that we need to make sure that all this length of the input that we are going to probably consider should be should be equal to we should be same the length of all these things should be same okay the length of all this particular sentences should be same so in order to make the sentences equal or same size we basically apply something called as padding and for padding we use a library which is called as pad sequences so what I am saying is that I am going to put the sentence length as 20 and the entire sentence length whatever are there in the Corpus we are going to make sure that every every sentence has a 20 length now why I have selected 20 because I've seen I've verified all the sentences hardly some of the sentences are only till 10 to 12 characters you know the maximum number of sentences in the title and within 20 if you are trying to set up it is going to cover all the sentences right so over here you will be able to see the sentence length is 20. now I am going to apply a padding on that I'm going to take a one hot representation I'm going to apply pre-padding on this and my max length will be my sentence length that is 20. so this basically indicates that suppose if there is a sentence of 12 words then it is going to add 8 zeros or eight zeros which is basically my padding which is also called as pre padding and finally here you will be able to see all the embedded Docs right so here you can see all the embedded docs and all the zeros that you have added so if I probably execute this and see my embedded Docs of embedded underscore docs of one here you will be able to see that I am will be getting all zeros in for for and remaining all will be the same thing that we have got on the top over here for this first sentence right so this is what we are doing why not TF IDF guys because we are actually solving an alistium right in lstm we basically use uh word embedding layer right TF IDM is not that efficient when compared to what to make a word embedding right we have discussed about that in our previous session right so here is all my embedded docs this is after we have specifically applied what are the advantages of pre-padding No Such advantages you can also apply post padding see if I probably make this as post padding so here you will be able to see that at the end you are going to get all the zeros okay so if I probably write embedded docs of one okay so here you can see at the end it is going to add all the zeros and it is always going to make sure that this length of this all the vectors of that particular sentence is going to be 20 okay now this is what we are going to do now this is the super important step because in this step we are basically going to apply the lstm layer along with that we also going to apply the embedding layer okay now first thing first the embedding Vector features that we have is somewhere around 40 that basically means each and every word that is present over here which is given by this index is going to get converted by a vector of 40 size so it can be like 40 features it may have right now why did I select this 40 features you can again play with 40 50 100 just like a kind of hyper parameter tuning uh we will try to see whether we'll be getting an accuracy or not with this 40 features itself so what we are doing every index will basically be represented by 40 different features with some different different values okay and that is what we are going to see how we can do over here and already I have explained you the theory part how this 40 features are basically created you know there is something called a c w c continuous bag of word representation or skip gram you know we have learned that in word to back that I have already explained then we are going to use sequential model then we are going to add the embedding layer in the embedding layer the first parameter we are going to add is the vocabulary size so what is the vocabulary size that we have considered it is somewhere around 5000. and then how many number of features I'm basically going to convert this particular vectors to right so here you can see that I am basically going to convert this into 40 different dimensions of features or any index is basically going to represent this as my 40 features representation this is basically my 40 represent features representation okay and finally you'll be able to see that I am giving my input length is equal to sentence what is the input length of my sentence so it is nothing but 20. now once I give this this will basically create my embedding layer all together okay so I'm just going to update the step over here so cleaning the data stemming stop words then what we do we basically create or fix a sentence length fixed a sentence length to fix the input right and then we apply one hot representation this all theory part I've explained in my tutorial ninth guys one hot representation after constructing one out representation let's construct the lstm neural network okay so we do all these steps and now we are in this particular step so after creating an embedding layer before this after one hot representation we create an embedding layer okay and then I have finally my lstm neural network so here I am basically going to take lstm with 100 neurons okay and again here you can select 200 neurons 300 neurons based on your choice and then I'm going to add model dot add dense as one because my output is binary so whenever my output is binary I'm going to use my activation function as sigmoid and when I'm compiling it I am going to use the loss function at binary cross entropy because whenever we have an output as binary we usually use binary cross entropy the optimizer that I am going to use atom and the metrics that I am going to use is something called as accuracy and finally I print my model summary okay so once I print my model summary here you'll be able to see that I'm using an embedding layer which takes 20 inputs 40 outputs lstm layer with 100 neurons and the total number of parameters that I'm going to use for this model is 256 K right 256 501 okay so here you can definitely see the shape of length of embedded Ox embedded docks is basically the length of my input and this is 1825 with respect to my output right so this is my final one all the embedded docs that I see in the form of list that is this indexes right what I'm actually going to do I'm going to convert this into an array right so NP dot array here is my X final and Y final NP dot array here is my y and finally you can see this is my shape of my final array so here I have 18285 comma 20 18285 comma this right so ah this is what we have actually done with respect to all the representation that you can actually see okay finally we do train test split so Trend test split over here so train test split is very simple and we then fit our model by running I'm running 10 epochs and The Bash size is 64. the validation data is x-test wise test and it is extra invite Ray okay so here now you can see that that model training will start the epoch one has basically started here you can see the accuracy the validation loss whether it is getting decreased or not here are the accuracy that I am seeing it is increasing right it has become 0.98 0.99 0.99 validation loss is almost like 0.41 point this one okay you can also apply early stopping over here but with respect to 10 epochs I have just executed it okay everybody clear till here if you are able to understand whatever things we did yeah can I get a quick confirmation if you are able to understand yeah I'm just having a little bit less energy guys but I will try my level best to explain everything right so I hope everybody is able to understand till here just let me know whether you're able to understand or not now what I'm actually going to do after this is that don't worry about the dropout dropout is basically to improve the performance of the model here I am basically going to see what is my performance metrics so performance metrics what I will do is that I will try to predict it with respect to X underscore test okay now once I do the prediction I will be getting my underscore thread and this y underscope red I have kept the threshold of 0.5 if you don't want to keep a threshold of 0.5 definitely use something called as AUC AUC Roc curve okay to basically find out what should be the probability that you should select okay now here I am saying that whenever my wipe rate is greater than 0.5 I'm going to take it as 1 or 0 1 basically means it is a fake news 0 basically means it is not a fake news so this will basically be my wipe red and then I will be using my confusion Matrix and here you can basically see if I go and see my confusion Matrix here I am getting these all records which is I feel this is a good accuracy so let's go and see the accuracy score which is nothing but 90 now one more thing that I can basically use is from a scale on dot metrics just to make sure that everything is good I'm going to import my um classification report and if I probably print my classification report foreign classification Report with Y test from a white bread here I am going to execute this one so overall a good accuracy that I'm actually getting is somewhere around 99 or 91 percent is there any special reason for choosing the 10 100 in model no it is not necessary 100 right we can choose any values right it can be 100 200 300 but again we should definitely try with different different values and see that how the model is performing okay here I have just randomly selected as 100 there are different types of hyper parameter tuning in the model training also okay you can definitely use that everybody clear yeah I hope everybody's clear with respect to understanding all the things false negative is high your false negative is bit high but the accuracy is 90 yeah because there are many data points you can see that over here and over here it is increasing right and this can also be made sure that like suppose if I select 0.6 as my ah predicted value or threshold you know we can definitely use our AUC Roc curve right but again it is a domain expert thing what we can do now here also you can see with respect to 0.6 also I am getting the same accuracy right so you can definitely go ahead and select different different uh threshold values and your you can see that how this is basically performing yeah what what can we do to increase the accuracy so there is also one of one more option which is called as Dropout we can definitely apply Dropout like how I have applied Dropout over here model dot add Dropout model dot so after lstm layer after the embedding layer I cannot apply a Dropout which says that 30 percentage of this lstm while training will be disabled similarly after an HTML layer also I have applied this so let's see whether this will increase the Dropout uh if you use Dropout with less increase or will this make this model better or not so I'm again doing model dot frame fit so let's see in Epoch one I'm getting an accuracy of 82 94 96 training accuracy validation loss is decreasing this is very good that is decreasing it is decreasing but I think it will again go back to 40 I guess okay now let's go and see whether we are able to see this or not or properly we are getting it or not so here is my wipe red now my confusion Matrix over here I don't think so it has become better uh it is a minor difference is there right but it definitely for a bigger problem statement you can definitely use dropout dropout will reduce overfitting okay it will it will definitely reduce overfitting if you are specifically using okay now this is was the entire thing now what I would suggest is that kaggle or UCI Repository if you go for search for UCI repository data set here you'll be finding a lot of problem statement which you can probably consider doing it you know so let's say this is related to text take up any problem statements and try try to you know probably do some text classification and all right so if you probably see this particular data set buddy movie okay let's see okay this is user ID no text information fine not a problem now what I am actually going to do I'm just going to search for some text classification problem let's see and that is Google search so you can take up any problem statements over here and try to solve it uh with respect to a text classification problem okay then you will be able to understand this in a better way and try to solve any problem that you can okay take up anything like uh label text from thread data set okay and just try to see whether you can apply any sentiment analysis and try to use the same technique of fake news classifier that what we have done over here and by this way I think I have also taught you how to create an embedding layer along with that how do you create an lstm layer and how you can basically come up with this threshold definitely see something called as AUC Roc curve and you can definitely apply right it is up to you not a problem at all okay so I hope uh how many of you like this video how many of you like this session now the next session will basically happen on Wednesday where I am going to start with something called as bi-directional lstm and encoder than decoder okay so we will try to basically ask that okay so I hope everybody's clear with respect to this can I get a quick yes if everyone is clear yeah I hope you like this session everyone yeah sequential sequence also we will be discussing and uh both are good see BCA or BSC statistics everything is good as such okay not need to worry about it so please do an assignment and make sure that you drop a mail to me at Krishna x06 gmail.com and I will definitely have a look onto your Solutions whatever you have implemented and then we'll tap uh I'll definitely provide you some of the feedbacks so what you can also do is that try to take this text data and try to solve it if you can okay and just try to find out whether things will be done or not the next session will be happening on Wednesday uh and again I'm still recovering so it may take some amount of time okay can we do text classification for getting the difference between comments and code in a program yes you can okay so yes this was it for my side guys please hit like at least make it 200 before we go ahead and uh I'll be coming up with more solutions more amazing Solutions and try to show you different different things right and yes this was it from my side I hope you like this particular session and yeah hit like make sure that you subscribe Krishna Hindi channel uh yeah it can also be applicable with music generation but there we have to use some other type of lstm neural network that is called as bi-direction okay so yes thank you and this is already added in NLP playlist I will see you all in the next session on Wednesday uh keep on rocking keep on learning never give up and yes please do make sure that you join some membership in my YouTube channel so that you can keep on supporting me in creating this kind of projects and doing some live sessions uh and any donation is fine you know it will definitely help me to make my channel grow okay so thank you guys have a great day keep on rocking bye