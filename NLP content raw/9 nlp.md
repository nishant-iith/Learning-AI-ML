# Word Embedding Layer And LSTM Practical Implementation In NLP Application

oh hello all i hope everybody is able to hear me out can you give a quick yes can you hear me out quickly okay very much clear so we'll wait for some time till everybody joins and then we will probably start so till then uh i would suggest everybody download this github uh just let me share you the github quickly okay uh just give me a second so that i can share you the github so that we can discuss about today's class so i will be coding along with you okay so that you can actually follow me anyhow so here we go everybody i have given you the collab link or should i put it in the description also okay so i hope everybody has got it yeah so please download it quickly everyone yeah please download it okay just reload the page in the description you will be able to find out the collab link so quickly go ahead and download it and just give me a confirmation once you get it okay till then i'll just wait but i hope everybody is doing fine everybody is doing well right yes okay now let me share my screen in front of you what all things we are going to do we are going to discuss a lot of things quickly just give me a confirmation if you are able to see my screen right so uh today uh is the ninth day oh it's amazing right ninth day of nlp session that is natural language processing today we are going to cover up two main things one is word embedding layer practical implementation we'll see we have already done you know word to work right we have used what do it but word embedding layer there is something in a neural network which we are going to see how to do this with tensorflow and python and python so we will discuss about this both the things okay second uh we'll do an end to end project with the help of lstm let's say let's take a sentiment analysis project so that you get an idea of how to train it okay and uh um we are basically going to discuss both of these things okay and we'll discuss in an amazing way uh where we'll be discussing both now till the last class right i mainly focused on one very important thing is that how to convert word into vectors so i hope everybody has got an idea about word to work so anybody who wants to say how does a word to work converts a word into vector i think we have done this today we'll focus on the in in in lstm also or in our neural network we have something called as word embedding layer and this will also convert your words into vectors itself okay this acts pretty much like what to wreck but uh the thing is that how do we train this today i will be showing you okay the training part we will try to we'll try to train this entire data set uh by just seeing some examples okay everybody clear what we are going to do today okay yes can i get a quick confirmation if you are clear too much or not interested in class today guys something some response should be there if not hit like hit like at least make it hundred uh and now people will lose interest so probably i'll make uh you know this session as today is the last session on nlp uh let let us do after a week time you know if you're getting bored in things okay so if you want to do like that i'm ready to do that okay let's take a graph of one week so that you know we'll try to do it in the next to next week okay if you want okay perfect so quickly let me just share my screen to you now what we are going to do is that see word embedding layer i'm going to show you okay and i'm going to show you like what all things we are going to do in the word embedding first of all in the word embedding layer we will be considering some sentences let's say so sentences which will be specifically having words then for the first step what we are going to do after we get the sentences we converted this into one hot encoding i hope everybody remembers what is one part encoding right so we are basically going to use one hot encoding okay and uh okay so one hot encoding basically means what i hope everybody remembers uh before considering one hot encoding what we have to do we have to basically take up a vocabulary size okay vocabulary size after considering one hot encoding then the next step what we do we do a step which is called as padding okay i will explain you each and everything how this will happen okay now in padding uh why we specifically do padding there are two types of padding which is called as post padding and pre-padding and pre-padding today is super important okay if you know this steps probably you'll be able to work with any kind of nlp data okay post padding and pre-padding then the fourth step what we will do is that we will convert whatever one what encoded values we have right this one hot encoded values will get converted into vectors okay so let's start one by one okay now first of all tell me if i have a sentence like this krish likes pizza okay okay and let's say shyam likes burgers suppose if this is my sentence let's say this is the example of my sentence okay let's say i have two sentences and i want to basically see how this will get converted into this one now let's say if my vocabulary size is somewhere around 10 000 or let's say my vocabulary what is vocabulary size basically means number of unique words let's say if it is 500 let's say i the total number of words that i have in my dictionary is 500 now if i really want to perform this one hot encoding for each and every word let's see then how it will become krish will become something like this let's say krish at one hot encoding will have all the numbers as zero at an one point let's say this is the index this is the index let's say 325 and this index is basically saying about krish in the vocabulary so this side it will become 1 i hope everybody remembers with respect to 1 not encoding yes i hope this is pretty much clear we have discussed already and remaining all will again be zeros right so this is what one hot encoding is all about right and similarly if i go with the next word let's say likes likes will be given in another one hot encoding and let us say 0 0 0 will be there there will be one index you know j k l after k only it will come let's say in the index in the index let's say 3 28 like what is there and remaining or will be zero right so in this specific way why 328 index i have taken because l will come after k right j k l right so it may be nearby now considering this one hot encoding i have to take this entire words and then probably create an embedding layer embedding layer embedding layer and convert this into vectors this is what we want to do and this will be basically done by a embedding layer in deep learning okay so there is a deep learning embedding layer like how we have dense layer we can basically train this network completely okay so this is what we are going to do and i am going to show you and once we complete this because this is the first task this task actually helps us to do text pre-processing okay in machine learning you can definitely use word to back here also you can use word to work but it is always good that whenever you have your own data set this is not for uh you know pre-trained uh we are not going to use any pre-trained model instead we are going to use some data set then we are going to train it using embedding layer and then we are going to find out the relationship now quickly uh the file that i have actually given you i'll go and show it to you so i hope everybody remembers the this you i hope everybody has got this file this file in the is in the chat comment or even in the description of this particular video just reload the page you will be able to find out the link has everybody got this file yes yes everyone have you got this file just give me a quick confirmation so that i can start yeah you can get you you can find out the file in the print comment or in the description of this particular video okay just reload the page you'll be able to see it okay okay let's start now the first step i'll just go and change my runtime i'll make it to gpu i'll save it okay so this is the google collab you can basically see from it itself the first thing i'll do i will install tensorflow gp okay because i am going to specifically use tensorflow gpu to quickly execute it it will take some time to get executed let's see how much time it is somewhere around 511.7 mb right and i'll show you how with the help of tensorflow you'll be able to do one hot encoding and all we will try to have a look okay so this will probably take some time let's see how much it will take so once this is installed because we are anyhow going to use tensorflow right so now you can see tensorflow installation is basically happening installation is done okay so the installation is taken place you can also execute along with me successfully installed okay fine now let's go and import tensorflow and see that version okay import tensorflow as tf i'm going to print tf dot underscore underscore version underscore underscore so this is got executed okay so now the recent version is 2.9.1 that is what i'm actually going to use okay now how to do first of all one hot encoding we are going to see okay so there is a library from tensorflow.preprocessing.txt i have a library which is called as one hot okay now this one hot will actually help you to do one hot encoding so quickly let's import this and let's try to see okay so i'm just going to execute this okay so i've got executed this one now let's consider i have some sentences the sentences you can see over here and our main focus is to convert the sentences into some kind of vectors okay so i have the glass of milk i have the glass of juice i have the cup of tea i'm a good boy i'm a good developer understand the meaning of words your videos are good okay so these are my sentences and i'm going to show you how we can basically use one hot encoding on top of it and then later on convert this into vectors using embedding okay that is what we are going to do now if i execute this you will be able to see these are my sentences right so these are my sentences here you can clearly see now the next thing that i am actually going to do in order to convert the sentences into vectors the one important thing is that you have to initialize the vocabulary size right i showed you over here you have to specifically say what is the vocabulary size okay vocabulary size so here i'm going to take the vocabulary size of ten thousand let's say in my dictionary i have to take this words and probably train it with the vocabulary of ten thousand okay so let's execute this so this will basically be my vocabulary size now in order to convert this words into one hot one what representation using this vocabulary size see i'm just going to use this one hot one hot and this is going to convert this into vocabulary size i mean the index is it is just going to go and capture those indexes which has ones okay because over here you can see that you can see over here one is over here right when one is over here the information that comes out of it is the basically the index in that specific vectors okay wherever there is one that is basically vectors uh yes chetan you can definitely use word to work again i am saying you why i am specifically using this because here we have our own data and we really want to create our own embedding layer and what to what kind of vector representation by ourself okay so what i'm actually going to do now for i will iterate through each and every sentences see over here for each and every sentences i'll say for words in sentences and then i am going to use one hot and i am going to give the words in the parameter and vocabulary size okay when i give this two parameters see one thing will happen is that i will get a word one hot representation that basically means for each and every sentence for each and every word i will be having the index number index number where it is one in one hot representation where it is one i am going to get that specific index number so if i execute this quickly here you will be able to see this is my first sentence now see what was my first sentence over here the glass of milk now with respect to this glass of milk you can see the is basically in the position index position of two four two three it is one that basically means out of the ten thousand vocabulary word two four two three index has the word the is everybody clear yes is everybody clear about this yes or no guys just tell me why i am getting 2 4 2 3 over here because at that specific index the value of the right was 1 very much clear yes or no each word will be ten thousand dimension yes it is ten thousand dimensional but here you are getting that every word right wherever the index is one in that ten thousand dimension only that index you are getting over here i hope everybody is able to see right why 10 000 vocabulary i have just initialized 10 000 you can take it 20 000 you can also take it 500 okay is everybody clear with this now similarly the other word glass glass is at the index 5188 it is one right similarly over here the off is there and off basically has at an index of one zero nine one it has the value one and similarly five three three six okay now see see the difference between this first two sentences the glass of milk and the glass of juice right the glass of milk and glass of juice through says it changes every time we run yes because over here the random variable is not put up this one hot right it has a random variable inside it will not be fixed every time okay because the 10 000 dimension it can be different different right now the glass of milk and glass of juice this both are same almost same sentences only milk and juice are changing so you can see over here both the sentences will be having the same vectors only the last only the last index is basically changing because milk is the has one at a different index whereas juice is basically milk and juice are basically having one at different different index is everybody clear yes yes yes everyone and again i'm telling you vocabulary size can be anything if i take 500 remember your values of the indexes will be between 0 to 500 okay so if i execute with respect to 500 also you will be able to see all the values will be getting between 0 to 500 okay 0 to 500 so you can see all these values 0 to 500 okay 0 to 5 if i take it 100 it will be between 0 200 okay but the larger vocabulary size basically means larger good amount of feature representation you will be able to get it now once we create this one hot representation now do you feel over here each and every sentences have a different length over here you can see most of the sentences have a different length increases in vocabulary size causes overfitting not overfitting but processing time it will take more okay yes okay everybody clear now now everybody focus over here do you feel each and every sentences have the same length how does it select the vocabulary vocabulary is completely a hyper parameter guys you can select by your own okay you really need to try out with different different vocabulary size do you see all the sentences are of equal size everyone just tell me do you see all the sentences are of equal size or not is it of equal size or is it of a different size see over here why i am asking you it is a very important question okay here you can see that here i have four words here i have four words here i have four words but here i have five words here i have five words here i have five words and here i have four words right so the sentences there is always difference right tell me in neural network if i need to train all these things i have to make sure that all my sentences size should be fixed yes or no yes or no all my sentences side should be fixed or not yes yes or no yeah i hope everybody is able to understand if i really want to train this with our neural network don't you feel that all the sentences size should be same that is the reason why i had written over here something called as post and pre padding okay this is the next step see i we did one hot encoding now we are into the next step which is called as post padding and pre-padding now what is post printing and preparing but before that let me install some of the libraries like embedding pad sequences this is basically used for pre-padding or post padding what is pre-padding and post padding i'll just say in some time and then sequential is basically for the sequential neural network i'm going to import this i'm going to import this now see what is the maximum sentence length that you can see over here is 5 okay in order to make all the sentences have the equal size and length what i am actually going to do let's let me assume that the sentence length will be 8 the maximum sentence because out of this we can definitely find out which is the maximum sentence over here it is somewhere around five words maximum so in the worst scenario what i have done is that the maximum sentence length can be eight okay now here what this pad sequence will do is that it will make sure that for all these words all these vectors right now wherever it is four or five it is going to make it eight ok it is going to basically make the size to 8. now how can we make all these sizes to it it is very simple either you add zeros at the last or zero at the first okay now in this particular case i am using one hot representation which is these values and i am saying add padding is equal to pre so this basically means pre-padding that basically means to make the sentence equal just add zeros in the initial and make it completely eight okay suppose if you have four indexes then how many zeros will get added forward how many zeros will get added forward guys tell me how many zeros will get added forward how many zeros will get added if my maximum length is eight how many zeros will get added forward tell me this is the question for you four right four zeros will get added in this what about this where there are five indexes how many zeros will get added over here how many zeros will get added over here here how many zeros here here where i have specifically five indexes five if there are five indexes then definitely how many will get added three will get added right so similarly this will happen in this way so that is what we are going to do over here right pre max light sentence is equal to length right so if we print embedded docs here you can see that i'm getting four zeros over here i'm getting four zeros over here i'm getting four zeros three zeros three zeros three zero zeros so this way the one problem that we are facing our input size is not fixed we made sure that our input size is fixed everybody clear yes yes or no yes yes zeros are added and the first if you use pre if you use post then zeros will added at the last okay if you use pre it will be added in the front if you use post it will get added in the last okay so i hope everybody is able to understand okay okay so right now this is called as pre-padding i'm going to just write a comment which is called as pre-pad okay everybody clear till here this now any problem that you solve in nlp later on we always need to follow these steps everybody clear we need to follow these steps without these steps you will not be able to go ahead okay so till now if you are liking the video please make sure that you hit like let's complete 100 likes i know this is all our free content nobody value free content okay so let's go ahead okay now coming to the next one okay what we are going to do after that now if we have done till here okay now we will try to convert each and every value each and every value with some vectors right how do we convert each and every value with some vectors okay how can we convert each and every value through with some vectors so as i said right now we are having like this right let's say one of the sentence basically has 0 0 0 and it has some index like 150 160 200 likes like that's let's say okay now what we are going to do for each and every value we may provide that in the form of feature representations right that is what what to use to do right each and every way and here let's say my feature representation size i'm just going to consider it as 10 because this is very small right i just want to show each and every feature with the 10 different features representation that is what we learnt in what to vect right that is what we learnt in what to work right so every one will be shown in the form of 10 feature representation okay yes yes everyone clear till here so let's go ahead and let's try to do it so here i'm going to take my feature representation dimensions will be 10 okay 10 now i will create a sequential model everybody focus over here now very super important till here okay now we are going to do something amazing okay now we are going to do something amazing now over here first of all we are going to use model equal to sequential okay then in the next layer do you see something called as embedding layer this embedding layer will is going to work similar like kind of what to work only okay and it is going to train all this neural network okay now in this embedding layer first of all i need to first parameter that you need to provide is something called as the vocabulary size now what is the vocabulary size you know that what it is vocabulary size over here you have already taken the vocabulary sizes 500 okay now with respect to vocabulary size the next parameter that you have to give is that how many features you want for each vector how it should be shown in the feature representation how much did i initialize 10 right that basically means each and every index out there should be represented with the features of 10 different values okay that is the reason we have taken 10 then we need to specify the input length which is equal to the sentence length okay so here the sentence length is somewhere around 8 right so we are going to supply this that is the total length three important information one is vocabulary size how many dimension you want what is input length and this is your sentence length okay everybody clear yes till here everybody clear yo everybody clear or not till here yes yes guys still here everybody is clear or not good enough good enough how do we decide dimension size see guys since this is a very small sentence right so that is the reason this feature this is nothing but feature representation how many vectors i need to show right now since i had a very small so i am using 10 if you have a huge data set around 300 feature dimension is more than 5. so 10 feature dimension i have taken that basically each and every index will be shown in that specific way okay so now let's go ahead and what is the next sentence i'm compiling you with adam optimizer and mean squared error i'm taking the loss function okay so once i execute and this is my model summary so my model summary is nothing but it is basically an embedding layer and the embedding layer is taking the 8 as a sentence length 10 as the dimension and vocabulary size is 50. okay sorry 500 okay so when you go ahead and do this and now if you take up any sentence any sentence of this right like embedded docs of zero let's say i'm going to write like this see i'm going to show you now if i write model dot predict of embedding docs of zero what is embedding docs of zero if you remember embedding docs of zero is basically my first sentence right so this is my first sentence so embedding docs of 0 is nothing but it is 0 0 0 and what is this sentence if you go and see in the first one it is nothing but it is saying glass of milk right so this is the glass of milk right so this this sentence that it is representing in the index format is this basically the glass of milk right now for this sentence if i see want to see what is my what is my uh you know feature representation okay all i have to do is that use this model and write predict of embedded docs of 0. if you do this see over here already everything is coming now see everything 0 is represented by this 10 dimensions why 10 dimensions because i have initialized 10 dimensions right over here 10 feature dimensions so 0 is represented over here by this the next 0 is represented by this both of them will be same both this will be same right because it is 0 0 then again if you have 0 again it is going to show over here as 10 dimensions right okay and then again you see one more 0 is there again it will be same with different 10 dimensions now comes 180 180 word is what 181 basically means the so where the is there you will be able to see this is basically represented by the okay yes guys it may not match with your data because you the initial random state is random seed is not checked on okay so you may be getting different values okay then again when we come to the next word that is glass glass is represented by this 10 vectors right and then you have off off is represented by this 10 vectors and milk is basically presented by this 10 vectors everybody clear yes yes yes or no now similarly if i do want to do for all the sentences just write model dot predict on embedded docs automatically you will be getting all the values whatever values you have for all the sentences right clear or not clear or not yes is it good or bad right i hope everybody is able to understand this in a very simpler way i'm showing you the practical implementation i'm going uh hi sir i got a job as nlp engineer without eye neuron it is impossible i got a good hike without a videos and financial class it is not present thanks a lot amazing congratulations buddy um being data scientist the name is only being data science this is nice congratulation you rock and you build rock okay so guys you can congratulate being data scientist because he has done an amazing work out there okay how do we define sentence length what do you consider the maximum sentence length can be okay consider that and write it over there right so that that outer vocabulary issue will not come right so we definitely have to take a little bigger sentence and probably use it okay now uh do you want to do some new use case and we now we have actually converted this into vectors right now what is the next step how do we create an lstm that we are going to okay arvind is his name is arvind okay okay arvind is there okay now quickly i'll give you an assignment do that assignment in front of me okay let's say this is your assignment okay this is your assignment let's say you have a sentence which is like this the world is a better place marvels marvel seas is my favorite movies movie right let's say this are my sentences i like dc movies okay and probably i can also use some weight the cat is eating the food tom and jerry is my favorite movie right so finish your blockchain series entered into live session amazing yeah guys quickly one quick announcement uh i neuron is coming up with the blockchain uh i neuron is basically coming up with the blockchain course if you want to join please join uh again it is an affordable course guys uh we we we give affordable courses that is fine you know you know you are you may be saying krish promotion no i'm not doing promotion i'm just trying to help you out okay instead of giving two lakhs three lakhs take up this course okay so blockchain development 4000 rupees starting from 23rd july we are going to learn so many things over here so if anybody is interested please go ahead i'll put down the information in description of this particular video and if you want any discount use christian coupon code so this is okay fine this is fine right if we really want to give something good if we really want to give something good we can't give it right we can give it okay so don't feel that way krish is doing promotion krish is not doing promotion krishi's wants to provide you the right thing okay so that is the reason why i'm saying you okay so quickly uh use christian if you want additional ten percent off and yeah don't miss this opportunity i and navin sir will be teaching along with that we also have in sanjivan who is an expert in this okay so please yes if you are interested go ahead with this otherwise it's okay free content is always there okay free content is always there right okay now yeah yeah it seems late 23rd july no we give pre-recorded videos also so you don't have to worry okay okay though tom like jerry's okay python is my favorite programming language okay so quickly do it for this guys everyone do this and try to find out the vectors if you can okay and one more assignment that i really want to give up is that go to kaggle and find out this imdb 50k movies review data set try to convert this all into vectors okay and probably we may take up this particular use case over here okay okay so here you can probably see all those are there i hope everybody gets this link so just to take out this course on announcement will be done today or tomorrow mostly i think we have time right to announce it okay so just take up this all questions and try to do it okay try to convert this into vectors and once you're able to do it uh the next class that is going to happen will be on monday monday no tuesday we'll do it on tuesday because saturday sunday i have classes and i'll be really tired okay so let's do it on tuesday and tuesday we will do one lstm project end to end and we'll try to do something related to that hey chris please take some classes on cloud tech azure aws related with ml yes i'm going to basically soon start ml with aws i'm also planning to start dockers and all so if you don't know see i've already started writing code okay let's see see this is the code i've already started doing dockerization and all see see over here you can see dockers and all right so all these things will be coming up soon you know so i'm probably in the next live session after this gets over is end to end machine learning project how you can create ci cd pipeline what is this yaml file how to do packaging you know everything will probably be shown to you so okay so i'm just going to do all these things and the next one which will be this only related to this will be coming up okay uh this will be the next one so we will try to write code from scratch okay what is this setup.py and all everything will try to discuss what is requirement.txt what is file what is docker ignore how to create a package see housing has also become a package how do you deploy this in pi pi all those things i will be taking it how do you deploy with dockers you know see i have deployed with dockers also i'll show you so this is my github okay i was working on this project from morning because i need to really uh teach you all these things because it's super important in any companies so this is the machine learning project if you see over here and this is deployed right this is deployed in here and uh this is quite super good because i have used github actions see github actions right and if you see my heroku you'll be able to see i have already deployed this you you will be able to see what all steps it had taken what is the build process this is all our ci cd pipeline what is setup job right what all things basically happened check out build push and release a docker container to heroku post checkout and complete job you know so all these things i will teach you from basics from everything okay and i just do two to three assignments of this okay i really want you all to do all these things because uh in the next one right when we are solving this kind of problem statement what we will do is that we will try to take up all this particular data set convert this into vectors and train into lstm after this lstm will be hardly 5 lines of code after this after we convert this into vectors it will be five lines of code how long nlp session will take uh it will be i have to complete bi-directional stm transformers bert right so there are some days but let's see how much time it will take or i'll take a pause okay i'll take a pause of probably 15-20 days you know after i complete lstm or all i'll start into dockers and all okay which id is the best i feel uh vs code is the best guys okay vs code is the best and guys definitely require your support go ahead and take memberships make sure that you subscribe krishna hindi channel so that i come up with this kind of free contents and support you a lot till till i can because again this will be a lot of efforts and all will be there but i really want to provide you the best thing okay why pause because um see every day we are having this live session right people are losing interest hardly 100 people are coming to see the videos and all right yes so but i'll not let's see till how much time i will go i think four to five sessions uh we can cover up an lp part okay but anyhow we'll focus on this but two assignments please do it and yes please support you know take up memberships uh from my youtube channel if you can uh you know give super thanks something like that so that you know i keep on preparing multiple tutorials as we go ahead just a request if you can but yeah i'll continue with nlp the next class will happen on tuesday because i have classes on saturday sunday so i will be super monday i may take a leave because too much time okay fine this is it for my side guys thank you one and all and yes uh i will continue the lstm practical part on tuesday okay thank you bye bye take care hit like and yes i will see you all in the next video tata take care bye bye thank you for attending okay yeah in fsds class this all things will be getting covered okay thank you guys bye bye take care