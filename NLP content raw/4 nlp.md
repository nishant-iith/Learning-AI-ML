# Word Embedding, CBOW And Skipgram Word2vec NLP

hello guys i hope everybody is able to hear me out uh if uh if you can hear me please can you give me a quick yes right i hope everybody is doing fine so today is the day four of natural language processing and today we are going to cover amazing things like uh what embeddings continuous bag of words and skip grams which is a part of word to weck so and after that we'll again have a quiz competitions and uh 500 5000 rupees inr will be the rewards that will get distributed right so i hope everybody's doing fine if you're new to this video please make sure that you subscribe like share with all your friends today we are going to cite something amazing for the learning purpose and there's all these all sessions are definitely for community building all the materials regarding the previous session like day one day to day three is actually given in the description of this particular video there will be a link of the dashboard right so just to begin what all things we have actually learned in the last session if anybody knows you can definitely let me know and we'll just start another two to three minutes so what all things we have actually discussed in the previous session hello hello everyone so hit like and please do let me know what all things we had actually discussed in the previous session tf idea bag of words one hot encoding even we completed practicals text free processing many people have also done the assignment and they have dropped me the mail which is quite amazing uh everybody i could not reply everyone but yes you have actually done some fabulous work right ngrams we have actually completed and all right so uh today we are going to continue uh ahead and we are going to learn something more amazing like what to wear and all or will try to understand before that we'll also try to understand what exactly is the word embedding okay so yes uh let's start am i audible i guess guys i hope i'm audible right yeah i feel uh i'm audible all together let me hear my voice again but i'm audible i guess yeah oh voice is low um it should not happen let me reduce the volume if you want yeah i guess now can you give me a quick yes if you are able to hear me out hello hello hello yeah i'm just reducing increasing the volume for you all but i think now everybody is like audible itself right everything is perfect right okay so let me share my screen uh please hit like at least make it 100 before we start the session so here is our screen so today is the day four uh that we are actually going to discuss so day four what is the agenda of this will be for natural language processing first thing we are going to discuss about word embeddings second topic uh that we are specifically going to discuss about is word to weck okay in word to work uh we have two different types one is cbo cbow that basically means continuous bag of words okay and the second one is something called as skip gram okay so we're going to understand both these things in terms of architecture and all and then the third thing after we finish this is that if time permits we will definitely do some kind of practical applications or implementation using python practical implementation using python so all the things we are going to cover in today's session and again uh here the main focus is to understand things in depth intuitions and after this we again going to have quiz so the quiz will be there the quiz as usual the first price okay see whatever quiz happens right it will be happening based on today's topic okay first price goes with two thousand rupees inr okay second price the person will get 1500 rupees inr 1000 rupees inr and the third prize will basically be 1500 rupees inr okay so we are going to have something like this and then uh we're going to do it okay so let's without wasting any time let's go ahead and let's try to quickly start the session and uh understand some things okay so obviously in our previous session now we're going to basically start with something called as word embeddings okay i think i had to talk about word and makings in the initial you know in in the initial class itself but word embedding is a technique uh where i can definitely say that it is a technique it is a technique which converts word into vectors word into vectors okay in word embedding techniques uh if i really want to categorize you know so if if i definitely write like word embeddings over here usually techniques whatever we learn inside this are mainly divided based on two types one is whatever things are there here i can basically say count or frequency based on frequency of words we can definitely implement word embeddings and second by using by using deep learning trained models deep learning trained models okay so usually word embedding techniques can be learnt in this way now if i talk with respect to count and frequency obviously we have discussed many things like bag of words tf idf right back of what's the tf idf we have already discussed and we understood how to do it third one one hot encoding right this also things we have actually seen right now if i talk about deep learning tree and train models so here is where word to vect will come right both the types of word to back specifically if i try to divide these types that is first type is nothing but c b o w this is nothing but continuous bag bag of words and the other one is something called as skip grams so here these are nothing but pre-trained models okay and we'll try to understand the architecture how this training will actually happen and again the prerequisite is to understand at least a nnn okay if you really want to understand this a n optimizes loss function and all if you really want to understand this okay uh then we also have different different kind of word to work you know average word to work and all which we'll be discussing as we go ahead okay in deep learning whenever i really want to use this word to wack we have something called as embedding layers okay embedding layer so we basically create this layers and this is specifically used for converting text into vectors okay i hope everybody is clear till now uh can i get a quick yes i hope uh some symbols some smiley something so that uh you're able to understand right so based on this uv basically you you need to know this if you are actually learning nlp from scratch or from basics okay uh and i hope uh you got an idea about what exactly word embedding is because in an interview they may ask you different kind of questions related to word embedding so you should be able to answer it now let's go ahead and let's talk about word to work okay i'll give you an idea about what exactly is word to wake okay word to wick okay okay perfect many people are happy hit like yes if you also want to do some super chat thanks do it so that i can distribute it to the people you know out there through this kind of quizzes that i usually keep for five thousand rupees from now onwards i'll take this live session and 5000 rupees is the quiz that i'll try to give it to everyone now uh one very important thing is in word to work is something called as feature representation okay feature representation let me give you a very simple example okay let's say i have a vocabulary of features vocabulary of features okay and each and every feature we will try to denote it through a vectors okay each and every feature will try to denote it through a vectors now forget about this vocabulary also let's say that i have some words okay and let me define some words over here so that you'll be able to understand let's say i have feature one feature two feature three feature four feature file like this i have a lot of features and this is a part of vocabulary okay i hope you know what is vocabulary okay let's say my feature one is something like boy my feature two is something like girl okay my feature three is something like king then you have queen then you have let's say apple and let's say one more feature you have something called as mango let's say this many number of features you actually have now word to weck basically says that and and let's understand from the previous methods that we did in bag of words and tf idf some of the issues that we know in bag of words and tafidf is that semantic meaning is not captured right semantic meaning semantic meaning basically means similar words you know words that almost have the same meaning let me tell you an example if i say honest and good right this both can be a similar word right semantic word right um there may be also some words like uh good better right both will be almost a similar kind of word i also say it as a synonym word there is some semantic relationship between these two right so whatever vectors that should be getting generated both this word should have a similar kind of vectors right and this thing was missing in bag of words and tf idf this is one of the major issue okay the second issue is that whenever we are trying to create the vectors with the help of bag of words or tf idf there was generating a sparse matrix what was the problem in sparse matrix many numbers were zeros or ones or using tfidf there were some kind of decimal values right something like 2.23.24 and all okay this was a very big problem and because of this what was happening there were huge dimensions that were created huge dimensions that were created that basically means we were having many vectors that was getting created now if there are many vectors obviously processing model training you know it will take time and it leads to overfitting also since you have lot many number of zeros and very less number of ones right so this was a major problem this all problems will be actually solved by work to work uh how it is going to solve that is what i'm trying to explain you now okay and i hope this is entirely covered in the previous class whatever i am actually talking about right now if i talk about word to work how it is solving this specific problem in word to back what we do is that yes for every word we try to create vectors but always remember this vectors that is getting created it will be of limited dimensions limited dimensions limited dimension this is the first point okay so i i'm just going to write again what is the first point the first point is that when what to when we are using word to work if you are creating features or when if you are creating vectors or over here it will be of limited dimensions limited dimension let's say from 100 dimensions or 200 dimension or 300 dimension within that many number of dimensions you will be able to represent the entire world the second thing is that the sparsity it will try to reduce sparsity is reduced in this now when i say sparsity is reduced that basically means uh you will not find many zeros or ones you'll be finding a vector which will be almost filled with different different values okay so definitely sparsity will be reduced apart from this whatever vectors it is actually getting created you know it is making sure that the semantic meaning is maintained semantic meaning is maintained what does this basically mean if i probably use these words like honest and good and convert into vectors with the help of word to back then we are going to get a vectors which are very near to each other okay very very near to each other so that vectors if i'm trying to subtract it you'll be able to see that the difference will be very similar difference will be very less or i can also say that suppose i have these two vectors which is over here and which is over here let's say okay based on this distance we'll be able to find out how similar it is okay so this will be represented by a different vector this will be represented by a different vector so one of the famous example that you have actually seen right uh i hope everybody have seen somewhere or the other way right if we if we take the vector of king subtract it with uh man and if i try to probably or if you if you say like this king minus man plus women what answer it is going to give you if i sub use this vectors and subtract it in this way i'm going to get queen okay so this is how we are going to do this and i'll show you how the subtraction will happen and how things will be represented but definitely if i try to apply all these vectors in this specific way i am going to get the most familiar word that is related to queen okay how it is going to happen i'll just explain you now let me rob this now let's understand how these things are getting solved like limited dimensions how it is coming up how sparsity will be reduced how semantic meaning will be maintained and all will try to understand okay now in word to back what we do is that for every word we represent it through some features or here i've specifically written feature representation now how let's say i've trained a model okay and this time every word this these are my words right these are my features words boy girl king queen apple mango let's say i'm going to represent this by some number of features okay some number of features okay so what kind of features will be there let's see okay uh let me give you a very good example uh by one simple example we are just going to generate some new features over here let's say there will be something called a gender gender is one of the feature let's say there is another feature that is going to be called as royal let's say there is one more feature which is called as age let's say there is one more feature which is called as food like this let's say we have some 300 dimension features okay why 300 dimension you can create any number of dimension but understand with the help of this many dimensions right we'll be able to represent this word in an efficient way who is creating this particular feature uh the word to work algorithm will be creating it is very difficult to know what this specific feature is okay what this specific feature is because here we have a lot of pre-trained models for that google has actually done it with the help of three billion words they have actually trained this kind of model which i'm again going to show you in some time okay but just understand just for your understanding sake i've written some feature representation and we are going to relate each and every feature with this okay let's say uh now boy and gender okay if i if i try to find out the relation between these two words let's say i'm giving a value of something like minus one over here okay let's say girl and girl and gender okay if boys minus one then obviously girl will be one because it will be the opposite gender right let's say king and queen okay so here also we'll be assigning some kind of vectors let's say we are going to assign some kind of vectors how this vector is getting assigned i'll explain you just in some time let's say it is minus 0.93 this is 0.93 okay and always remember since these are opposite genders so you'll be having this minus or plus i'm just giving you as an example over here okay if i try to see apple and gender there is no relation between apple and gender so obviously this value will be somewhere zero okay mangoes let's say that the value somewhere between point one point two something like that okay now similarly if i go with the next with the next feature royal and boy obviously i cannot say that you are a royal boy okay so hardly there will be any kind of feature representation with respect to this let's say that the value that is getting assigned a vector that is getting assigned is 0.01 okay based on this importance and obviously this will get this will be a trained model okay which will be assigning this kind of vectors so let's say royal and girl and again there will be a different value like point zero two let's say king and royal obviously king royal king we basically say right so here the value may be 0.95 here the value may be 0.96 okay here you can see that this two these two words are almost similar right that basically means we are able to capture the semantic information right related information you can see related words are there okay related words are there i'll talk about how we can create these vectors and all and how this model is basically getting trained but here you can see wherever they are related right those vectors will be almost same okay now similarly if i talk about apple and royal hardly there will be any relationship so i'm just writing uh minus point zero point zero one you know some vectors over here let's say age and boy okay there may be some kind of relation or not i'm just putting some values over here let's say if it is not at all related both these words okay boy and age okay hardly you can't find anything king and age and obviously there will be some kind of relation right we usually say that okay the king whenever you are sinking the age may be a little bit high so there will be some kind of relation let's say 0.7 let's say with respect to queen it will be 0.6 apple and mango age can play a very important factor right because if it is of maximum age you know apple may get you know uh apple also has some kind of lifetime right if you just keep it like that for five days six days it loses nourishment right or nutrients right so here i can basically write 0.95.96 something like this okay so in short whatever things are basically getting uh assigned over here right whatever things are basically getting assigned over here here you can see that each and every word is represented by some kind of vectors okay so this is the vector with respect to boy what is the dimension let's say 300 dimension because we have trained our model which will be able to create this kind of numbers right so again here you have girl and with respect to girl you have this kind of dimensions right this kind of vectors right similarly like this you have this kind of vectors right right so i hope everybody is able to understand this this representation is basically created by the model this feature representation is basically created by the model okay i hope everybody is able to understand till here guys yes i hope everybody is able to understand till here can i get a quick yes if you are able to understand till here yeah uh i hope this is pretty much easy and you are able to understand till here at least getting an idea about all the things with respect to whatever things we are actually discussing right okay how these values are getting assigned i'll just talk about it how the model will get trained and all everything but i hope you are getting an idea about it now let's see one thing over here obviously we have 300 dimension let's say that there is one word okay one word which is called as boy okay let's say boy is let's consider that it is represented by two vectors which looks like this or let me give you an example forget about 300 dimensions okay how these numbers are coming i will just talk about it okay just give me some time i'll show you how this entire model will get trained you know this value will be generated by the model okay whatever model pre-trained models that we are using or any model that we are training from scratch okay so just give me a sometime it'll it'll all make sense now let's say king is represented by two dimension vectors which looks like this okay let's say it is 0.96 0.95 okay queen may be represented by some dimensions like this 0.96 0.95 let's say let's say we have we are assigning these vectors how these vectors will get created i'll just show you in some time let's say man is created by a vector which looks uh something like 0.95 0.98 right and uh let's say human is created by a vector which is like 0.94 and 0.96 minus okay now tell me if i'm representing this particular words like this in this kind of vectors if i do this operation king minus man plus queen or plus human what will i get what will i get that basically means i am just i'm just subtracting this dimensions with this dimension and i'm adding these dimensions and obviously my answer will be queen right because whatever vectors i will be getting out of the result right whatever vectors i'll be getting out of the result you'll be seeing that that vector will be matching this right it will be matching this right how it will be matching let's see uh there is something called as cosine similarity cosine similarity i hope i have explained you this also right in cosine similarity what we do is that let's say there is two words that is represented in this way okay if i really want to calculate the distance between these two points i can definitely use something called as euclidean distance okay i can definitely use euclidean distance right or i can also use magnet and distance other than that if i really want to calculate the distance i can also use cosine similarity so my distance will be nothing but 1 minus cosine similarity now what is cosine similarity if i want to explain about cosine similarity it is nothing but we will just try to find out what is cos theta theta is nothing but the angle between these two points okay the angle between these two points let's say if the angle is 45 degree so if i want to find out the cosine similarity between this point and this point okay what we'll do we will just write cos 45 what is cos 45 anybody let me just google it over here what is cost 45 so i hope everybody have studied some amount of maths it is nothing but 0.7071 0.707 now 0.7071 basically means in this particular case okay it is nothing but this point zero seven one is nothing but one by root two right i guess yeah one by root two yeah now this point zero seven seven zero seven one is basically saying how much percentage these two points are similar right now if i really want to find out the distance between this i will just write 1 minus 0.7071 so it will be probably approximately equal to 0.0.29 right let's say okay i'm actually getting so it is saying that okay the distance between this is 0.29 that basically means these are very like more the value towards the one the more similar this point is now suppose if i may have another points which may look something like this let's say there is one point over here there is one point over here there is one point over here now what is the distance between these two because i need to find out this distance so this is 90 degree so if i want to find out the cosine similarity it will be cos 90 what is cos 90 0 right so the distance since since it is cos 0 then what is the distance then distance will be 1 minus cos similarity because similarity is 0 sorry cosine similarity then it will be one that basically means these two are like completely separate things right completely separate things now suppose if i have one more point which looks like this if i try to find out the cosine similarity between these two it will obviously what will be the cosine similarity over here cos 0 is nothing but 1. so can i say these two points will be similar yeah i can say definitely these two points will be similar i can also extend this line to this axis and this axis here if all the points are here then obviously my similarity will be almost 1 here it will be 0 here it will be minus 1 and here again it will be 0 and again here it will be 1. so if we try to calculate a distance it is always better that if it is coming near minus 1 that basically means if there is one point over here and one point over here we need to find out how similar this is it will be coming towards the minus sign that basically means from it is from the opposite of one right so this actually helps me is to find out how good or how similar the points are i hope you are able to understand so similarly this vectors we can also do this kind of operations to find out the similar semantic meaning is it clear everybody yes yes yes yeah value towards zero more similar value towards one not similar perfect prashanthas i'm amazing i think you are getting the idea about it okay everybody clear yes with respect to vectors and all and all now the question arises chris how do this word to vec actually get trained how is the this model getting created okay how is this model getting created and can you show me an example with respect to some practical application the answer is yes i'll try to show it to you okay so uh should i show you one example first or should i first of all explain you the architecture and then probably go to the example with respect to practical implementation tell me should i should i basically uh show you one practical application once example once or should i first of all show you the architecture of two types that is cb ov ow or and uh yes architecture is good to know right perfect good i was expecting this answers from you amazing now let's go and understand word to vec the first type is something called as cbow cbow is nothing but continuous bag of words so we are going to understand continuous bag of words in an amazing way okay what exactly is continuous bag of words and how does word to wack model gets trained you know let's say i have a corpus okay i have a corpus okay corpus basically means a group of sentences now this will be my data set let's say this is my data set with respect to the training part training data set and using this data set i really need to create uh this word to wack model okay this word to back model okay now if i really want to explain you this uh things let's say i let let's take a simple sentence i'll say that krish channel is related to is related to what it is related to guys what it is related to it is related to data science okay now let's say i want to train a word to work model in this specific sentence okay word to vect i want to train okay now the first thing over here is that if i am using cpc continuous bag of words model here first thing is that we need to decide our window size window size okay let's say my window size is 5. okay everybody got it my window size is actually five now what is this window size this window size will be helpful for creating our training data okay so now if i go ahead and make you understand how does the training data get created so here is my training data training data so obviously with respect to training data you have two important things one is your independent feature and one is your output feature okay now if i'm using a window size of five let's say so what what it will be is that i will take five words one two three four five okay this is the five words now what is the center word in this guys what is the center word in this what is the center word in this it is nothing but is right is this my center word now this word i will basically say as my output word output or target okay and this word i will say it is a context word why i'm saying context because i really need to bring up some kind of context into it okay context into it uh so that it will be context basically semantics some some kind of semantic meaning i really want to bring so i'm basically writing it as context word okay very simple now since we are actually creating uh you know some data sets with respect to independent and dependent features now is will go as my output feature okay as my output feature see with respect to my first data set then over here just a second so with respect to my independent feature i will be coming over here now my first independent feature will have words like krish channel this is the context word okay context word and then you will be having from the right hand side related and you'll be having two everybody understood this how i got the first sentence i'm taking the two context word okay krish channel related to so this all i have actually put it in the first sentence so this will be my input features and this will be my output feature everybody clear yes yes or no yes or no everyone yes yes yes yes yes guys you can you can definitely change window size you can also keep it to three you can keep it to four you can keep it not four but always make sure that you keep it as an odd number so that you have the left context and the right context almost similar then i will go over here going to the next word now what i'll do i will slide my window by one more step so now one two three four five what is the center word now okay why middle word is taken because always we need to have left context and right context must be same okay because we say that our input should always be same right that is the reason okay now tell me what is the center word related to this next slide which is the center word it is nothing but related so my output will now be related and my second sentence will have what channel is to data okay what is the usual window size it is a hyper parameter the more the bigger window size the more better model we can get okay so uh don't try to take it as an even number so again i told you right the left context and the right context should be same okay now this is my channel is two data is my input features and this is my output feature okay then again after this i will move one more step towards the right so if i move to one more step towards the right so how much it is this is the new context right now which is the center word center word is obviously two right so my next input will have 2 and here i will basically be having is related okay data size everybody clear how do we do uh this independent and the output feature how we have actually created it perfect now i cannot slide more forward because i'm not going to get the five sentence but usually in a bigger problem statement if google is creating this kind of model they'll be having a billions and billions of words and sentences okay they're where they really they can take the entire dictionary or take shakespeare book and probably try to create this kind of amazing thing okay now see what is the aim of word to back what to wake basically will take this text and based on this output it will try to generate vectors right this is what we are doing we really want to take the text and generate into vectors and the vector should be in such a way that the output features that you will be seeing it should be almost similar semantic meaning should get captured semantic meaning should get captured but still we are into words where is vectors chris you did not talk about vectors okay now here comes a simple uh bag of word representation okay simple bag of word representation now tell me how many how many how many unique words are there in this vocabulary one two three four five six seven right if i want to represent krish how will i represent how will krish vector get represent krish how will krish vector get represent according to bag of words simple how how it will get represented it is nothing but one okay how it will get represented tell me someone it will be 1 0 0 0 0 0 0 right so all this will be there right what about the next word channel channel channel will be 1 1 sorry this will be 0 yes yes yes okay so this will be 1 0 0 0 0 0 okay then similarly if you have is then it will become 0 0 1 0 0 0 0 if it is related then it will be 0 0 1 0 1 0 0 i hope everybody knows this right this is based on bag of words that is the reason why i'm saying continuous bag of words so there is some reason why we are doing it okay now this data independent features along with the output feature along with this data representation will be given to our fully connected layer what is fully connected layer guys i have taken the community session sessions with respect to deep learning also right fully connected layer basically means a nn artificial neural network okay artificial neural net now in this artificial neural network the input how many inputs we have over here one two three four right all the time we have input s4 yes or no yes or no everybody see input is four input is four input is for every time input is four okay input is four input is 4 right so whenever input is 4 that basically means we are representing each word with vector of how many words 1 2 3 4 5 6 7 right 7 words so if input is 4 can i say that this is my seven vector one two three four five six one more seven this is one this is second let me draw this properly otherwise again people will get confused so this is my one let's say this is my first one two three four five six seven this is my word one okay word one is represented like krish is represented by the seven vectors right so in input i'll be having one word two word three word forward so i'm basically going to have how many number of input layers in the input i will be having this many number of nodes right one two three four five six seven then again i will be having this one two three four five six seven and one more word which is represented by seven i can have over here so i will be having this all one two three four five six seven everybody clear so this is my four words this this vector will represent word one this vector will represent word two this vector will represent word 3 this vector will represent word 4. everyone clear yes or no so in short in the first instance krish will get passed from here channel will get passed from here related will get passed from here 2 will get passed from here ok now coming to the hidden layer can you imagine how many nodes will be there in the hidden layer what is the window size window size is 5 right so window size is 5. so i'm going to basically have another input layer hidden layer which will be having five nodes okay five nodes this is not zero guys this is these are neurons hidden neurons this is input this is not zero okay this is not zero these are neurons if you don't know deep learning okay if you don't know deep learning then sorry okay that is the prerequisite okay these are not zeros these are neurons this is the input layer okay when i'm passing krish then this value that is going to get passed over here is nothing like this 1 0 0 0 0 0 0 if another word that is going to get passed like channel then it will become 0 1 0 0 0 0 0. understand this these are hidden neurons okay so i've basically written a circle okay it is not zero come on okay why five neurons in a hidden layer because window size is five what is the window size we have defined window size what we have defined window size over here five right so this will be five right now let's go to the output one output neuron now over here with respect to output each each word is represented output is only one word right output is one word right now when output is one word that basically is one robot is represented by seven vectors right so here i'm basically going to have the output layer which will be having seven nodes one two three four five six seven everybody clear and in in the output we will be using a layer which is called as soft max layer okay okay soft max layer everybody clear yes guys on the first day only i have explained you what is the prerequisite right harishma ashok [Music] pre-requisite on the first day right when i'm making a video announcement at that time only you'll be able to understand things okay fine okay so i hope everybody is able to now understand right now in this hidden layer now what will happen this is since this is a fully connected layer every points will get connected like this right every points will get connected like this like this like this right then this points will get connected like this so in short every things will get connected right this is how a nnn works right and similarly this this entire thing will get connected like this and this entire things will get connected like this right so everything will actually get connected like this and then this will get connected to this right now if i talk about matrix this will be 7 cross 5 right this will also be 7 cross 5 now why if it is connected that basically means we are going to just have uh weights initialized over here right why seven outputs because understand here i have one variable as an output ease right this is will be represented by a vectors right seven vectors so seven out seven nodes in the output okay seven nodes in the output clear every one yes yes okay perfect and again this will be seven cross 5 this will be 7 cross 5 okay okay this will be 7 cross 5 and here also i have something called as 5 cross 7 right 5 cross 7 because this is 5 and output is 7 right now very important thing how the training of this a n will happen we will initialize weights over here everyone will initialize weights over here so we will always have one hidden layer yes you will have only one hidden layer that can this can window size can get extended only there will be one hidden layer okay now over here you will be seeing that we will be initializing different weights will be passing over here then from here to here again weights will get initialized and here soft max layer is there so we'll be able to solve multi-class classification now what we can actually do over here is that see this everyone okay soon we come to this doing the forward propagation then loss is calculated how loss is calculated you know that if i pass let's say if i am passing this first input chris channel related to so it will pass krish will get passed over here channel will get passed over here related to will get pass over here then when we go over here we know what is the output output is basically is is is represented by what vector this is represented by 0 0 1 0 0 0 0 right so whenever the forward propagation will actually happen uh whenever the forward propagation will actually happen then what will happen it will go over here then whatever value is there this loss y minus y hat predicted will happen so this 2 will get computed the difference will get calculated and again we have to do the backward propagation by updating all the weights okay so this backward propagation will happen and then like this forward and for backward propagation will happen unless and until we don't get a we don't get a loss value very less okay now finally fine this is fine let's say this you know that right this first first value is krish right at the end of the day how do you represent how do you represent each and every word okay you know that this first node represents krish second node represents channel third node represent is related node represent is represented by fourth right so how do we know what will be the vector of this node after the training so it is very much simple guys whatever value because you know that right over here whatever 5 7 is there that basically means this 5 nodes will get connected to this right 2 3 4 5 right so that basically means this node that is representing trish will be given by five vectors and whatever value will be in this layers it will be assigned to this vectors let's say it is assigning 0.92 0.94 0.25 0.36 0.45 so this will be my five different values of vector since my window size is 5 it will be represented over here similarly for the second node which will be the 5 1 which will get connected to this right which will get connected to this 3 4 5 whichever is getting connected over here that will basically be represented as my five vectors for the next word similarly for the next word similarly for the next word similarly for the next one so every time each and every word will be represented by the five vectors because the window size is 5 okay okay why seven neurons in the output layer it is very simple because each and every word is represented by seven vectors this is represented by seven vectors okay i hope it is clear shalini okay each and every word is represented by seven vectors so it will be shown in seven output okay so those are probabilities values yes it can be probabilities value it will be the weights value that is assigned over here so where does embedding layer comes into embedding layer will come over here okay so this will be my embedding layer okay in short whatever weights are there inside this layers that will get basically assigned now tell me don't you feel that now whatever vectors i am getting from this training data set will have some kind of semantic meaning because it is getting trained and the weights are getting assigned with respect to that okay so that is with respect to continuous bag of words okay continuous bag of words and that is how it is basically represented over here okay is it always seven for cross five it need not be seven cross five again it depends on the window size you know we can and how to do the hyper parameter tuning we can also extend this windows right we can extend this windows to any number okay and that is how the entire training actually happens everybody clear now yeah if for the people who have not understood i think you need to revise you need to know the basics of a n okay because here optimizers here this is there and all okay everybody clear yes what is strategy to define hidden layer strategy is very simple whatever is the window size that will be the hidden layer with that many number of nodes that's it okay perfect now let's go to the next one so i hope everybody is clear with the cbov and this is how you train the model and this is how you get the vectors okay i'll show you how to train it from scratch also and probably in the next class we'll try to see this okay how word-to-work and cbi is different cbi is a type of word-to-work now let's talk about the another type which is called a skip gram in skip gram only things are what things are changing okay only two things are changing in skip gram what will happen this will be my input this column will be my input and this column will be my output that's it okay this column will be my input and this column will be my output so that basically means in this particular case my input will be this words like is then what is the next word related to okay is related to is related to okay window size can be defined through hyper parameter tuning okay and the output will be all these words that you have probably find out at chris channel related to krish channel related to okay similarly the next word will be channel is to do a data channel is to do top and to is nothing but is related data science okay is related data science okay now if i'm saying if google is creating a model of what to work with 300 dimensions can i say in the hidden layer we'll be having 300 hidden uh 300 neurons in this hidden layer yes or no if you say if google is creating a word 2x which has which is giving you output of 300 dimensions can i say the window size is 300 window size is 300 yes or no i can say it right so it will be present over here in the hidden layer it will be having that specific number okay okay everyone clear now you can actually create your own uh neural network again over here so neural network will be very simple over here if you you just have to reverse this neural network that's it if you reverse this neural network you will be getting ah for the skip gram okay perfect right so this was an example with respect to uh cbov and word to vec now let's go ahead towards practical everybody quite excited with respect to practical yes so let's go and do practicals and here is my practical everyone yeah so don't worry if you have not you don't have to write the code i will give you this entire code to you so that you can practice over there okay uh please focus over here the library that we are going to use is something called a gensim okay when do you skip gram and when to use cbov i would suggest see both are almost providing the same things uh okay you can use skip gram also you can use cpov but again if you have a bigger data set i would suggest go ahead and use c script gram otherwise use cpov okay okay perfect uh let's go ahead and first we are going to install gensim so uh everybody see to this and quickly uh don't write any code i will just try to show you and this code will be given okay then i'll be importing gensim and then uh you'll be seeing that from gensim. models import word to work and keyed vectors this two we have to basically import now understand one thing guys right now i'm first first try i'm going to use a pre-trained model pre-trained model uh google has actually created this pre-trained model which is called as what to wear google news 300 so that basically means google has trained with this google news text data which had more than 3 billion words and it has created this kind of word to work which gives you 300 dimensions as the output okay 300 dimensions as an output so here i will show you over here and by just using this code i can load this model from the internet it is somewhere around 1600 mb so it will probably take some time so i'm just going to download this but just make sure that you wait for some time for this downloading till then you can hit like and we will i'll show you once this model will get downloaded you'll be able to see this okay and again uh you can also come up with your own word to wake model but again who has more huge data than google right uh it is very difficult to come up with a better model other than that dac okay but i hope everybody is liking this session whatever things i have explained you today yes the output has 300 not the hidden layer hidden layer will have 300 neurons right window size of 300 yes it is 300 you can consider in that way i don't know about the input size but definitely you can understand okay window size you can basically say okay 300 and the middle word is taken and the context word is basically there right so it will probably take some time 167 mb uh and this is how it will happen just a bit let's wait for some time and we'll have much more information going on okay yes you can join the full stack data science course anytime so uh it's about understanding things guys because anyhow you're going to use some models to do all these things so not a big deal onto that we'll wait for some time till everybody everything's get downloaded 20 is there right hit like let's make it some like i know this is this is a very important session i can say but people are not interested to join can't help okay i can't force people to join but i will teach in this way and i hope your learning is quite good going on good right dl community is also going on live which dl community which dl community guys have you teach gpt3 i'll teach it we'll come to bird transformers rnn then also we'll be learning about embedding layer right what does cosine similarity gives how similar those two words of vectors are you know what's next to what w to vector tomorrow we'll be doing practicals related to it we'll we'll create our own word-to-work models so till here i think we have covered many things right so i i like all these things right this is so good to understand you know when you understand why it is getting used how it is getting used you know so it's quite amazing okay globals i'll complete it okay yeah yeah this 15 day sessions are enough for nlp interviews right but you have to create a lot of end-to-end projects at the end of the day see by using a deep learning model how you are able to quickly create and things right so let's wait for some time i think the model is yes i'm there if this much time it is taking for downloading what should i do right yes guys thank you video for the super chat guys yes please make sure that you do super chat and all so that i can invest this money on you all okay i can give this money directly to you by giving this kind of quizzes and all right it'll be amazing so can we see quickly in five minutes what we saw till now sure uh till now what all things we have actually discussed uh we discussed about word embeddings word to work cbo week skip gram we understood architecture uh we what was word embedding we have actually taken right what is feature representation how what to work actually creates this how is this feature actually created i showed you with the help of a data set example right so everything is basically getting created over here right yeah end-to-end projects also i'll do it guys now they say that so 54 is done probably another four to five minutes it will be completed okay anyhow let's wait once this is done uh you'll be able to see amazing things okay see i'll show you till then let that get downloaded so here you can see right when i write when i take a word of king and probably see the vectors this is nothing but 300 dimension words right 300 dimension words see this 300 dimension words right vector of king right similarly if i go and see next what is the vector of man here you will be getting 300 another dimensions okay so here also you will be able to see another dimensions okay uh there is also a method which is called as most similar most similar vectors okay so if i give man over here so what it will do internally it will convert that into vectors and it will show you what all cosine similarity or similar words are there with respect to man you can see human is there boys there teenager teenager girl girl and how much this basically shows you cosine similarity okay okay so here you can see that here you are actually being able to get it okay yes this is 300 window size that only i told you know 300 window size okay now see this if i try to see what is the most similar word to king right here cosine similarity is used internally with respect to vectors so i'm getting king again then queen monarch crown prince prince sultan ruler princess prince paris throne right is it amazing guys if you want to find out the cosine similarity also you can use another method which is called a similarity how man and king are similar it is saying that it is clear it is it is similar to 0.22 not correlation guys cosine similarity is completely different cosine similarity basically means angle between the vectors that you are trying to calculate okay so here you can basically see w v dot similarity with respect to html and programmer here you can see that if i am searching for program and html it is 0.25 related right then here also you can see uh word to cloud see this is word to cloud okay word to cloud if i if i convert this dimensions into two two dimensions and probably plot it in this graph you will be able to see this kind of word to cloud okay i think you have seen this in mentimeter also right so let it download and uh let me just show you uh it will make sense so another 17 will you teach how to build models using this feature tomorrow yes vic tomorrow i will be teaching that also we will train our own word to back okay so can we limit the window size in virtual pre-built model yes we can also do that but you have to train your own model right but always understand if i keep the window size more right we will be able to get amazing models we'll be able to see that difference right that is a kind of hyper parameter again we can play with it it should not be over fitting but yes it should give you a good words till then please go ahead and subscribe krishnak hindi channel because i also upload data science videos over there guys krishna in the channel just search for it till the model gets uploaded okay you have to give me opportunities then only i will be able to give amazing giveaways for you right tomorrow we are going to have a session there also i have to give you 5000 rupees right so it'll be amazing search for krishnak in the channel and subscribe subscribe push okay code i will give you in the dashboard you will be able to see it i will give you this link you want this link let me give you this link then you can thank you guys already subscribe people are saying thank you okay done 100 about to get completed some ending is things are happening okay nice till it is taking time come on come on come on come on my friend happen happen quickly can you mention the required specs of laptop if i i7 google collab is there you can definitely use it i have an amazing desktop altogether that paid membership is just to support me you know because of uploading videos and all i upload a lot of videos so okay let's see it is taking time i don't understand why it is taking time the file was okay so but i hope you have understood what i'm actually trying to do and if you do this kind of computations also right w of v king minus w v man plus w if you human you'll be able to get the answers which is like queen and all also you'll be able to get it okay it is taking time i don't know why it is taking time but something is happening over here let's keep this ready i don't know i'll just reload it let's say okay again i think it will it will cry and it will take time it's okay guys it shows busy anyhow let me just show you in tomorrow's session if possible okay and we'll try to do it uh it's anyhow one hour is done so let's do one thing is that let's try to participate in quiz till then and this is not working but anyhow let me just do it tomorrow if possible okay no no everything is correct because i ran this and all okay over here i think it is showing some problem with think i have opened google collab somewhere else hey what ran just a second now i think it ran i hope so it ran yay hooray okay it's running perfect let's go ahead so here you can see i have taken vector uh underscore king okay and here you can probably see all these things okay now you have all these 300 vectors okay the code got deleted okay so if i write a w v see w v is my model okay if i write w v dot let's see there is something called a similarity similarity similarities there similar by word is there if i write similarity with some word like uh cricket if i execute let's see similarity is missing one positional argument okay just a second not a problem i will fix this issue just a second guys uh just a second okay almost most similar most similar let's see with respect to cricket what all things you actually get there's some inbuilt functions which you can basically use most similar to cricket but all the time it will be giving you 300 dimensions so see for cricket it is like cricketing cricketers test cricket 20 20 cricket this this this is there okay and uh if i again use wv dot most similar to happy okay too happy again you will be getting some similar words like glad please spectate now see you can find out the semantic meaning over here right the main semantic meaning you are able to uh get over here right so this is quite amazing okay there is also something called as similarity so here i'm going to basically write w dot uh most similarity or similarity similarities then similarity is there i can basically also compare two words and find out how similar it is cricket let's say and here i'm going to write sports okay so here you can see it is 40 similar so yes obviously cricket is a sport i can also use hockey hockey right i can also use so it is 53 right and i can also write something like this let's see wv of king okay minus wp of queen oh quick man plus wv off what is uh wv of women right singular it is right so let's make it so this will be my vectors okay so here you can see that again this vector will be 300 dimensions right so if i probably see this this will be 300 dimensions and now what i can do is that i can basically write more similar to this vector so here you will be able to see this output more similar i think to this vector it will work i guess okay vector i think you have to give a word i think uh there is also something like let's see whether there is something like models or something like that just give me a second i'm just checking out the documentation what function it will be present over here okay most similar it is and here we have to give in the form of list then i think we'll be able to get it so you can see i'm getting king queen monarch princess crown prince so it is being able to capture it easily right so yeah uh in tomorrow's session we will try to see uh some more things you know related to it and uh let's see uh how things will go ahead tomorrow we'll be doing some other problem statements so now without wasting any time okay i will just share this please make sure that you all these materials will be shared okay in the description you have that entire community dashboard link so let me go ahead and let me quickly start this present okay so first thing is that you have to follow me on instagram so that i can get your id card and you can basically start participating right i will be taking live sessions yes definitely on computer vision also one or the other day so quickly go ahead uh make sure that you follow me on instagram keep your id card ready so that you can send me your id card if you are the winner right so everybody uh you can follow me so that uh in the instagram so that i can actually see your qr code is not visible okay go to mentee.com now if possible go to mentee.com or should i give you the link okay the link link is given in the chat okay go go to the link that is given over there right don't forget to follow me on instagram then only i will be able to give you go to krishnak zero six follow over there then only you will be able to send your id card and i'll be able to verify okay please do that specific thing before we start the quiz now guys one one super cool assignment for you is that i would suggest try to use any kind of data sets and try to see i try to apply the word to vec that is actually present okay everyone quickly get into it [Music] and i've seen a lot of assignments from people who have submitted yesterday it was quite good that is very nice please make sure that you submit assignments and you participate in things right so please make sure that you follow me over there in insta so that i will be able to see you okay another one minute and then i'll start the session you can give any id card not a problem so that and also make sure at the end of the day whatever rank you are getting okay if you are in the first top three take that screenshot and send it to me okay perfect uh 198 have joined come on quickly guys 350 people are watching hit like make it 300 okay okay till then i will add the dashboard link in the description of this particular video the community dashboard you can actually check it out just a second i will add the community dashboard link so that you will be able to find the materials there okay uh community courses this is nlp community okay nlp community link i will try to add it in the description all materials will be added in the below dashboard enroll for free you don't have to pay anything over here enroll for free so you'll be able to find out the dashboard link in the description of this particular video uh i've updated it please check it out quickly right check it out guys quickly reload the page just reload the page and check it out quickly reload the page and just give me a confirmation whether you have got the dashboard link or not okay so just give me a confirmation whether you have got the link or not guys just reload the page quickly and in that dashboard we will upload it all the materials within some hours okay okay enroll in that dashboard and you will be able to get it okay okay 231 uh i think other people are not interested let me go to the next step now everybody joined or still joining everybody joined or still joining joined shall i continue shall i start okay go which city do you belong to go and write the city name mumbai pune jaipur word to cloud is happening i showed you this word to cloud right just a while back this is again a concept of what to act okay hyderabad bangalore pune come on come on bangalore is the highest right now hyderabad pune noida chennai any people from outside india i don't see many of you out there okay dusseldorf is there right okay join join join guys 300 people are joining shall we go ahead so i hope everybody has written it let me go ahead and best of luck to everyone and here we go so the first quiz in your screen is going to come in another five seconds but here you can see you have got your own avatar please make sure that you write your name and here we go i'm pressing enter so coming the first question on your screen in word embeddings a vocabulary of 500 words will have 500 dimension vectors true or false three two one a very simple question to start with okay perfect let's go with the next question now coming to the second two question on your screen now tell the one hot encoding vector for the word in vocabulary krish subscribe chris channel when binary is true quickly guys come on perfect one zero one zero many people have said it right because krish will become one subscribe will become zero crash will become one and channel will become zero perfect everyone this is amazing let's go to the next question quickly before that let's go to the leaderboard who is in the first position raja varun second maneeshwi third adarsh i think absolutely people are doing fine okay roger varun is the first manish is there okay and let's go to the next question everyone question number three what to vek can be a model that can be trained from scratch pre-trained model both none i hope this should be a very easy answer for every one of you out there yeah i guess many people are participating in an amazing way so time's up the answer should be both uh this i have already explained you so 241 people have said right 34 people said pre-trained model i think they were not listening to the entire lecture what i said but yes it can be trained from scratch and it can be a pre-trained model and it can be both so coming to the next question which is your fourth question on your screen and here we go cbow and script grant can be trained with convolution neural network recurrent neural network fully connected layers like riann and lhtm rnn another 20 seconds to answer this question quickly 10 9 8 7 6 5 4 3 2 1. so amazing people have said it right fully connected layers like a n and again i've explained you just now just a while back okay cb and w can be trained with okay neural network will not give you a good error good accuracy okay so again let's see who is first now raja varun is still in the first other second and more so in third okay perfect let's go to the next question here we go with respect to question five and i think this is uh what to vek is good at okay capturing semantic meaning image classification reducing sparsity and reducing dimension both 1 and 3. time's up so both one and three is the right answer people have missed reducing sparsity and reducing dimensions i've already explained you over there clearly 173 people have heard it right and some may be in very hurry buddy to answer the question okay now let's start this still raja varun is in the top i think he can be a winner right now okay we don't know because now we are going to have some python sessions okay python programming is also going included so here we go with respect to your python question what is the output of this code what is the output of this code guys come on so the third output is right the program will execute with error please try it out you cannot append two values after one time okay you can you have to put it in a collections okay so definitely for the people who have said it right that's amazing for the people who have said it wrong you're confused okay let's go to the next question the final question of the day and again it is a python programming language here we go what is the output of the python code 25 125 2500 25 150 25 625 i hope many people should say be right amazing okay so 205 is the right answer now let's go to the leaderboard quickly and see who's first second third you can drop me a message along with the screenshot everything okay raja varun i think he's the winner i guess so raja varun is the first winner then we have animal soy then we have shubham kumar you can directly ping me in the insta guys quickly ping me in the insta okay and more sign done you can send your uh photo and mall okay and mole has been the second winner two times he has actually won okay two times he's basically coming to participate in quizzes nice so again i have to send him 1500 rupees let's see okay same winner quiz winner 1500 here we go other people uh you can send your swan mole i've already sent it what about other people [Music] please drop me a message shubham sarkar okay subhan sarkar please drop me your id please drop me id shubham shubham is in the third reward so it was really hard competition shubham also gets 1500 quickly i will send him i know guys uh rank will get improved don't worry what about raja varun rajavaron message i did not get so shubham also money has been sent message i did not get raja if you are not in instagram please drop me a mail at krishnak the right 0600 dot com okay shubham sarkar says got it [Music] perfect raja you can drop me a message or mail if you don't have if you don't have a instagram okay increase to 10 questions sure from next time i'll increase it and i'll put random random things now okay i'll not keep quiet at one thing okay please make sure that you follow me on instagram uh so that you know you'll be able to participate in a better way okay level this is simple uh i will try to keep more difficult okay see this was some of them were tricky okay some of them were tricky so you cannot just say that it was very easy okay but definitely from next time i'll do it okay so i hope everybody liked this session please make sure that you subscribe both the channels krishna krishnakindi channel and yes this was it from my side i'll see you all in the next session keep on rocking keep on learning and yeah i'll see you all in the next session till then bye bye take care thank you guys okay and uh raja varun what you can do is that you can just drop me a message either through mail or something i will try to give it to you okay if you are not in instagram it's fine okay so thank you everyone bye bye take care