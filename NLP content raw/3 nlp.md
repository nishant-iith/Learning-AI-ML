# TF-IDF With Practical Application NLP

hello guys i hope everybody is able to hear me out yes can you give me a quick yes if everyone is able to hear me out so that we can quickly start the session yes i guess everybody is able to hear me out can i get a quick confirmation okay perfect so hit like before we start we have i i joined five minutes earlier so we'll wait for uh you know wait for some time so that everybody joins and then we can probably start okay so i hope everybody is able to you know understand whatever i have taken in the past two classes today we'll be continuing the day three okay and uh again we are going to learn something in nlp okay so i've seen some people doing cheating in the quizzes you know they are editing and they're saying they're editing in the console and basically writing okay they are first and they're sending me the screenshot in our instagram so this time after the quiz competition you also have to send your id proof okay once any id is fine like pan card just send the photo along with that send the rank that you basically have okay but i saw many people doing extremely you know you know for money anybody can do anything you know so i have to really take this precaution now they don't want to even participate properly and they want to say that okay i have done this that okay perfect uh so i think we have covered a lot of things initially and uh we have learned so many things in the past two classes and what we will do is that today we will try to learn bag of words back of words is almost completed tfidf will try to learn and again with by saying some good examples i'll try to explain you but before that i really want to announce one batch that is going to start from tomorrow you know that is the full stack data analytics batch again the link is in the info given in the description so we are basically starting the data analytics batch let me just share the screen and show it to you so that you do not miss this so this is the full stack data analytics bootcamp um that is going to go and the cost of the course is somewhere around 4 000 rupees and this is starting from tomorrow every saturday and sunday the classes will be between 8 pm to 11 pm okay and uh yeah so you can if you are interested you can join this batch and again if you really want some discount you can use my coupon code christian on top of it to get 10 percent discount okay so in this we are trying to focus on if you're planning to become a data analyst and also this course will be super beneficial for you we have three mentors krishna jayanth and sudan shukumar sudanshu and myself will be the main mentors and uh you know remaining all i hope uh you know but there will be doubt clearing sessions skype support and many more things and this comes with placement uh job assistance program now this job assistance program through i neuron job portals you'll be able to apply for jobs and you'll be able to participate okay so yeah all the information is given in the description of this particular video you can definitely check it out and if you're interested you can also go ahead with the course now let's go back and let's start learning so today is the day three so day three of nlp okay so agenda will basically be again i'm just going to revise you bag of words which was left in the previous class or anyhow i had also shown you practical so bag of words is one okay and the second one that we are probably going to learn is something called htf idf third practical implementation practical implementation with the help of python and nltk libraries and after that we are also going to have a quiz okay so quiz again the first price so winner will be somewhere around 2000 rupees second one is 1500 and the third one is 1500 okay so all these things are there and uh yeah uh let's see whether it will take one hour to complete i guess it may take one hour to complete and we'll try to complete it as soon as possible okay and then again i have a quiz for you today again related to nlp and some part of machine learning will be there and uh if you are new to this channel please make sure you subscribe the channel and don't forget to subscribe krishna in the youtube channel also because they also have started uploading data science in hindi okay so if you can give me a quick confirmation so that we can start just giving a quick confirmation so that uh we can start learning things and that'll be quite amazing right so just give me a quick confirmation should we go ahead if you want to share to your friends please do share and uh once everybody's ready we'll start participating right okay perfect just give me a second yeah let's go and let's start learning things okay so uh yesterday i i mean day before yesterday i started regarding bag of words so again i'm going to revise quickly so bag of words was again a technique to convert text into vectors so if we have any specific text and if you really want to convert this into vectors so we can definitely use bag of words and inside this suppose if i take an example of some sentences or some documents let's say my sentence one is like he's a good boy okay he's a good boy he's a good boy sentence too can be uh she is a good girl okay and sentence three can be boys and girls or i can write boy and girl are good right so usually in this particular scenario first thing is that what we do uh whenever we want to convert this first of all we try to apply stop words okay what it will be stop words will actually help you to remove all this unnecessary word like he is a she is right and r right along with this will also be lowering the keyword so that we don't get repeated keywords in different different manner because if i write capital good and small good this both are same only right so we need to have a small letters so that we'll be able to understand the text over there okay so what we will be doing first we'll apply stop words and lowering of the words and then after we apply this what will happen is that we will be getting the output which will look something like this so my sentence one will now become something like this so after applying this my sentence one will he is a will get removed so we will be only left with good boy okay similarly sentence two will be left with good girl right good girl and sentence three will be left with something like girl sorry boy girl good okay so this is what we will be left with now if i really want to convert this into a bag of words there is a simple process that we are going to apply so this is my first step this is my second step now coming to the third step what will be happening in the third step is that we'll try to just compute the frequency of the words right so how many unique words and this is basically my vocabulary right vocabulary which are the important words that we are actually trying to focus on and try to create all these things right so in this vocabulary what we are actually going to do is that we are going to write the important words one word is good one word is boy and one more unique word that you can see is something like girl right and now with respect to this words we are going to just calculate the frequency okay what is the frequency how many good is present one two three three goods are present how many boys are present one two so if i go and count it it is two boy okay and then you can see over here as boy uh girl how many times are present two counts right so we are going to basically do this and then the next step after you compute the frequency with respect to each and every words this is what is going to happen to our futures nest so in the fourth step i will be defining like this feature one feature two feature three right and this will be sentence one sentence two and sentence three right so feature one obviously you know it is good okay feature two is boy and feature three is a girl right so with respect to this i'm just going to define the uh words over here that are actually present so good future one boy and girl now you may be telling that okay krish boy and girl are having the same frequency then what may happen is that based on alphabetical way this can be arranged okay so b usually comes before g so over here with respect to alpha bits you can see over here put it now with respect to sentence one we will go and see what words are actually present here you can see good and boy is actually present right so wherever good is present that will become one and remaining all will become zero boy is also present in the first sentence so here you can see one right then apart from that if you go to the sentence two here you can see good girl so good good is basically present over here one boy will be zero and girl will be one okay and in the last case you can see all this are one okay let's say let's say if if uh now what are the advantages and disadvantages which i which i'll try to repeat it again and this is that over here you can basically see that either you are getting ones and zeros okay now when i say ones and zeros over here most of the time this will like in this particular scenario you just have three words in the vocabulary now just imagine if you have 50 to 100 words or 100 vectors so this is basically getting converted into vectors right now just imagine if you have somewhere around 50 words in this then what will happen your sparse matrix will be will actually become very bigger so because of this the computation will not be very smoother you know it will take time for the computing part and all this is one of the issue the second issue is that out of vocabulary right out of vocabulary suppose if any word is there in this specific sentence right let's say there is also something like cat now cat is not there in the vocabulary now if you really want to represent in this particular vector what will happen this will get removed right cat will just get removed if we get cat in the new sentence right so over here you can basically see that okay there are a lot of problems and one more thing is that semantic meaning semantic meaning is missing now let me give you a very good example suppose i say the food is good and i may also have another word or sentence which says food is not good now you know that this to sentence is quite opposite right but if you try to convert this into vectors so my words will be something like this right i let me just remove uh let's say these two sentences are completely different now what i am actually going to say let's take the most important vectors like food good okay food good and there is like is not and the let's say this many unique words are there i think i'm not missed any now over here with respect to this first sentence you will be able to see that what will be my vectors that will be available over here wherever food is food is there so i'll write 1 good is there i'll write 0 okay is this there i will write 1 not is not there so i'll write it as 0. the is there so i'll write it as 1 whereas in the case of this sentence if i try to convert this into vectors okay what will happen over here so food is actually there as we are going to just write it as 1 good is there sorry good is also present over here okay good is also present away so i'm just going to make it as one here also in the second sentence good is there is is there not is also there and the is also there now you know that this two sentences completely opposite you know this is not at all similar not at all similar but if i take this vectors and probably try to calculate the cosine similarity okay try to calculate the cosine similarity so in this particular case what will happen is that they will try to say that this sentence are almost similar right because of these vectors so write exact semantic meaning is missing i'll not say write exact semantic i'll not say semantic in this particular case but at least you can see that the vectors are almost same over here even though this sentence are completely opposite right so this issue needs to be fixed and how do you fix this issue by using tf idf tf idf is nothing but term frequency and inverse document frequency okay so i'll discuss about term frequency and inverse document frequency but let's understand what did i actually say in this okay where you can see that this both the sentences are completely not similar at all okay but we are getting the vectors which are very much near to each other because if you just try to find out the cosine distance between these two vectors it will be very less because there is only one value that is getting changed okay now this is one of the issue one more issue let me tell you over here either you are getting ones and zeros let's let's check in this sentences here either you are getting ones and zeros right so you can understand that most of the words are getting the same value like one one because the value is either one or zeros right in this particular word also this word is also getting one and this is also getting one right so because of this what will happen is that the real semantic meaning is not getting captured in this entire sentence which word should i focus more because over here good is also there boy is also there and both are given the same importance right both are given the same importance now you may be telling krish why did you not remove the stop word okay let's remove the stop word remove this is okay and remove the okay but not don't remove it because knot can play a very important role but in stop words knot is actually present okay so you should create your own separate type of stop words so that this knot should be captured over there now if i remove this two also then what will happen this word or this sentence are almost similar because this is zero this is one only one by value or one vector is getting changed okay so i hope you are able to understand the issues with respect to this and here you can also see that when i try to find out the differences between these two words also they are also given the same value or weightage okay so we should try to give some importance to the specific words then how do we give it so for that we use something called as term frequency term frequency and inverse document frequency okay so let me just go ahead and explain you about from frequency and here we are going to discuss about inverse document frequency okay now in term frequency and inverse document frequency we focus on two important concepts one is term frequency okay and one is the inverse document frequency okay let me talk about this okay let me take the both the sentences which i have actually defined on the top so over here my sentence one after cleaning the text i have got it as good boy right in sentence two i have got something called as good girl okay in sentence three i've got something called as boy boy girl good right so these are my three sentences that you can definitely see over here now obviously you have seen how to do for the bag of words right but in this specific case now what we are going to do is that we are going to basically calculate the term frequency and inverse document frequency what exactly we are trying to solve see both this both these words are given the same weightage like one and one we should not give this specific weightage we should try to make sure that some of the words that are rarely present inside this document now in this particular case you can see that good is present in all the three sentences but boy is actually present in two sentences so boy should be given some weightage in sentence one and sentence three girl is present in sentence two and sentence three so girl should be given more weightage in sentence two and sentence three right whereas in good since it is present in all the sentences it should be given an equal weightage or less weightage okay so we will try to convert this into term frequency and invert the requirement frequency so in short whichever words are rarely present in the sentences we should try to give more weightage okay again remember this specific word more weightage to the rare words that are present in sentences or this entire documents okay so very much clearly i'm trying to explain you so please make sure that you understand okay and you will be able to understand this only when you clearly listen to me okay now in this specific sentences suppose let's say uh i have something like cat eats food okay something this is my first sentence bat eats food okay and probably krish eats food okay now here you can see that all the words are almost same right all the sentences are almost same now out of this which is the rare word cat bat and krish so this must be given higher weightage when we are creating the vectors okay when we are creating the vectors okay now always understand i will go with respect to making you understand how we can make sure that this rare words are getting captured this rare words will get captured by using this term frequency okay and this common words will get captured by inverse document frequency so when that both will get combined when we combine both of them then you will be able to see that we will be able to find out the most rare word that is present in the document so let me quickly write down the formula for term frequency and inverse document frequency and wait till the output is coming okay once you will be seeing the output there you will be able to see that how the differences will now come okay so first thing how to calculate term frequency so here i'm just going to write the formula for term frequency okay so the term frequency is basically divide defined as number of number of repetitions of words in sentence number of repetition of words and sentence divided by number of words in sentence okay again understand number of repetition of words in sentence divided by number of words in sentence okay just just just keep this formula in your mind and then inverse document frequency formula is log to the base e of something divided by something this is nothing but number of sentences divided by number of sentences containing the word by multiplying this two i will be getting tf idf okay now see this first let's go ahead and compute the term frequency so here i'm just going to basically write the term frequency okay so the term frequency over here that i am basically going to write is sentence one sentence two and sentence three let's say these are my sentence one sentence two and sentence three how many words i have with respect to the unique part here you can basically write good boy and girl okay so these are my words and this is my sentence one sentence two and sentence three now how do i calculate please everybody focus over here how do i calculate with respect to the term frequency go and see the term frequency formula number of repetition of word in sentence so let's go with sentence one okay now if i really want to find out the term frequency of good okay the term frequency of good so number of repetition of word in sentence so how many time good is present in this sentence okay how many times good is present in the sentence only one okay only one good is present over here so this will be one if i really want to find out the term frequency of good it will be one divided by how many number of words are there in the sentence so here you can basically say that they are two words okay so i hope everybody is able to understand okay so one by two okay so over here basically says that good how many number of words are repetition of words are present in the sentence only one time and how many total numbers of words are there in the sentence there are two times okay so tell me about uh so good in sentence one is basically present one by two okay with respect to term frequency if i try to compute for boy so in boy how many times number of words are present with respect to boy in sentence one only one so one divided by two since there are two number of words so here also i'm going to basically write 1 divided by 2 what about girl in sentence 1 what about girl in sentence 1 so over here you can see girl is not present so i'm just going to write 0 by 2 so here i'm short i'm going to get 0. did you understand clearly everyone yes yes or no everybody now let's go with respect to sentence two in sentence two good is present girl is present right so good and girl will be present so boy will be zero okay good is basically again one by two because how many words are there there is two words so one by two is basically print okay one by two is basically present okay so now with respect to sentence two let's see in sentence two boy is present no so i am going to get 0 okay okay 0 over here now with respect to sentence 2 let's see with respect to girl so girl is present so i am going to write 1 by 2 ok so this will be my 1 by 2 okay now with respect to sentence three good is present present so here i am going to basically write one by three because the total number of words that are present is three okay and similarly with respect to the boy over here again one by three again one by three so all these specific words are actually present with respect to sentence three okay so clear everyone yes if it is clear give hire like hit like heart symbol something to motivate me okay how do you motivate me okay you can also give donations if you want you know there is a thanks button in your youtube channel right so please give that if you can it'll be amazing anyhow i'm distributing in quiz so it is good that you give some thanks also okay okay now let's go and compute the inverse document frequency inverse document frequency okay okay now suppose if there was one more boy over here what would i what would bring my boy in sentence one can i say this should be like two by three yes if there is one more boy so this will basically be two by three i hope everybody is agreeing right yes so very simple so based on the number of words divided by the total number of words repetition of words with respect to that okay so right now it is one by two only okay now with respect to inverse document frequency now inverse document frequency here i'm just going to write two things one is words and one is idf okay so over here i have good okay what is the inverse document frequency with respect to good because words are how much boy and girl right now inverse document frequency this gets computed for every words this term frequency is getting computed for every sentences okay and this is going to be same throughout throughout the any problem statements okay so in that specific way okay so uh here we go okay thank you for the super chat bhupath mahara and uh kabbab so thank you thanks a lot okay so sometime this kind of motivation is required right now how do i calculate for good so good idf basically means number of sentences divided by number of sentences containing the word so how do i write over here i will basically write log to the base e how many number of sentences are there how many number of sentences are there there are three sentences right one two three okay so three sentences divided by how many number of sentences contains the word good so go and see good is present in sentence one good is present in sentence two good is present in sentence three so one two three so it will be log base two three divided by three so this is basically going to be zero okay so i hope everybody is clear now similarly with respect to boy boy how many number of sentences are there this is always going to be same log base 2 3 log base 2 3 only divided will be changing so how many sentences can consist of the word boy so here you can see 1 and 2 okay so 2 words are actually present so i'm just going to rub this 1 and 2 okay so log base t 3 by 2 so here it is going to become 3 by 2 and 3 by 2 okay i hope everybody is clear yes i hope you are able to understand this okay now what we are going to do now is that we are just going to multiply this two now see what will happen term frequency multiplied by inverse document frequency it will become okay okay now final one you will be able to see that what we will be getting first of all let me write down the feature names so it will be good boy and girl right so this will be my feature one feature two and feature three okay feature one feature two feature three thank you sharma for the super chat you are motivating me a lot i can now take at least four to five hours classes you know then i will be having sentence one sentence two and sentence three okay now see let's go with respect to sentence one okay sentence one what is the sentence one value this specific value okay sentence one okay now what we have to do for sentence one to find out good boy and girl we will be multiplying this value with the idf value okay with the idf value okay with the idf value that is the inverse document frequency okay now everybody see let's go ahead and multiply for good it will be for sentence one good see sentence one good right i'll just traverse this okay for sentence one good okay so good this will be 1 by 2 multiplied by 0. so can i write 0 over here okay can i write 0 over here then let's go with respect to boy so boy is over here 1 by 2 multiplied by log 3 by 2. so here i will just go and multiply log base e 3 by 2 then for the girl part 0 multiplied by this value so this is going to become 0 okay now similarly with respect to sentence 2 i'm just going to take this vectors and multiply with this specific vectors okay this specific vectors so here what i'm actually going to do i'm just going to multiply 1 by 2 by 0 so this is going to be for good it is going to be 0 then this 0 multiplied by this will be 0 last one will be 1 by 2 multiplied by log base e 3 by 2. but in case of sentence 3 we have all the values 1 by 3 1 by 3 1 by 3 so only the first one will be 0 since it is 0 over there and then we will multiply 1 by 3 log base e 1 by 3 log base e 3 by 2 and 1 by 3 log base e 3 by 2. now see one very super important thing over here if you see over here in this word in this sentences you can see good is a very common word right good is also present here also it is present and here also it is present now if you go and observe in these vectors do you see good is having all zero values right see with respect to z good it is zero because it is present in every sentence so it is not playing a very big role right it is not playing a very big role over here right it is just playing a normal role right but since good is present in all the sentences it is saying the vectors to not give importance to that specific good vector okay so we have just assigned it to 0. now boy is present rarely in two sentences over here over here and over here right over here so i have to give some importance in this two sentences to boy so here you can see some value is there with respect to boy and similarly in girl you can see that girl importance is there in two sentences this and this right so here you can basically see girl has some value over here right so now through this you can see that some amount of semantic information is getting captured right some amount of semantic information is getting captured now you can distinguish words that are frequently present and words that are only present in a specific sentence okay so here you can basically see all these things and this is how we convert this entire words into vectors at the end of the day you can see that there will be one output vector right because if you are solving a text classification or any kind of classification so this becomes our entire data set and we can train our model over here okay i hope everybody is able to understand yes yes yes can i get a quick yes yes everyone yes if you are liking the session please do hit like okay please do hit like okay perfect so this is how we can basically use term frequency and inverse document frequency to basically uh you know solve this uh convert words into a vectors but again if i talk about advantages advantages is that yes intuitive uh it is bit complex to you know implement tf idf but it is imputative and simple but if i talk about disadvantage here also if our vectors are very big you know sparsity issue is there sparsity issue is there yes it is reducing when compared to the bag of words but over here the main advantage is that word importance are getting captured word importance is getting captured is getting captured okay with respect to disadvantage there is again an issue of out of out of oov out of vocabulary this is not getting handled over here anyhow okay but just understand that here you can basically find something okay some issues definitely okay so disadvantage this is there uh out of vocabulary is also there okay word importance is getting captured this is the most important thing and because of this it can really perform a better role okay and again uh these are some of the things but yes in the upcoming classes we'll learn about word to work you know so it will be a definitely good situation okay so yes till here i hope everybody is understood shall we do some practicals okay so let's do some practicals anaconda uh let's do i'm just going to open my jupiter wait cd nlp jupiter notebook okay and probably i can you can do whatever i think everybody has got the materials right everybody has got the materials yeah from the dashboard that is given in the description of this particular video thank you subrat patra thank you so yesterday i think we were doing this right we did this right how many of you did uh got the materials yeah from the description of this particular video right so again i will quickly execute this if you have this material no ip1b is uploaded guys see i will show you go to the community go to the description over there and open this community session let me sign in all the materials are present all right some other people will not even try to check and they'll say no did not get okay cry babies everything you want but you will not go and see to it okay this is the most things right see day 2 day 3 everything is there see resources are there see go and see the class materials everything is uploaded but people don't want to do it because people are very lazy so hats off to the people who are lazy claps guys for all the people who are very lazy who do not check out the description everything okay so just check out in the resources right resources okay so let's quickly do it so let me just revise whatever things we have actually done yesterday okay glove also i will try to teach you don't worry okay so here is a paragraph then what is the use of porter stemmer what is the use of porter stemmer quickly this may come as a quiz to you also in the upcoming quiz competition tell me what is the use of porter stemmer what is the use of porter stemmer guys for stemming purpose we basically use it okay yeah for stemming purpose we basically use it so here you can see if you sent tokenize then basically we are converting paragraph into sentences that i showed you porter stemmer is basically used for the stemming purpose word net limitizer is basically used for the limitization purpose right so we all discussed about this right now what is this entire code doing over here what is this entire code doing can anybody tell me what is this entire code basically doing so in short we are taking the sentences we are removing all the special characters other than tell me what we are doing exactly over here other than a to z and capital a to z remove everything remove all the special characters because i had copied that entire text from wikipedia right so sentences of i and then we are basically uh using this and appending it still we have not applied this so this is how my corpus will look like okay so this is how my corpus will look like so here you can see this entire corpus and uh oh you are basically removing all the spaces and all everything if you write stopwords dot words of english here you will be able to get all the stop words so one question had i had asked in the last quiz that which all of these words are stop words right so there are many many stop words there is one stop word which is like don't not right not is also there so i always suggest that you create your own stop word okay based on some of the words that you didn't don't really want to put over here okay now let's go and try to do something like stemming right so what does word tokenize actually do it converts the sentences into words so for words in words you can see i'm just checking whether it is a stop word or not otherwise i am applying stemming okay so this specific code will actually do that so whatever sentence of words that you will basically be seeing here you are actually seeing that okay it has got stemmatized okay now similarly with respect to lemmetion you just have to use lemmatizer.lemmatize okay so here it is now here you are going to get a good work now see we'll go step by step over here first of all we'll apply stopwords remove all the words lemmetize we are going to do so first of all i went and read all the sentences i removed all the special characters other than a to z and capital a to z okay and then we went to do review dot lower review dot split we did right when i do review dot split that basically means i'm going to get each and every sentences and then here you can basically see that i'm applying for each and every uh you know words and basically finally i'll be able to get the corpus i don't think so let's see how much is the corpus so here you can basically see the corpus is available okay okay everyone clear okay one thing that we did not do is that i had to write something right word tokenize okay not a problem we'll do it okay so let me do one thing now quickly okay bag of words tf id i'm just trying to open a code so that i can show you what exactly we are trying to do okay okay so for word in review if not word in this set stopwatch.words i'm going to check and basically i'm appending it so by the by this way you'll be able to see that my all the sentences will be limitized now this is specifically the code for bag of words so for bag of words what is the function that we use we use something called as count vectorizer okay apart from this if i say count vectorizer there are also something called as vectorizer okay count vectorizer scalar okay now in count vectorizer you have some amazing parameters like ng gram range okay now see the power of ng gram okay now everybody focus on the power of ng gram if i use ng gram over here okay like ng gram ng gram range and if i write 3 comma 3 so that basically means it is just going to create the trigram okay 3 comma 3. if i execute and if i execute the uh uh fit underscore transformer corpus now if you go and see the vocabulary here you can see narendra damo rudar das modi okay da da so here you can see listen born september born september indian so everywhere you will be able to see that there will be only trigrams okay so by if you're probably using this ng gram so here you can definitely see that there is only trigrams nothing else right but now if i write 2 comma 3 so it is basically going to consider both bigrams and diagrams so here if i execute this and if i execute this sentences also and this sentence is also so here you can see that it will start with bygram like this indian politician so after you complete two entirely then you will be able to get trigrams okay everybody clear good enough so from the vocabulary you can definitely see all the indexes of the words that are present okay and now if you probably see corpus of 0 and if you try to see the output the output will look something like this okay for the first one okay so how do you verify just go and see which index is matching to which index okay it's a very simple and this many features just imagine now this is the problem of sparsity right this is the problem of sparsity which is really really bad okay now let me try to do the same thing for tf idf so here now i'm going to just write down a comment and basically say that okay let's go with tf idf okay so for tdf idf i will just go to write from sklearn dot feature extraction dot text i'm going to import another library which is called as tf idf tf idf vectorizer okay so here i'm just going to write tf idf vectorizer okay in tf idea vectorizer also you have ng grams and all you can apply for it but here i'm just going to write cv dot cv dot fit transform and here i'm just going to write corpus okay so once i execute this if i probably now see purpose of 0 okay and if i probably see x of 0 here you will be and try to convert into an array to array so here you will be able to see do you see different different values in each and every sentences you can see different different values sorry in this sentence you can see point one nine six six point two five two three point two five so this vector that is getting created right okay that is getting created here you can basically see that different different values are coming up okay okay everybody clear see whatever i have done simple i've just used tf idf vectorizer okay and then based on that you will be able to see okay everybody clear yeah so here you can see 0.25 0.178 0.216 obviously the feature is basically coming from here you can definitely check it out all the features from here okay so if i probably try to see over here whether bigram and trigram are also present or not just go and press shift tab here you will be able to see somewhere you can see vocabulary is there bigram diagram is not available over here it should be right tf idf okay so if i go and see tf id of vectorizer hmm ng gram is they see ng gram 3 comma 3 so i can basically also use this and write over here and let's say i want just trigram then i can execute this so with respect to diagram you can see that i am getting a different different words right and there are also chances that different different vectors will may get the same thing because the frequency is almost same okay suppose i want two comma three then it will be somewhat more different so here you can basically see different different things are available okay but again i want to try with one comma one then i will be getting some more different different different values okay okay there is also one amazing uh feature okay now just imagine guys if i have this specific words with some frequency right let's see let me define something like this suppose i have word like frequency like good is it's 10 times boy is seven times and probably you may also have words like girl girl six times toy five times and like this i have a lot of features and let's say after one specific feature it is less than three okay so we can also make sure that we can put this count also okay so how do i put this count so there is another another feature over here which says max features if i say max features as three let's say so it is going to consider all the words that are at least present three times okay so here i'm just going to basically use this like this max feature is equal to three so that basically unless and until not present that that many number of features are not present with respect to any words that are present over here suppose this full-time worker if it is only present for three times then only it is going to get used okay so here we can also use this specific parameter and we can reduce the vectors now you can see that the vectors that got reduced right vectors are getting reduced right so this max feature let me just show you over here i think oh if not none build a vocabulary that only considered a top okay top three max features top three max features ordered by term frequency okay top three max features suppose i want just the top three then i can actually be able to get it okay okay so here you can basically see that if i use top 10 max features which are the top 10 occurring max features based on that also i can basically do it so i'm just going to write it over here as saying that okay i won't just start the top 10 and execute it i'll be able to get the top 10 vectors okay everybody clear with respect to this okay one super assignment for you shall i give you one assignment so again i'm going to say you if i write max feature is equal to three so what it is going to do it is going to pick the top three high occur high element accuracy or high frequency elements or high frequency element uh features so in short if i say max feature is equal to three whichever is the top three highest occurring frequency i am just going to consider this so it will be good boy and girls so in short you will be getting a vector of dimension three okay okay and this is very much important guys because there will be some words that will only be present one time right that will be present one time or two times or three times right so it is always good that we take the top maximum number of words or highest occurring words that are present in the entire corpse okay everybody clear okay assignment time uh go to kaggle don't go to kaggle otherwise ucf repository you see i repository go and pick up any data set that you want go and probably see text data set text classification text classification because you go and search for it you'll be able to get a lot of data set over here try to do something on that okay so if i see a test data set here you can basically see asian religious data set just try to try with this take up any data set that you want take up any data set that you want try to apply all these things so this will be an assignment for everyone so assignment time right unless and until you don't do assignments you will not learn so assignment is that take any data set any text data set i don't want to create model right now okay take any data text data set apply bag of words apply tf idf apply ng gram and just apply on this specific data set and just try it out okay just try it out okay just try it out all these things and after you do it just drop me a mail at krishnak 06 at the rate gmail.com after doing your assignment just drop me a mail at send your assignment to me at this specific date okay i'll have a look and based on that because we need to learn a lot of things right so did you like today's session everyone yes so in the next session we will focus on word to work we are going to learn many things in word to work first of all we are going to see what is embedding layer what is embedding layer uh second topic that we are basically going to see is word2week word2week has two different things uh one is cbov and the other one is skipgram we're going to check it out then i'm going to basically discuss about the architecture of word word to work okay then we are going to solve practical problems and then i also want to take one topic which is called as glove okay so this this entire thing will get completed in the in the next week monday class okay not on saturday and sunday okay saturday sunday i have to take classes okay so all the seven days i'll be teaching okay so cbov skip gram will get completed uh and then we will be doing something called a spam classifier a lot of different different examples we will try to okay so everybody ready for the quiz what is the differences between two max df min df where they are searched what is so big in that min df minimum df minimum df go and search for it when building the ignore terms that have document frequency strictly higher than the given threshold simple enough yeah yeah label two vector also i'll try to convert okay lbl back okay okay so quiz time everyone go ahead for the quiz here we go okay first step follow me on instagram and today i will also be requiring your id card okay today i'll also be requiring a okay so everybody quickly do it 396 people are there but do it here and uh please follow me in instagram and drop me a message over there follow follow me on instagram so that you can send your id card i require an id card also so keep your id card ready and drop me a message okay i require a id card whoever is coming first i will be able to check from that okay i will start the session in some time but please make sure that you follow me and whoever wins please send your id card later on okay but please make sure that you follow right now before we start so that i can validate and cause you okay id card basically means any id driving license anything so that i can validate it's your because many people are editing and uh you know saying that i am the first winner okay insta id i've given away krishna x06 just search for krishna x06 okay search for krishna zero six okay krishna zero six everyone just search for krishna x06 and just participate for right now okay okay guys uh just one request subscribe to my krishnak hindi channel also please make sure that because i'm also uploading videos in hindi channel so please make sure that you subscribe to that okay okay 207 why less number of people are joining uh i want 399 people are watching come on quickly okay i'll also try it from my end server is not that slow your internet may be slow okay so search for krishna quickly follow me in the instagram so that the score that you will be able to see and price i will be able to send you through google pay okay so should i participate in the quiz mentee.com all right six six one three eight four five okay so i have also joined i'm also giving the heart symbol okay so we will just start in some time i want people to join quickly 232 people have joined only why why why why join quickly no 243 just go to this particular url mentee.com and type the code triple six one three eight four five yes i'm also participating and i'm also competing for the first price two thousand rupees okay in the dashboard you will be able to see the resource folder there you'll be able to find it okay perfect mode 261 come on guys join quickly join from your mobile phone join from somewhere okay join join join join quickly so that we can start once it crosses 300 i'll start guys quickly we have started the session now people need to join you are not supposed to participate right krish i'm not participating i'm coming to give right answers to see everything is working fine or not do you pay paypal yes i'll also pay paypal it's stuck on insta like page okay okay do one thing just follow me on instagram so that at the end of the day i can message you from there and validate each and everything okay guys uh come on join fast another one minute i'm giving you then i'm going to start everybody ready okay let's go with the name now go ahead and write your name everybody is writing your name yes everybody is writing your name okay so here we are going to start and here comes the first question nlp can be used in so i have answered it right this time so you just have for 15 seconds quickly time's up everyone so yes 20 people have said wrong 21 machine learning they have said none 278 good very good people are saying it right okay and i hope everybody wins this it's a very easy question come on okay so coming to the second question okay here we go in your screen on your face what is the process of turning a different morphologies of a word into its base form i am writing i am writing a wrong answers so here you have another 15 seconds quickly and here we go time's up 207 people have said the right answer 51 wrong it is lemmatization guys okay so i've just used some different words to confuse you but i think this was the first question that i had actually asked so if you're liking the session please hit like if you want to also give some money so that i can distribute to people please give hit super chat you know super chat thanks something is there no how how things will go i have to take that money and give it to you all you know come on so please hit like let's complete it to 400. yeah having fun how many of you have said wrong answers still now both are wrong how many of you have said both right or tell me like that okay so come on hit like make it 400 before we go to the next question okay here comes the first reward i think sony agarwal is in the first position is in the second position raja varun kurapati is in the third position so sonia congratulations still there a lot of questions i hope you stay in this way and participate in a proper way itself right let's go my rank is 9 35 guys see i have proof i have proof okay now let's go to the next question everyone and here we go how many trigrams is in subscribe to krishna youtube channel trigrams how many how many trigrams in subscribe to krishna youtube channel so i've given 30 seconds for this answer so that you can think and probably answer amazing let's see how many people may have said right amazing 199 people has said three this is absolutely right subscribe to chris is one two krish youtube is second chris youtube channel is third so have you subscribed have you subscribed or not come on subscribe subscribe then only you'll be able to say right now amazing so 199 people have answered it right let's go to the next question so hit like hit like then only you do you want more complex question or easy question um let's go to the next question question number four in front of you what is a high frequency which is a high frequency and low document frequency a low weight in tf idf a high weight in tf idf i think many people will say wrong in this because this is a little bit confusing things you know which is a high term frequency and low document frequency a low weight in tdf idf or high weight in tf idea of a bag of words a document i have clearly mentioned in today's class a high weight in tf idf okay so 160 people have said right it's going on amazingly well i hope now let's see the rank board okay sonya agarwal is still in the topmost position of what no so now we have again sonya agarwal let's see how what is my rank my rank is 2752 oh my god sorry 70th place my rank is 70th place okay i'm coming towards first rank let's see so sonya agarwal is in first anmol so in his second raja varun kurapati is in third okay amazing so hit like okay and obviously for the people who are just participating for the first time okay so here we go to the next question the fifth question and here we go which of the following algorithm is widely used for text classification so here we go decision tree support vector machine name bias all of the above i know many people will basically say this as wrong let's see let's see who will be saying it the right answer so 6 5 four three two one many people will be selecting bias i knew that all of the above is the right answer guys you can use decision tree you can use support vector machines you can use name bias whatever algorithm you want you can definitely use because at the end of the day you are converting a text into a vector so every machine learning algorithm will work okay perfect so i can see many people are saying wrong wrong wrong answers definitely you cannot come first by saying so many wrong answers final question on to your screen and before we end this session okay let's go ahead and gradient descent adagrad adam what are these loss function optimizes regularization none this is something out of nlp oh it is in nlp only in deep learning but yes it's fine loss function optimizes regularization none yes many people have actually said it as optimization that is the right answer now let's go to the leader both and probably i will be knowing who is the winner sony agarwal is still in the topmost position this is awesome so sonya agarwal has won first and mol sony so in has second adarsh has third so sonya please make sure that you ping me in instagram congratulations to the winner congratulations to the winner sonya agarwal you can drop me a message in insta along with your government id okay i'm waiting unmold so in others amazing winners come on animal so in oh okay one person says rank improved amazing congratulations three five seven five one zero nine come on guys why you are not sending me the information 91 where is where is sonia agarwal and molson and others they're not dropping me a message why people are not okay 40 rank okay just ping out all your okay i've got a mole sign message animal swine will basically be getting 1500 and mole up id along with your id proof okay perfect i've also got his id so he's basically from lovely professional university okay what about others uh animal saying i'm just sending you just send me your email id okay this is there i've got his information up id so he is basically second so i'm going to send him 1500 rupees okay guys you can improve that only i said right now it will become tougher so animal i'm just going to send him 1500 for quiz winner sonia garwal you can drop me a message so anmol i've sent the money you can check it out so i have dropped a message okay others please send your screenshot okay please send your uh id also others others please send your id otherwise it will be difficult for me others please send a id others please send your id sonia please send me your info only the first rank has not sent me any info right now no class tomorrow guys we'll be having the class on monday okay sonia i don't know where sonia is but she's not sending me okay i've got wealthy vivek adarsh perfect so aadhaar is getting 1500 so i've sent others to you also just have a look on to the phone so i've sent him also perfect sonia is remaining let's see whether she'll message me or not sony a message i have not got this was it from my side please keep on rocking don't forget to subscribe krishna hindi channel krishnak channel and don't forget to follow me in insta so that next time whenever you're participating in the quiz we are going to have more amazing things okay and uh yeah this is it thank you bye hello yes i'm audible so thank you guys this was it from my side bye bye take care