# Training Word2Vec From Scratch And AvgWord2vec Indepth Inutuition

hello guys i hope everybody is able to hear me out i'm bit unwell today so please uh today we are going to have one amazing practical session i will wait for some time till then everybody joins i've already given the link in the chat section please make sure that you download it so that we can start the session and sometime i'll just wait for some time again let people join but i hope everybody is doing fine every day everybody is doing well i hope everybody is able to hear me out can you give me a quick yes not a worry so we'll just start the session please make sure that you download it uh the github link i've already given in the chat please hit like okay please hit like and i will just start the session in some time okay till then please download the github link which i have actually given you so guys i've already shared you the github link this is the github link it is in the chat section please make sure that you download it keep the keep this file ready open this file probably in your local folder like this how i've actually opened today we are going to do what to wack and average word to weaken with respect to practicals we'll try to complete it and we'll try to solve a problem statement over here so we are going to go ahead and have a look on to that okay so please make sure that you are ready with respect to this just give me a quick yes once you're ready and we can quickly start the session yeah just give a quick just give me a quick yes once you have downloaded each and every data set in your local folder so you probably see like this you'll be able to download it okay this is all files spam collection day five nlp and all okay okay so once you download each and everything keep it ready and then we'll start okay just give me a quick yes once you have done it we'll wait till then now once everybody downloads it we'll start teaching this particular project okay the download link is given in the comment section in the chat please have a look onto the pinned comment i'll also be putting it in the description of this particular video yeah just reload the page i have also put up in the description of this particular video so please make sure that you download it from there just reload the page you'll be able to see in the description just give me a confirmation once you're able to see the uh project in the description of this particular video yeah just reload the page you'll be able to see the information in the description of this particular video okay so let's go ahead uh so whatever things we have learned till now okay like a bag of words tfidfs and apart from that we had also seen word to back today i'm going to again cover one more concept which is called as average word to work okay and we're going to see that how we can solve one text classification so we'll try to cover up with bag of words and tf idf and then we'll try to solve this project and then we'll go and see how to solve it word to work and all okay so here it is uh first of all i'm going to import the pandas as pd so right now you'll be able to uh see over here in the sms spam collection you have a file okay and in this particular file this is basically a tap separated data set okay and here you'll be able to see you're getting as a output as ham and this is your entire text information okay and then here you'll be able to see ham and again some text information spam and ham so today's problem statement that what we are going to solve is basically spam or ham classification okay and we are going to solve this with the help of all the techniques we have learned till now okay we are first of all going to see with bag of words then we are going to see with uh tf idf then we are going to see with a word to work inside word to back we are going to see something called as average word to work also okay and average word to work i'll try to explain you today okay so as usual uh we will be reading this particular data set from this particular folder the separator that is used over here is something called as tab okay so here you can basically see that tab is over here and based on this tab right it will try to see that what is this specific feature this is my output feature and this is my entire input feature okay so i'm going to read this i'm going to put the column name as label and message okay message will basically be talk about the entire description of the text that is present in your data set label is basically your output feature it can be spam and have okay so here it is we are going to execute this so here you can see clearly once i go and see my messages here you have something called as label and message okay so this is my input feature this is my dependent feature okay and based on this problem statement what we are going to do is that we are going to do some text pre-processing into this we are going to convert this entire text into some vectors but if i probably go ahead and write messages dot shape here you will be able to see that the overall total number of text is 5572 okay that many number of rows are actually there and total number of columns are two okay so if you really want to pick up any column you can do it with the help of index like 450 100 index suppose if i write 100 in text i'll be able to see that there is something called as please don't text me anymore i have nothing else to say okay so this can be a spam or a ham text altogether okay so everybody clear till here can i get a quick confirmation can i get a quick confirmation everybody's if you are able to understand till here with respect to data collection this data set has been taken from a uci repository so i've given that in the you know github link okay so i hope everybody is able to understand till here now we are going to do some kind of yeah you can do it in pycharm you can do it in visual studio code wherever you want okay so the next step that we are going to follow is something called as data cleaning and pre-processing okay now in data cleaning and pre-processing what all we are going to apply is that what we have whatever we have learned right so i'm let me just open this quickly and let me write what all things we are going to do so i'm just going to make this as my full screen okay so i suggested this is the day 5 of nlp day 5 of natural language processing today in this session we are going to do practical implementation as suggested practical implementation so we are going to basically take a spam classification data set spam classification data set okay and we are basically going to solve this problem with the help of machine learning algorithms the machine learning algorithms can be anything that you want to select but let's go ahead with the steps first step what we are going to do is something called as text preprocessing okay text pre-processing the second step in this text pre-processing what all things we are going to do we are going to basically do tokenization we are going to apply stop words okay and uh along with this tokenization and stopwatch we are also going to apply something called as uh you know limitation or stemming so i'm going to apply stemming lemmetization okay stemming and limitization we are going to solve this okay and apart from that in this stemming and limitation we also going to see how we can do with the help of nldk okay coming to the next step inside this after the text preprocessing we are going to convert text into vectors and now to convert this text into vectors whatever things we have learnt in the previous session like bag of words tf idf and apart from that we are also going to use something called as what to wack average word to work okay so all these things will get completed in today's session okay and this all will be implemented with the help of python and machine learning and we'll see that how good we are able to get the accuracy okay so this is the entire plan now quickly uh let me go over here and quickly show you what all things we actually have okay now first thing first over here for the data cleaning process i'm going to import three important libraries that is called as a regular explanation nltk and stop words okay because since we need to really apply stop words so i'm just going to import this already the code is given to you entirely you can also run it from your side and basically have a look then from nltk.corpus i'm going to import stopwords and then i'm going to also import porter stemmer this porter stemmer is used for stemming purpose okay stemming focus okay guys many people are asking about the quiz uh so uh i have received a lot of complaint from many people yesterday everybody's cheating okay uh in this world right if you just talk about money people will start teach you know cheating right and menti meter also has one hack uh where you can actually cheat i've seen many people doing the cheatings yesterday also and there before yesterday also so i'm looking for a new platform which i will probably soon find it out i'm talking to the mentimeter team you know there is a big bug over there and some smart people you know uh who who likes to cheat anytime you know any any time of the day they don't want to do hard work in their life you know so those people what they will do is that they'll try to cheat anyhow and they'll try to you know grab money whatever way they want okay so i found out some problems with the mentimeter app application and people have cheated a lot and the money that i actually gave previously right it was i i feel that i will i feel that i should stop taking quiz okay because quiz uh was something like uh you know uh where people had fun you know it was just not about money but people started cheating over there also okay uh and many people i know that they uh they what they do is that you know many many people i have seen many people over there what they do is that uh when we actually uh when i'm teaching over here right many people are just there for the quiz itself but now uh from tomorrow i'll take up quiz but uh again uh it let's see like i don't know whether i'll be putting money over there or not money was just to motivate you so that you can actually learn properly you can there was some kind of giveaways i was giving it from my pocket you know and people i don't know you know but anyhow so i will i will have a look on to the quiz okay and uh this time uh i think uh i'll not give the money you know because people they found out a hack to basically see from that application they were inspecting the elements they were actually finding out the answers you know and uh people who did not cheat it and you got a good rank be proud of yourself okay and anyhow uh let's see i'll distribute the money or not but i will definitely take a quiz uh over here okay and at the end we will have a quiz let's see okay i have to prepare some questions but when i when i probably saw that right i i felt really sad you know when i'm doing some giveaways i really want to do giveaways based on the efforts that you put right and if you don't put any kind of effort i'm sorry this way it'll not work anyhow okay the classes that i take completely for free over here is to motivate you is to make you learn in a better way for the people who do not have money right they basically learn from these youtube videos and you know try to crack jobs okay and people they just for the sake of money you know whatever and at the end of the day also i see that uh when you come for the live sessions also right uh you don't stay till the end you know and uh that is also a very pro problematic things you know like initially 500 600 people will come and later on you know it gets reduced uh i know not interested in studies but later on they'll also be demanding new new things okay anyhow uh let's work for the betterment of all the people and let's try to uh do something amazing for the people itself you know and uh this is the thing that right why i'm taking this is that to benefit a lot of things okay but uh yeah i will try to put some quizzes so let's see but let's focus on this practical session right now okay now uh over here i've already shown you that we will be downloading the stop words uh i have used porter stemmer i hope everybody knows what porter stemmer is for okay just comment down in the chat uh like what porter stemmer is actually for uh usually for the stemming purpose and if you really want to do limitation okay um with respect to lemmatization we basically uh make sure that uh you know when we are actually creating the base form of a specific word over there you will be able to see that we will be getting the root forms but the spelling will be absolutely fine and correct okay so that is why we basically use uh word limit ordinate limitation okay so quickly i will try with stemming first and then we will go ahead and try to understand about the other things okay one one amazing message from manisha versus she got internship in uh microsoft wow this is amazing manisha can you just share your experience manisha like people also wanted to know about your experience on cracking interviews uh this is an amazing thing of getting internship in uh you know microsoft so definitely just write down a small text in the chat so that people will be able to see it okay so here it is uh i'm just going to import the stopwords and porter stemmer protostreamer is specifically for stemming purpose and here you have for i in range of zero comma length of messages okay i'm traversing through all the messages and this first word is basically to remove all the special characters other than small a to z and capital a to z okay then we really need to do the lowering of the sentence so that duplicate words will not be present over there okay then we actually do the splitting of this entire words and for each and every word we traverse it we apply stop words and then we apply stemming okay so three different things we actually do with respect to this and finally i joined my review and i put that back in corpus so if this will probably take some time if you are actually doing stemming and it will take more time if you are actually doing lemmetization so in this process we are actually doing stemming in the future i will also be showing you about limitation in this specific project okay so here it is uh it has got executed now if i probably show you the core corpus here you will be able to see all these words that are present in the sentences have got applied with stemming okay so this is where you can see trouble has become trouble you know wait finished lecture has become left you know so this all things are actually there but let's see whether we are able to get a good performance by applying stemming in this uh by applying a simple machine learning algorithm then here you can see that we are applying count vectorizer this is just for the bag of words model okay now inside this bag of words model what we are actually going to do is that we are going to take all the corpus that is present over here after cleaning it okay as i said guys i know stemming creates a lot lot of wrong words but this is a kind of a spam classification spam or ham classification so if we have sufficient amount of data with respect to in our training cases i think this will definitely work yes for some of the questions like q and a chat bots you know we should definitely try to make sure that we use it okay so over here it is yes you can also write 0 to 9 that is also not a problem so over here you can also write 0 to 9 something like this so that that also remains over here so let me just execute it once again now you will be able to see the numbers over here okay okay so this is probably taking some time let's see whether our corpus is ready again it is checking all the condition over here with respect to the corpus and here's it got executed now here you will be able to see that numbers are also there right 2000 price price claim call this right thousand and obviously for this kind of sentences right you will be able to see that okay this is the kind of spam that is available okay now when i have this corpus the next thing that i'm going to apply the bag of words in bag of words we are going to use something called as count vectorizer i've already told you about count vectorizer it is basically used to create the bag of words over there and one thing i'll going to make it over here is that binary is equal to true okay so that i want all the numbers in the form of binary right so i'm going to import count vectorizer i'm going to basically use this count vectorizer max features 2500 binary equal to 2 and then i'm going to basically use cv.fit underscore transform corpus to array okay and here you will be able to see that when i do this cv automatically my bag of words will get created so if i probably go and see my x here you will be able to see that you'll be able to find zeros and ones these are all the arrays with respect to all the other other features that we have okay so if i write x of 1 you will be able to see this is my features that is there with respect to the sentence one okay so i hope everybody is clear till here so my input feature is basically given now let me talk about this maximum maximum features okay max features why i have taken us 2500 we can select different different values but again understand that what does max feature actually say it is saying that you can basically take the maximum top maximum 2500 features if you want okay and you can basically use this in in the case of uh the maximum occurring features right 2500 maximum according feature okay so you can basically select this apart from that you also have something called as ng grams right so you can also have ng gram range here you can use 2 comma 2 based on your things and here you can actually define that too let's say that i have selected this this can be selected with the help of hyper parameter okay and you can basically use this so this is my x of 1 and all are there you can also check out what is the shape of it suppose if i write x dot shape you will be able to see sorry x dot shape you will be able to see the entire things so there are around 2500 features which i have actually mentioned over here as max feature okay so here you can clearly see that i have 2100 features so i'm getting 2500 features over here okay uh if i putting binary i'm only going to get ones and zeros suppose if that same word is present two times in a sentence it will not become two but instead it will become one okay now uh the next thing i'm also going to make sure that i do the label encoding for y feature because right now it is spam and ham if you go and see on the top it is spam and ham so here i'm basically going to just use get underscore dummies and probably take the y values so here is my y so if you probably see this y you'll be able to find out all the features over here okay sorry guys i'm feeling a lot of cold right now my nose is running like anything but again i promise the classes will be going on so i'm taking it out you know okay now coming to the next step that is the train test plate okay train test split over here you can basically see uh the cv is the model count vectorizer model okay count vectorizer model okay binary is equal to true basically says that uh if if a word is present two times in a sentence anyhow if it is greater than one the value in the in her in the back of words will be only one itself okay so here you can basically say that okay now let's go to the next step i've also get why this is basically labeling coding i've done it now the next step that i'm going to do is that train test split okay to enter split you can basically see that i have imported the library from model selection i have taken the train test split over here x and y and the test size i have taken as 0.20 and random state is equal to zero so i will just go ahead and execute it okay and uh you can see that over here i will be able to get my x train white rain and extract white rest so you can definitely have a look on to this so it will be x train so this is my x strain and if you probably want to see your y train there you will be able to see it so this with respect to this particular inputs this is my output okay and here is your input and output features right so you can actually check it out all the things then uh usually for uh if i really want to apply any kind of algorithm for this machine learning for this kind of text task i can basically use something called as multinomial name bias so here you can see multinomial neighbors i have initialized it i have done fit extra in white rain i'm not going to do you know i'm not going to do hyper parameter tuning because that is not is our aim our aim is basically to do how we can do text pre-processing how we can convert the text into vectors and basically work onto that okay so here it is i've executed this you can see this multinomial name bias and then i'm going and doing the prediction ahead i have done the prediction on x underscore test so finally i get my y print i can also check out my accuracy score so right now my accuracy score for the test data is 97 and the entire classification report is coming somewhere around 98 okay so it is good uh the type of uh this problem statement can definitely be solved with the bag of words right now but it is always good that we should try to look some other features also like tfidf which is efficient one and even word to back okay so uh now let's go ahead and create the tf idf like how we did it for the bag of words similarly how we did over here now we are going to do it with the tf idf okay now for tf idf it is quite simple okay from sklearn.feature extraction we up we actually import tf idf vectorizer okay and tf id of vectorizer here also i'm going to use max features equal to 2500 again we can play with this specific parameter here also you have something called as ng gram ng gram range so let's say i'm going to take one comma two so all the possible combination of ngram okay and uh it one comma two basically means from one feature to the combination of by feature by gram okay and then i'm doing fit underscore transform corpus to array okay then again uh you can basically do this again train test split which since i've created my x data again so here it is what i'm actually doing okay over here uh i've actually done this one test data and then i'm going to do the fit on multi-normal lay bias on x2 and white right now if i probably try to see the accuracy here you can see that it is being improved when compared to the previous one i had 97 somewhere over here and here i'm actually able to get 98 percent the reason may be because i may also can take you know over here also i can basically take uh if i see over here i can also take one comma two over here as nj graph okay by gram okay so all these possible options are there uh so here you can basically see this okay so finally i get this accuracy if i execute this i'm getting this specific accuracy now it's not like you just have to use only this specific model so you can also use random forest so if i run from sklearn okay dot dot ensemble import import random forest classifier so i can basically write classifier is equal to random forest classifier initialize it and then i can basically write classifier dot fit on x twin comma y train okay uh office says why you fit on the bag of words yeah that also you can do this just for uh teaching purpose okay i'm not following the exact good practices because if i try to follow the exact good practices it will be quite amazing so what you can do is that you can also apply this strain test and you can go ahead and then you can probably apply this count vectorizer only on extrane data okay like this right so here you can do fit corpus on x strain this also you can actually do not a problem not at all as a problem okay so don't worry about it so much because i'm just not following the good practices so here you can basically see has no attribute lower why it does not have lower okay just a second um you can do it guys i'm i'm just not following the exact process over here but according to your implementation you can definitely do that i'm not stopping you okay yes you are i've taught you all about data leakage and all definitely you can apply for that not a problem first of all what you do is that try to get the x data separately and then do the train test split and then try to apply uh this transformation if you want okay no problem at all uh if i'm actually doing in this way here the main aim again i'm telling you your main aim is not to follow all the best practices but trying to talk about how you can convert a word into vectors okay definitely data leakage will be there okay data leakage so if you want to go ahead with the best practices what i can do is that i can do something like this for you okay let's see um let's say uh here what i'm actually going to do this is my corpus uh here what i will do is that i will just clearly talk about something just after uploading this data right i'll separate x and y over here and then i will try to do the train test split if you want okay so just try to see it and you can definitely do it but let me focus over here what what i had actually started now here it is classified.fit on xtrain y train and then it will probably take some time because random forest classifier will definitely take some amount of time okay so here it is random forest classifier it is going to take it let's see how much time so yes default parameters has been selected so i'm just going to write classifier dot predict extreme sorry x test don't no need to memorize the programs guys this all are learned right you can actually learn it so this is my wi-fi now all i have to do is that write accuracy score on y pride comma y test okay and this i'm going to basically print it okay so here it is and then i will also make sure that i will use classification report on my bread for my white test so here you can see 98 percent uh with respect to this also am i able to get and this is performing better than the multinomial clinic bias right so here you can basically see that over there also okay now this is the main thing word to back implementation now we will try to go ahead and see with respect to word to work how we can actually do and there are a lot of steps with respect to word to back and we can definitely apply all these things okay now word to work uh it's very much important because word to work has a lot of advantages i've already told you about what to back two types right one is if i talk about word to work if i talk about word to back there are obviously two types right one is skip gram and one is c boy okay c-ball continuous bag of representation okay now if i talk about this two algorithms right obviously we have seen something called as gensim library right now in gensim library we have word to work through which we can train the model from scratch there are two type of word to back models one is pre-trained pre-trained models whenever i say that basically means i can use this google news 300 dimension data points or model that i have already shown you and the other one is that we can train this model from scratch train this model train this model from scratch now you may be thinking chris should we always train the model from scratch or should we go with the pre-trained model that is the first question that you basically have okay uh here you can basically if i really want to answer this suppose if your data set if the data set that you actually have right suppose let's say the new data set that you actually have over here if if inside this pre-trained model it already captures around 75 percentage of this entire words then i would suggest just go ahead and use this pre-trained models but if you see that inside this data set if such text are used wherein those texts are not present in the pre-trained model at that point of time you can train your model from scratch okay and most of the industries wherever they use you know what to work what they do is that they try to create their own specific model and that is a very good purpose okay but one one one thing when we learned about word to work right word to wake right you know that in word to work we convert words to a fixed size of vectors let's say if i'm using this google news 300 uh word to work model this will basically convert this word into 3 300 dimensions 300 dimension that basically means suppose if i have a word which is like king so this king will have 300 dimensions then again there is a another word which is like queen this will have another 300 dimensions okay now you need to understand one thing here every word every word okay every word is basically given by 300 dimensions right 300 dimension 300 dimensions right every word is given by 300 dimension but in our input right what what what will be our input our input will be one kind of message and one output will be one kind of message like spam or ham right spam or ham now inside this message i may have any word i may have many words like this right let's say i'm having a word right i want to eat pizza okay so this is my first first sentence now for each and every word if i convert this into 300 dimensions then you know how many total number of dimensions will happen tell me guys if each and every word is converted into a 300 dimensions so since i'm using this pre-trained model if i let's say 100 let it be 300 100 dimension fine but just understand each and every word will basically have 300 dimensions right like this how many number of words i have this all will be have 300 dimensions right but i need to always make sure that this this entire input is basically converted into a 300 dimensions right this entire input needs to be converted into a 300 dimension this is what we need to aim but if i'm using any kind of pre-trained model if you train the model from scratch by just using word to work we are going to get for each word we are going to get that many number of dimension so how do we fix this problem how do we fix this problem the fix the the problem can be actually fixed with the new concept which is called as average word to work now please everybody listen to this this is super important average word to work now what does average what to work basically say okay what does average what to basically say average what to work basically says let's say i'm i have i've written over here like this is my sentence let's consider this is my concern sentence please subscribe let's say please subscribe chris channel okay suppose this is my sentence and let's say i want to convert this word into 300 dimensions okay but according to the word to work what will happen this is my first word please this will be converted into 300 dimensions subscribe will be converted to another 300 dimensions okay crash will get converted to another 300 dimensions and channel will get converted into 300 another dimensions everybody clear yes yes or no everyone i hope everybody is clear till here yes can i get a quick yes if everybody is able to understand so here you will be having this 300 dimensions like this right here you will be having the 300 dimensions like this right now but i want to make sure that this entire sentence is basically represented by 300 dimensions right now it is like since four words are there so it becomes 200 1200 dimensions 1200 features so if i really want to convert this entire sentence in this 300 dimension we basically use average word to work average word to work is nothing but all this dimensions you will be able to see right this will get added to this this will get added to this this will get added to this and this entire average will get calculated and we will get one value okay now similarly if we go with the next one this will get added to this this will get added to this this will get added to this this will get added to this and we get a new vector which is like this now when i probably have all the vectors do i have all this calculated as 300 dimensions 300 dimensions right i hope everybody is able to understand yes and this is nothing but word to work okay this is nothing but average word to x sorry average word to make so this is what we have to do when we are training our models and when we are solving this spam and ham classification now once this happens this 300 dimension will be my input feature if i transpose it like this and then i will be having a output feature which will be spam and hat okay i hope everybody is understanding now yes i hope everybody is able to understand when you do average does not in individual word importance get slots no because all these values will be assigned a vectors based on different different semantic meanings right so i think that should not be a problem and this is the concept of average word to work and this thing we are going to write with the help of code also okay everybody understood guys yes on that specific dimension let's say this specific dimension this this this this will get added and an average will get calculated for finding out the new dimensions okay okay so this will basically be my new dimension values but always remember my size will be 300 in this case suppose if you're creating word to vect as 100 dimensions okay let me just show you one more example suppose let's say let's say i am designing a word to work model let's say in this word to work i have i want five dimensions phi dimension basically means let's say there is a word like krish data science channel rich data science channel is my vocabulary let's say okay let's say now what will happen over here so krish will be represented by a five dimensions right with some feature representation so what do we say in word to vect this is feature representation right right similarly i will be having another five dimensions over here another five dimensions over here each and every vector will be represented by five dimensions okay but i cannot give my data set like this to train in a machine learning model because my inputs needs to be same the input vectors needs to be same right now in this particular case obviously this is 5 5 5 5 20 if i totally combine but my main aim is that i need to give a five five dimension in my input right vectors five dimension vectors so what will happen this this this this summation will happen and an average will get calculated which i will be getting the value value value like this right i hope everybody's able to understand now can i get a quick yes and this is the same thing that i'm going to do i'm basically going to just find out the word to vec for each and every features and then we can actually do the average of all these things everybody clear yes so let's go ahead and let's try to see this okay okay now first of all we are going to install pip install gensim okay i've already told you how to download what to wake right so this is how you download you can download it from an api okay what to wear google news 300 but this is not my aim today i want to create from scratch now whenever i see this vector this will basically be 300 vectors because this model is basically trained to create a 300 dimensions okay so uh i hope everybody is clear till here but i want to train my model from scratch but before that let's apply wordlet nematizer i also want to see how my model will perform if i apply word net limitizer world and lemmatizer is that what we do in world net nematizer is that we apply limitization so i am going to initialize wordnet limitizer and you can see that i have applied for this corpus same thing from port stem stemmer dot stem instead of writing cement stamina on stem i'm going to basically write limitizer dot limit ice okay so i'm going to do this and probably this will take time can anybody tell me why this will take time yes can you tell me why this will take time why this specific thing will take time right any any any reasons why this will take time right yeah why why this will take time so here you can basically see your corpus and this corpus will be quite good with respect to word let nemetizer here you can see that here all the spellings will be perfect right all the spellings will be perfect right lol always convincing catch bus frying egg matey eating mom left dinner fee love right ah work vaguely remember feel like lol so i'm going to basically apply word to work on this kind of text because word to back will give me a fixed dimension so it will be giving us a good result now first thing first okay what all things are there okay if i go and execute corpus of zero here you will be seeing the first statement before this i'm going to import two things one is send tokenize and one is gensim.util simple process okay so here two things are there i'm just going to execute this now everybody focus on here and try to understand what is this simple process simple process function so if i probably try to just execute this and execute on the top so here is my simple process and simple process if i see that it says that converts a document into a list of lowercase tokens okay this is super important because you don't have to lower it again okay so we will try to lower it over here by just using the simple process function okay so let's go and see the sentence so for sentence and corpus each and every sentence i'm doing i'm going and tokenizing it and inside that particular sentence i'm just processing it and making it smaller if you don't want to write this code you can just go ahead and write sentence dot to lower sent dot to lower okay but in short it is lowering each and every words over here okay so let's go and execute now if i finally go and see my words these are all my unique words that are present in each and every sentences okay so here you can basically check it out all the unique words that are present in each and every sentences okay so i hope everybody's clear with this simple understanding of this now let's go ahead and try to create our word to work from scratch by using all this words and sentences that is present okay so first thing i will import gensim in gensim uh you'll be seeing models dot word to back which is again a what to function here the first parameter is basically your sentences that you're going to give the second is what is the vector size so can i say how much is my dimension can anybody tell me how much is my dimension by default how much is my dimension by default you can see over here this vector size is nothing but it is a dimension okay so here you can see the vector size is nothing but dimension okay everybody clear okay dimension is hundred can anybody tell me what is window size window basically means what i have already explained you about windows yesterday or they before yesterday what is this window when we use cbob continuous bag of word or when we use uh you know uh skip gram i hope everybody remembers we use something called as a window size window size basically means to understand the context how many context of the words on the left hand side and right hand side we basically have to take okay so here you can see window is equal to 5 what is this minimum count minimum count basically says over here if i go and show you what is minimum count by default so you can play with this all parameters here we go see this is c baud mean okay c bao is also an option over here okay so if i probably try to see this here you will be able to see all the parameters just try to have an explore on the parameters try to see what all each and every parameter mean okay so minimum count ignores all the words with total frequency lower than this right so here also you can find out that we actually can set up a minimum count so let's say that i am going to initialize my window size as five and minimum count as two and i am going to just give the sentences and execute it now after i execute this if i really want to see all my vocabulary so these are all my vocabulary over here see based on the text the entire uh you know the entire data set that i have actually taken this is the entire text okay so here you can definitely clearly see all the text all the vocabularies over here okay if i write model dot wv dot index to key you'll be able to find out all the vocabularies right like call get your gtlt free no come like everything is there okay and all are good words you know the meaning is correct absolutely correct so if i want to show you what is my total vocabulary size so here you can see five five six four how many epochs it has got trained so here you can see five suppose if i take any word like this like price and i can basically find out the similar word by just using this function called as dot similar by word and if i execute this here you can see claim line call land guaranteed show draw cash hr code so it is being able to capture all the specific information okay it is able to capture all the specific information over here right how price are similar to claim here you can basically see right so similarly if i take up any other word which word you want tell me so instead of price let's let's take some other messages like please message happy let's take up happy okay and if i probably try to see the similar word with respect to happy here you'll be able to see your day make hello dear new keep app like love love also it is being able to determine okay okay so i hope everybody is able to understand over here right clearly what things we are actually doing okay now similarly if i go and see this uh okay now till here i hope everybody's clear till here we have actually executed something called as what to wack okay yes this similar words have got gotten from our data set guys okay whatever data set we have like let's say hope okay hope if i really want to see the similar word from our data set i'll be able to get day love one much go going need last keep right so this is what we have done in directly we have created an amazing simple word to work for you over here right and here you can basically see all these things right now comes the time where we need to focus on average word to work i hope i have explained you what is average word to work now we are going to each and every word is going to get a vector of 300 dimension and in this particular case when i am designing my word to back you are going to get 100 dimensions because by default the 100 dimensions will be there now all these things needs to be added with respect to each sentences and the average should get calculated after we calculated the average we should be getting 100 dimension vector for each and every sentences irrespective of what the size of the sentence may be okay so let's quickly go and see and here we are going to create one function which is called as average word to work and it is just saying return np dot mean for every word in the documents i am just going to compute model.mv of word here we are going to get the vectors and i have to make sure that always those vectors should be present in the vocabulary okay and when i do that np dot mean on x is equal to 0 i am going to get the mean of the entire things yes it is using cpc over here so this is my average word to work function a simple one liner code okay and here you can see that i'm just going to execute this now if i go over here here it is c for i in range of length of words i'm just going to apply this average word to work on words of i okay so if you probably see my words this words are nothing but these are my sentences right so here you can see this is my first sentence go during this this is there this is my first sentence and then this is my second sentence then this is my third sentence so i'm just going to apply this entire functionality on this now you see this now it has got executed here quickly right see it has got executed average word to work with respect to each and every sentence has been computed now what i will do is that right now x is a list i will convert this into an array now if i write x of nu of 0 the first sentence so the first sentence will be having this 100 dimensions see this 100 dimension if you want to see the dot shape i will be able to show you right so this is 100 dimensions now here if i basically write word of words we have applied it for words right words of words right so if i go and see words of 0 here you can see this is my first sentence and for this specific sentence i have got a hundred dimension vectors right similarly so this becomes my input feature right similarly if i write words of one words of one so this is my second sentence right okay lar joking if only right and if i see the vectors or dimension that is created with respect to this particular sentence here you can basically see all these things right so in short what we have done for each and every word so this is my input feature x underscore new right x underscore new is my input features right so if i want to see all my input features all my input features here you'll be able to see these are all my input features right all my input features right right if it is not present also then what is going to happen see guys uh if a specific word is not present obviously with respect to the dimension it will get some values right so here you can basically see this right so simple if i go and probably write xnew of my fifth sentence fifth sentence here you will be able to see my dimensions and this is my dimension for all the letters now in the next step what you can do is that do the train test split you have your you have your x new do train train and test split you have your y variable and then you apply a model onto this okay this can be an assignment for you guys please try to do this as an assignment i have simply not done in both the things okay so this can be assignment okay and here you will be able to see this right very very simple so i hope everybody's got to understood guys how this average word to work has actually worked out and this we are doing it with the help of machine learning i guess everybody was able to understand can i get a quick yes i hope you had fun yes yes or no how is it creating 100 features i have already explained the theory part there before yesterday right it is basically using the continuous bag of words right and we have already trained the model over here from scratch these are all the models that have got trained from scratch and we can also find out the similar words that we want okay so this in short was about word to work average word to work and i have also shown you this so one assignment that i really want to give you is that try to do it for this particular data set i am db data sets or csv so here you basically have 50k movie reviews uh you can try it out this obviously it is 50k okay so just try it out with this 50k reviews okay and i will be also giving the link in the chat okay and this link i will be uploading it also over here okay so try to do it but uh instead of doing it bag of words and tf id i definitely want to see uh okay i definitely want to see this uh with the help of word to work okay okay so uh now what you can do do train test split apply any machine learning model it will get over right you will be able to understand it right but understand this is one one sentence input feature one sentence input vectors right so all these things have actually executed it okay so please make sure that you do this specific assignment of 50k review data set this looks good you know you have your input feature you will be able to create an amazing word to back i know memory it will take but try to do it in google collab okay i hope everybody will do it yes devel model over it otherwise what is the use please help it is not easy to apply ml model over it please you do one more it is very simple no this is my i have my x data i have this x news this is my features right and i have my y features do train test split no everything i should do it huh same thing what we did in the top right same thing you have to do whatever we did over here right train test plate just copy and paste it and do it now main thing i had to teach you was this right so this is my y feature this is my x new now do train test plate simple try to do train test plate see what output you are able to get right very simple very easy okay okay so from tomorrow mostly into deep learning and lp we are getting into where we will be understanding about rna and lstm rnn but if before that if time permits i'll also be teaching you about glove and uh later on when we learn about transformers and bert at that time i will be taking a new library which is called as hugging face okay so till now we have done a good amount of work uh probably you will be able to clack jobs if you know this much okay yes some part of deep learning is always there okay and that will be going on okay so overall i hope everybody liked this session if you like it just hit like make sure that you subscribe the channel and don't forget to subscribe krishna in the channel guys i've also started uploading hindi videos just for the beneficial for all right and to make sure that everybody learns in a better way everybody gets the right meaning and your support is definitely required otherwise i will not be able to work without any support right okay uh yes so let's start about rnn tomorrow and yes i'll take a rest for today please make sure that you hit like make sure you subscribe krishna in the channel thank you one doll take care have a great day bye bye and all the information materials has been given i'll also be uploading in the dashboard community dashboard that is present in the description thank you guys bye