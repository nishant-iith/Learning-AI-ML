# LSTM Recurrent Neural Network In Depth Intuition And NLP Application

hello guys i hope everybody is doing fine can you give me a quick confirmation if you can hear me quickly and then we will probably start the session another couple of minutes so just give me a confirmation if you can hear me clearly thank you yeah yes so we'll wait for some time so that everybody joins um i see very less number of people have joined till now why i think the notification is being sent still okay and i will wait till then i hope everybody's doing fine i hope everybody is doing well you know i hope you like the community sessions that are going on okay and uh yeah i think uh you know the learning process that you see with respect to nlp since this will be continuous for some time you know it will be quite amazing uh you know we'll try to learn a lot of things as we are going ahead today the topic that we are going to discuss about is you know lstm recurrent neural network which is super important for anybody uh you know probably we are now entering into the deep learning section which is quite important altogether okay so yeah let's start till then i just want some people at least 100 people to join till then hit like and i hope uh i made a video regarding tech neuron also today so please make sure that if you really want to learn many technologies at just one go you can actually go ahead with tech neuron also okay so let's go ahead and quickly do the discussion so yeah then other than that how are you all just let me know okay so today is basically the day eighth the eighth of natural language processing so today is the day eight and uh again in day eight we are going to learn many things as we go ahead you know so the agenda is completely very simple one at a time so today we are going to basically say lstm recurrent neural network okay and here we are going to see the in-depth architecture and we are going to understand okay in-depth architecture okay um and uh before we go ahead uh first of all i think we'll again revise the problems with the recurrent neural network problems with rnn so what are the problems that we have already seen with respect to rna whenever we have a deep recurrent neural network then what happens like if you have a longer sentence you know and probably let's say that i have a i have an application which is to predict predict the next word let's say of the sentence right like i may say like this on sunday right i want to eat pizza then on monday i want to eat i want to eat eat dash okay so suppose if i really want to make sure that i want to do this specific prediction you know then how can i make it right so if i probably use an rnn i hope everybody remembers how does an rnn look like so this will be your input and this specific thing will the output will be interned given to the same neural network with different different time stamps right and uh probably if i expand it like in this particular case suppose if i try to expand it so it will look something like this this will be my input and this will probably be my input again it will be my input it will be my input it will be my input so here all the inputs will be going on with respect to various neural networks right and obviously you will also be kind of on having an output over here right so can you tell me if i really want to do the next word prediction what kind of neural network it will basically be you know so i interview question to you what kind of neural network what kind of rnn you know what kind of rnn do you find over here what kind of rnn for solving this particular use case finding out the next word right which is the rnn technique you are probably going to use it will basically be many to one right so i hope everybody remembers this many to one because at the end of the day the final output that i am going to basically consider over here itself and uh here you can basically get the output itself but what is the problem over here if i have a longer sentence right to get this specific output we definitely need to focus on this context and probably on this context also right because on sunday i may probably like to eat pizza on monday i may like to eat something else right now since you when you have a longer sentence right uh there will definitely be dependency of the output probably with respect to the first word or the second word right so here we can definitely find out a lot of problems right with respect to rnn we usually rnn can only remember short term we basically say short term memory right what is short term memory i hope everybody has seen this movie called as gajani right gajani i hope uh we have seen the movie from the main actor is amir khan right he used to forget things you know just after five minutes so if you really have shorter sentences then definitely rnn will work well because let's say uh if i have just a sentence of four to five words let's say okay so if i really want to predict the next word within those contexts within those i mean within those range of context suppose if i want to find out whether i want to eat pizza i may just focus on this sunday context right so in this case rna may work but if you have a longer sentences it will definitely be a problem with rnn so this was one of the things that i had actually discussed yesterday right now considering this we will try to discuss about lstm rna okay so let's go ahead and discuss about lstm rnn now lstm basically says that it is long short term memory okay long short term memory now we will go one by one and we'll try to understand over here now the first thing that we want to probably discuss if i really want to represent let's represent rnn in some way okay let's represent first of all rnn okay and then we'll try to find out what will be the changes with respect to this rna and it will differentiate with lstm rnn so i'm just going to quickly take one small figure and again remember guys this entire explanation is referred from this blog which is called as cola github okay you can definitely search for cola lstm and i will be taking this figures over here i'll try to explain you each and everything okay so again those link i will try to put it in the description so the first thing that we are going to focus on is how does an rnn actually look like okay so i'm just going to use this specific image let me just copy and paste it so a basic rnn is basically represented like this okay let's say i've already created one small diagram with respect to r and n right now in rnn what happens is that i told you right i will be having one like a specific set of neurons and here i'm going to pass the input here i'm going to pass out but this is basically represented in this specific box okay and this will be like let's say i'm passing x t minus 1 at t is equal to t minus 1 and then this will be at time t and this will be at time t plus 1 right so this are the words that are getting passed okay and internally let's say what exactly this is what exactly tan h is okay whenever i use this kind of square box that basically means it is a specific neuron and top of it tan h is getting applied okay tan h activation function okay so over here you can clearly see that if i probably have when you have this kind of uh diagram that is represented over here this basically means the activation function that is getting applied on top of the neuron is specifically tan h everybody clear this is how we represent a traditional rnn okay this we are specifically representing a traditional rna so this is basically a traditional rna and this is super important to understand because then only you'll be able to get an idea how does lstm rnn look like okay everybody clear with this what is happening see from the previous state whatever output i'm actually getting i'm combining it right suppose this output will go to the next neuron right and then i will also have this specific word this will be x11 x12 so this and this word will get combined so it is getting combined over here concatenated you can basically say concatenated and then it is passed to the neuron where and 10h activation function is getting applied so the same thing will also be available over here will also be available over here is it clear everyone i hope you got an idea about it yes can i get a quick yes if you are able to understand till here okay quickly yeah before i move ahead i really want to make you understand in a better way okay so it is always good that if you try to understand in a better way uh there will be no confusion going forward right so everybody clear till here yes clear okay perfect so this is my traditional rnn now with respect to this traditional rnn now we are going to see what is the difference in rnn okay now lstmrn and i'm again going to take an another figure for this quickly without wasting any time okay so the next rnn uh lstm rnn changes what what are the specific changes you will be able to find out okay so lstmrn has this important property which is called as long short-term memory so this is basically how a lstm rnn looks like okay and i'll try to make you understand what exactly this is and how things are actually there now here you can see initially there was just only one tanh right and if you see over here they are like sigmoid is also there sigmoid is also there tanh is also there and tan h and one more sigmoid is there so right now not we are not just focusing on traditional rnn but there will be lot of changes if we are specifically using lstm rna now everybody understand why specifically we are using lstmrn two main important things first whenever you have long sentences right and suppose there is a dependency of the output probably with the first word right and i really want to predict the first word and in this sentences you know regularly context will be switching context will be switching right because whenever you have longer or longer sentences you know obviously content will be switching i hope everybody is able to see this yeah diagram or not it's okay if you are not able to see it what i will do just tell me guys whether you are able to see this diagram or not okay if you are not able to see don't worry i will make it bigger is it clear now yes yeah now i hope everybody's playing with this okay perfect right now see whenever we have this longer sentences let's say uh my sentence is something like this my name is krish my name is krish and my friend name is something dash i need to predict this okay i need to predict this specific output okay now over here as the as this rna first of all if you consider rnn okay uh you can see over here as this r and n with t is equal to 1 t is equal to 2 when it will start reading things and probably one of the context that you will find okay it is something related to me like krish is over here and my friend name is something so here the context will change again here when we come over here again the context should be related to crash and finally my output will basically be coming right so over here you can see that the context switching is happening a lot that basically means in order to find out this specific output i also need to be dependent on this i also need to be dependent on this i and this particular model also needs to be dependent on me right that is my name right so then only it will be able to predict who is my specific friend right so obviously you cannot do with rnn because here you know that if there is a long term dependencies obviously you will not be the this rna will just have a property of like short term memory and it will not be able to predict it so what are the changes that are actually happening in lstm so that it is being able to remember for a long term so we basically say long short term right so that is the reason we say lstm long short term memory okay now context switching basically means what see context switching is like krish likes pizza likes pizza but my friend likes let's say my friend likes burger okay now over here you can see that first of all it says about what is my like okay what is my like okay over here but when over here you can see the switching of the context is happening right now it is not talking about me it is talking about my friend so my lstm my rnn should now focus on my friend then only it will be able to predict this specific word i hope it is clear now regarding context switching so over here context basically means the sentence is getting changed over here nowhere krish is mentioned going ahead now friend is mentioned so my lstm rnn should make sure that it should forget about this information and should focus on the current info okay this kind of switch should actually happen you know and suppose if i am talking about myself and it should be able to also make sure that it is able to refer krish at a later stages okay now perfect now let's go ahead and let's try to understand i hope everybody understands this this is also a lstm neuron this is also an lstm neuron lstm this is this is my input at time t is minus 1 at t at t okay this is super super important now now the first thing that i really want to call about is that and we'll discuss this we'll break it down step by step and we'll try to understand it okay now first of all whenever you find this kind of image whenever you find this kind of image this kind of image with color over here you can see if it is probably with let's say if it is with this sigma that basically means this is a neural network neural network with sigmoid activation sigmoid activation clear suppose if you have this with tan h then it becomes neural network with tanh activation function tanning activation function i hope everybody is clear with it okay then whenever two lines whenever two lines are joining let's say two lines are joining like this okay so over here see you can see that two lines are joining this is basically called as concatenate okay we are concatenating two information okay whenever there is a split in two lines like this you can see one line like this and one line like this when there is a split like this this is basically called as copy okay copy operation and then wherever we have this specific arrow arrow arrow arrow wherever there is a kind of arrow like this this is nothing but it is called as vector transform okay vector transform everybody clear with this yes yes i hope sorry last one is t plus 1 okay fine t plus 1 i hope everybody is clear with this so i hope everybody understood the notation right till here now there are some notation which is given in circle okay this circle notation is basically called as pointwise operation okay so this will specifically be pointwise operation now pointwise operation can be addition it can be a dot operation it can be anything okay so i hope everybody is clear with this these are some of the basic notation that you should always refer to now think over it how probably lstm will make sure that you know the longer context information is also kept in mind and it should also have a focus point wherein it should be able to do the context switching okay so if i take an example krish likes pizza but his friends likes burger okay and i need to predict this specific word now what will happen let's say everybody at at a specific time first krish will go right then like will go then pizza will go then we start bud right after this now the context switching is happening that basically means now the focus is completely entirely on friend now do you think my neural network should remember this previous thing if this context switching is happening yes or no yes or no okay now you can see that over here the sentence completely the meaning of the sentence will change because now the main noun is his is krish friend i hope everybody is able to understand so what we will gonna do is that when we are training with our recurrent neural network after this specific sentence we should forget this information we should forget this information then our new context will basically give an importance to this specific words okay very much simple now how this will thing will forget okay so that is the reason first of all let's break down this layer by layer okay let's break down this entire neural network layer by layer this line that you see this line from the top that you see is called as memory cell memory cell and this will actually play the short-term memory cell okay this entire line that you see okay this this top line right whichever whichever you are basically being able to see the stop line is basically called as memory cell everybody remember this it is called as memory cell now what is the importance of memory cell see in airport in airport also you will be seeing some um let's say uh if i if i take this specific word i'm not able to remember this word yeah conveyor belt okay so in airport you'll be able to see right luggage will be continuously coming in one belt right like this like this luggage will be coming in one belt and some people can take out the luggage or they can also put down luggage right so the this memory cell acts like this conveyor belt okay conveyor belt basically means okay fine you can add luggage you can remove luggage okay similarly in this memory cell you can add info you can add info and you can remove info that basically means information everybody clear what is the super important thing regarding memory cell so in memory cell always remember memory cell two things you can do first of all you can add information you can add information and second one is that you can remove information okay remove information okay now when should you add information and when should you remove information that we'll talk about so this was regarding memory cell okay now the next thing is that let's discuss about this the second layer that you see over here is called as forget layer okay so this is basically called as forget layer okay forget layer now what is this forget layer or forget layer cell okay i can also put it as cell so in forget layer cell forget layer cell this is specifically used for one critical information okay okay now in this forget cell here you can see that when when this information is passed like krish like pizza but here the context switch is happening when the context switch happens like but his friend now we are talking about the friend right so as soon as the switch happens let's say at xt this friend is going then what will happen is that we have to make sure that our memory cell should forget about this entire info is it true or not so our memory cell should forget about this previous information right this forget cell should forget about i mean this memory cell should forget about this previous information okay now what happens over here let's say i am passing friend over here now as soon as i pass friend okay then since the context switch is happening we are going to pass this previous information along with this friend to this sigmoid layer and we are telling it forget the information so that the sigmoid layer will be trained in such a way that for most of the information here you will be getting zeros why why because sigmoid actually transforms your value between zero to one if the previous information and the current information that is getting passed are similar or having the same context then what will happen most of the values will be nearer to one or nearer to one okay if this two are not similar then what will happen most of the values will be nearer to zero okay now if it is nearer to zero that basically means what will happen we are making the memory cell forget this information because forward we have a point wise operation of cross okay i hope everybody is able to understand yes yes or no till here everybody clear or not right just understand what i'm actually trying to say okay so suppose if i let me take a dedicated uh neuron with respect to this okay and uh i will probably explain you with this okay so first thing first thing first okay let's go ahead if i really want to check out memory cell so this is what is a memory cell okay so this is an example of memory cell so first one i'm basically writing this as memory cell now why this is called as memory cell because over here you will be able to add info or remove info remove information okay how you'll be adding and removing the information i'll just tell in some time but just understand that this line that is coming this is called as memory cell okay everybody clear with memory cell at least yes okay everybody clear with memory cell at least right now i'll come to the second one okay i'll come to the second one so this i'll just make it little bit smaller okay so everybody clear with this yes yeah okay perfect now let's go to the second one forget second one it is called as forget gate okay forget gate you can basically say or you can also say it as a layer now forget get layer if i specifically talk about it you know i will be having a separate one and this is what is a forget gate let me just exit it and i will be using this diagram again so this is my forget gate okay so out of this entire information now what did i say why i'm specifically using forget get again the guys this entire uh video i have taken it from cola github blog it is super super important please make sure that you refer those blocks okay now over here let's say this forget gate is super super important why i'm saying is that because in this memory cell we need to add some info or remove some info but suppose if i have a sentence let's say krish lip like pizza like pizza but he does not like burger let's say he does not like burger over here you can see that there is hardly any context switch because this he is basically talking about krish only okay but up suppose if i have another sentence krish like pizza but his friend likes burger okay here you can see that context switch is happening here context switch is not happening now first case i will try to make you understand what will happen and in the second case i will try to make you understand what will happen so let's go with respect to the first case in the first case over here since the meaning of the sentence we since we are just talking about krish over here so what will happen is that from the previous state whatever info is coming and with the current state whatever information is coming since we are talking about the same person over here after we pass through this particular sigmoid activation function information will not be lost right so info will not be lost why why info will not be lost because over here no context switching is happening no the sentence is not at all changing right so over here what will happen most of this merging when happens and when it passes through the sigmoid your sigmoid output ranges between 0 to 1 so for most of the time what will happen it will be nearer to 1 if it is nearer to 1 as we go ahead there is a point wise operation over here in the memory cell so this multiplied by values near to 1 it will always get saved so that basically means the information is not getting removed because the sentence is talking about krish only okay now what about in the second case in the second case the sentence is entirely switching it is saying that krish likes pizza but his friends like burger so when it comes over here and when we talk about friend that basically means at this particular instance when friend is passed the previous context needs to be removed the previous context needs to be forgotten right so what will happen now when we pass this information to the sigmoid along with this neural network then most of the values will be nearer to one sorry nearer to zero right and when it is nearer to 0 when this previous information from the cell will get multiplied or point wise operation will happen over here then what will happen automatically this will be forgotten right so this previous information will be forgotten because why because if we do a point wise operation let's say i have one one one zero uh one like this right if we do a point wise operation with the values like zero zero or near to zero then what will happen everything will get become 0 right everything will become 0 so when everything is going to become 0 or near to 0 that basically means your information is basically forgotten the previous information previous context information is clearly forgotten everybody clear so this gate is basically called as forget gate and over here you can basically see what all function is happening x of t and h t of i1 is getting concatenated along with this there will be another weight that will get applied which is like f f and this w of f is nothing but the combination of two weights this the one weight is over here and one weight is over here right so this w of f is nothing but let's say this is w one combination with this h uh w t minus one okay like w of t or w of t minus one because for x of t here you'll be having w of t and here you will be having w t minus one okay so this will get combined and this is represented by w of f and then you get a sigmoid activation function but before that you specifically add a bias okay so this was the information regarding which layer as i discussed right now forget gate layer okay forget get layer i hope everybody is clear right now let's go to the next layer which is basically this part this part which involves this part okay so let us talk about this and for that again i have to take out a screenshot and let us continue and let us discuss about this so here you have the next layer and in the next layer we basically have again two operation so here it is okay we are going to discuss about this so in the next layer we basically uh what we do what we do not do we'll try to discuss about this first is fine we have forgotten some information from the previous context right so let's see over here we are going to discuss about two things one is input layer and the other other other or i can basically write input gate layer input gate layer right now with respect to the sentence let's say i'm just going to consider the same sentence okay i'm going to consider this same sentence so that it will become easy for me to explain you over here okay so the question the the sentence was very simple krish like pizza but his friend like burger okay now you know with respect to this particular sentence this forget layer this forget layer is forgetting some information definitely the previous information is forgotten but as we go ahead we really need to store this information about the friend right this information i need to add it in the memory cell right now in order to add it in the memory cell what i am going to do the same information with the sigmoid activation function will get passed and tan h information whenever i pass through a neural network with tan h and in tannic you know that the values will be between minus one to plus one so whenever you get this and when you combine this both then the new context information that is there that will only get passed at this layer everybody clear i hope everybody is clear yes guys weights weights are simple right okay you do not understand the weights see what is the weights over here i explained you right regarding weights when x of t is passing then i'll be having a different weight w of t when ht minus 1 is passing i'll be having a different weight wt minus 1 so i can combine this both weight and call w of f right yes or no yes yes everyone yes yes or no i hope everybody understood y w t of 1 and w of t this is basically getting combined and then i apply it along with our ht of -1 and x of t okay very much clear now let's talk about this quickly now you may be thinking krish why exactly this thing is there see with this information when i'm passing through this information that basically means it will almost act like a forget layer right so this is a forget layer this is also a forget layer or i can also say it as an input layer but understand one things okay the operation that it is basically doing it is just like a forget layer right and this tan h will make sure that your output is between minus one to plus one so any new info any new info any new info like this this friend that is information that is going because of the context the recent context that we had right now this information should get saved in this memory cell yes or no everyone so this forget layer obviously will have the same values right so just imagine guys in this particular case what will be the output of this since the context is basically happening it will be almost towards zero right nearer to zero approximately near to zero from this output yes yes yes or no right and in case of tan h when we pass tan h don't you think we'll be getting between minus 1 to plus 1 so let's say minus 1 0 1 1 1 something like this we'll get right now when we do this concatenation opera sorry when we do this point wise operation between these two don't you think only the new information will get passed from here to here yes or no yes or no everyone yes yes guys tell me whether did you understand this or not no see if the context i'm saying it will not be exactly zero okay just understand okay it will not be exactly zero it will be approximately equal to zero see whatever information if there is if there is a context change right over here that is basically happening obviously the neural network need not understand the previous things so when i'm this is also similar like this only you know so when i'm passing through this don't you think most of the values will be zero and from tan h what we are doing we are getting other values which is between minus one to plus one and when we do this point wise operation between these two only the important information like this this friend information will only get passed and get merged with the above cell okay point wise operation will be concatenation or multiplication in input gate point wise operation will be concat point white operation is a dot product when there is a cross it is a dot product when there is a plus there will be a addition product addition okay vector addition i hope everybody is clear so this information will get passed so here in short this this part is called as a input layer input gated layer okay and here what we are doing is that we are trying to create a vector where in the recent information that you have that needs to get passed so that it can be added in the memory cell okay that is c of t very much clear right so i hope everybody understood this is my forget layer this is my input layer and this tannage layer is basically passing the new information okay so okay first get is for memory yes second gate is for what to remember where we need to what to forget you can basically say that this is my cell okay this is what all things we need to forget from the previous info and this entirely combined is what new information needs to be added okay everybody clear so two types of operation here you you can see this one you can see this one bfi is added and then for this tan h is added you are getting two output i hope this is very much clear because x t h t minus one both will get passed the new weights that is getting merged from both of them is w of i okay here you will be having a combination of wt and wt minus 1 which is represented by wfi and then this will further get passed with tan h and similar this kind of operation along with bias is getting added everybody clear till here see we are trying to understand with respect to what things are basically happening and if you are able to explain lhtm in this specific way it is quite amazing okay everybody clear yeah what is the use of third gate i'll just explain you about it you know okay now let's go to the next one i will just quickly draw another diagram for you all now we are going to discuss about this what is the next step see all the operation will happen and then what will happen over here this is a what kind of operation this is a pointwise right pointwise multiplication or dot product operation right dot product so can i say from this layer we will be able to understand what needs to be forgotten and one needs to be remembered right what needs to be remembered and what needs to be forgotted and then here the thing that we are adding is nothing but new information right so new information is getting added into this cell so then it changes from ct minus 1 to ct right over here forgetting is happening over here adding is happening adding info everybody clear simple yes yes yes yes i guess okay now let's go to the third one now third one is super important again because whatever things are basically getting saved right we have to combine all those things so third one okay so i'm just going to copy this i'm going to paste it over here the final layer which is called as output gated layer okay so this is basically called as output gated layer output gated layer now what is happening from this cell i will be having the recent information this is basically getting passed to tan h so when i pass it back to tan h or from this tan h you know along with this neural network so what will happen in this specific case if i pass it through ranh just tell me think over it what will happen will i be able to get my new operation new new values over here and with respect to the previous forgetting things this both will get combined and this will basically be my new output and this will also be passed as an output over here and this will then get passed to the next cell okay so what will happen from this memory cell right first of all this operation also it will be same right this is getting combined with tan h of ct 10h of ct basically means from the memory cell i'm just applying a tanh operation so i will be getting the recent context information and this and this both will get pointwise dot product it will happen in short here we are trying to combine both the information the previous forgotten information and the recent information and that is actually passed as an output over here to the next cell okay later on next cell will come like this and this similar thing will keep on going with respect to different different time stamp right now here you can definitely see that later on the weights will get updated based on back propagation but this is how uh you know you really need to understand what all things are actually happening so as we go till here you know finally the output will make sure that this context information is when we talk about this burger right this is basically referring to this friend but if we are uh again we are forgetting the data in third no we are not forgetting the data in third see this operation that we are doing right here also it is done here also it is done right it is not at all forgetting just imagine that whatever things we are actually getting over here forgetting and remembering is basically happening over here okay okay so in short over here what is happening all useless data is going useless data is removed and only important data is considered and forwarded to the next network okay yes all data minus useless plus spanish context is the third gate yes right so finally you can see from the memory cell take out the most entire entire information that you have in the memory cell apply tannach on top of it okay then you get the recent new context information and combine the previous information over here with the dot product and send it to the next layer okay so i hope you understood about this this was all about lstm recurrent neural network and now we'll try to do some kind of practical applications tomorrow okay and the next neural network that we are going to do is something called as gru you really need to understand that too which is super important all together right but i guess the session was quite amazing i hope you are able to understand different different things and again all the materials with uh okay all the materials with uh all the information will be there okay just a quick announcement guys tomorrow i will not be available tomorrow i think i have to skip that day because i have some important work to do okay so we will have the session on friday okay everyone so a quick reminder we'll have a session on friday just for a reminder yeah i'll cover again i'll cover everything this will be a continuous session that will be going on okay so yes uh tomorrow there will be no sessions so it will be on friday just a quick announcement for everyone hit like at least make it hundred and yes i will continue this session in the next session okay day con day seven content it is going to upload it a couple i gave it i gave the material today itself right today evening okay so it is available over there so hit like make it to 100 make sure that you subscribe krishnak hindi channel and yes we'll continue the session on friday thank you bye bye take care keep on rocking and please do make sure that you follow collab lstm blogs that you will be able to understand it and friday will have a practical session uh on lstm okay thank you bye bye take care thank you guys