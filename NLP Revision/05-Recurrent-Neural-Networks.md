# Chapter 5: Recurrent Neural Networks (RNN)

## ğŸ¯ Learning Objectives
- Understand why RNN is needed for sequence data
- Learn RNN architecture and feedback mechanism
- Master types of RNN (one-to-one, one-to-many, many-to-one, many-to-many)
- Understand forward propagation in RNN
- Know RNN applications and use cases
- Learn when to use RNN vs traditional ML

## ğŸ“š Key Concepts

### Why RNN?

#### Limitations of Traditional Methods

**Problem with ML/BoW/TF-IDF/Avg Word2Vec:**

```
Sentence 1: "dog bites man"
Sentence 2: "man bites dog"

Using Average Word2Vec:
- Both sentences have SAME vectors (order lost!)
- But meanings are COMPLETELY different!
```

**Key Issue**: **Word order** and **sequence information** is lost

#### Sequence-Dependent Applications

**Applications where sequence matters:**

**1. Chatbot / Question Answering**

```
Question: "What is the weather like outside?"

- "What" + "is" + "the" + "weather" â†’ Context builds up
- "like" + "outside" â†’ Completes meaning
- Sequence = Grammatically correct, meaningful question
```

**If word order changes:**
```
"Outside weather the is what like?" â†’ Nonsense!
```

**2. Language Translation**

```
English: "I love to eat pizza"
Hindi:   "à¤®à¥à¤à¥‡ à¤ªà¤¿à¤œà¥à¤œà¤¾ à¤–à¤¾à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ"

- Word order different in different languages
- Grammatical structure must be preserved
- Sequential translation needed
```

**3. Text Generation / Auto-Completion**

```
You type: "The food is"

Auto-suggestions:
- "The food is good"
- "The food is delicious"
- "The food is amazing"

â†’ Model predicts NEXT word based on SEQUENCE
```

**4. Sentiment Analysis**

```
Sentence 1: "The food is good"          â†’ Positive
Sentence 2: "The food is not good"      â†’ Negative

"not" changes entire meaning!
â†’ Sequence and context critical
```

#### Why Machine Learning Fails

**Machine Learning (BoW, TF-IDF, Avg Word2Vec):**

```
Input:  Sentence â†’ Vector (fixed size)
Output: Prediction

Problem:
- NO memory of previous words
- NO sequential processing
- Word order LOST
```

**Deep Learning (RNN):**

```
Input:  Wordâ‚ â†’ Wordâ‚‚ â†’ Wordâ‚ƒ â†’ Wordâ‚„ â†’ ...
Processing: Each word processed sequentially
Memory: Previous words remembered
Output: Context-aware prediction
```

**Key Difference**: RNN has **memory** and processes **sequentially**

### RNN Applications

**1. Chatbots**
- User asks question word-by-word
- Bot understands context from sequence
- Generates grammatically correct response

**2. Language Translation**
- English sentence â†’ Sequential processing â†’ Hindi output
- Word order preserved
- Grammar maintained

**3. Text Generation**
- Start with seed text: "Once upon a"
- Generate next word: "time"
- Continue: "there was a"
- Generate: "king"
- Result: "Once upon a time there was a king"

**4. Sentiment Analysis**
- Process "The food is not good" sequentially
- "not" negates "good"
- Final sentiment: Negative

**5. Speech Recognition**
- Audio â†’ Sequential phonemes â†’ Words â†’ Sentences
- Order critical for understanding

**6. Music Generation**
- Generate notes sequentially
- Maintain musical structure
- Create melodies

**7. Time Series Forecasting**
- Stock prices: Previous values â†’ Predict next
- Weather: Historical data â†’ Future prediction
- Sales: Past trends â†’ Future sales

### Basic RNN Architecture

#### Feedback Loop Concept

**Traditional Neural Network:**

```
Input â†’ [Hidden Layer] â†’ Output
```

**Recurrent Neural Network:**

```
Input â†’ [Hidden Layer] âŸ² â†’ Output
           â†‘__________|
         (Feedback)
```

**Key Feature**: Output is fed back to the same network

#### Basic RNN Diagram

```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  RNN    â”‚
xâ‚ â†’â”‚  Cell   â”‚â†’ outputâ‚
     â”‚         â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚ (feedback)
          â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  RNN    â”‚
xâ‚‚ â†’â”‚  Cell   â”‚â†’ outputâ‚‚
     â”‚         â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚ (feedback)
          â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  RNN    â”‚
xâ‚ƒ â†’â”‚  Cell   â”‚â†’ outputâ‚ƒ
     â”‚         â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Explanation:**
- Same RNN cell used repeatedly
- Each time step receives:
  - New input (xâ‚œ)
  - Previous output (outputâ‚œâ‚‹â‚)
- Output at time t depends on current input AND previous state

#### Unfolded RNN Architecture

**Folded View (Compact):**

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
x â†’â”‚  RNN   â”‚â†’ h
    â”‚  âŸ²    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Unfolded View (Time Steps):**

```
    â”Œâ”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”
xâ‚ â†’â”‚RNN â”‚â†’ hâ‚ â†’â”‚RNN â”‚â†’ hâ‚‚ â†’â”‚RNN â”‚â†’ hâ‚ƒ â†’â”‚RNN â”‚â†’ hâ‚„
    â””â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”˜
     t=1         t=2         t=3         t=4
```

**Key Points:**
- **Same weights** used at all time steps
- **Sequential processing**: t=1, then t=2, then t=3, ...
- **Hidden state** (h) carries information forward

### Types of RNN

#### 1. One-to-One RNN

**Structure:**

```
    â”Œâ”€â”€â”€â”€â”
x â†’â”‚RNN â”‚â†’ y
    â””â”€â”€â”€â”€â”˜

Input:  1 element
Output: 1 element
```

**Example: Image Classification**

```
Input:  Image (single input)
Output: Class label (single output)

Example:
Image of cat â†’ [RNN] â†’ "Cat"
```

**Note**: Not common for RNN (CNNs better for images)

#### 2. One-to-Many RNN

**Structure:**

```
    â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”
x â†’â”‚RNN â”‚â†’ â”‚RNN â”‚â†’ â”‚RNN â”‚â†’ â”‚RNN â”‚
    â””â”€â”¬â”€â”€â”˜   â””â”€â”¬â”€â”€â”˜   â””â”€â”¬â”€â”€â”˜   â””â”€â”¬â”€â”€â”˜
      â†“        â†“        â†“        â†“
      yâ‚       yâ‚‚       yâ‚ƒ       yâ‚„

Input:  1 element
Output: Multiple elements (sequence)
```

**Example 1: Music Generation**

```
Input:  Starting note (C)
Output: Sequence of notes (C â†’ D â†’ E â†’ F â†’ G)

Process:
1. Input "C" â†’ Generate "D"
2. Use "D" â†’ Generate "E"
3. Use "E" â†’ Generate "F"
4. Continue...
```

**Example 2: Image Captioning**

```
Input:  Image (single input)
Output: Caption (sequence of words)

Image of dog playing â†’ [RNN] â†’ "A" "dog" "is" "playing" "in" "park"
```

**Example 3: Text Generation**

```
Input:  Seed word ("Once")
Output: Generated sentence ("Once upon a time there was a king")
```

#### 3. Many-to-One RNN

**Structure:**

```
    â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”
xâ‚ â†’â”‚RNN â”‚â†’ â”‚RNN â”‚â†’ â”‚RNN â”‚â†’ â”‚RNN â”‚
    â””â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”˜   â””â”€â”¬â”€â”€â”˜
                                  â†“
                                  y

Input:  Multiple elements (sequence)
Output: 1 element
```

**Example 1: Sentiment Analysis**

```
Input:  "The" "food" "is" "very" "good"
         â†“     â†“      â†“     â†“      â†“
       [RNN]â†’[RNN]â†’[RNN]â†’[RNN]â†’[RNN]
                                  â†“
                             "Positive"

Process:
1. Read "The" â†’ Update hidden state
2. Read "food" â†’ Update hidden state
3. Read "is" â†’ Update hidden state
4. Read "very" â†’ Update hidden state
5. Read "good" â†’ Final prediction: Positive
```

**Example 2: Document Classification**

```
Input:  Document (sequence of words)
Output: Category (Sports, Politics, Entertainment)
```

**Example 3: Next Day Sales Prediction**

```
Input:  Sales history (Day 1, Day 2, ..., Day 30)
Output: Predicted sales for Day 31
```

#### 4. Many-to-Many RNN

**Structure (Same Length):**

```
    â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”
xâ‚ â†’â”‚RNN â”‚â†’ â”‚RNN â”‚â†’ â”‚RNN â”‚â†’ â”‚RNN â”‚
    â””â”€â”¬â”€â”€â”˜   â””â”€â”¬â”€â”€â”˜   â””â”€â”¬â”€â”€â”˜   â””â”€â”¬â”€â”€â”˜
      â†“        â†“        â†“        â†“
      yâ‚       yâ‚‚       yâ‚ƒ       yâ‚„

Input:  Multiple elements
Output: Multiple elements (same length)
```

**Example 1: Named Entity Recognition (NER)**

```
Input:  "John"  "lives" "in"   "Paris"
         â†“       â†“       â†“       â†“
       [RNN]   [RNN]   [RNN]   [RNN]
         â†“       â†“       â†“       â†“
Output: "PERSON" "O"    "O"    "LOCATION"
```

**Example 2: Video Classification (Frame-by-Frame)**

```
Input:  Frameâ‚ â†’ Frameâ‚‚ â†’ Frameâ‚ƒ â†’ Frameâ‚„
Output: Labelâ‚ â†’ Labelâ‚‚ â†’ Labelâ‚ƒ â†’ Labelâ‚„
```

**Structure (Different Length - Encoder-Decoder):**

```
Encoder:                    Decoder:
xâ‚ â†’ [RNN] â†’ [RNN] â†’ [RNN] â†’ [RNN] â†’ [RNN] â†’ yâ‚
xâ‚‚ â”€â”€â”€â”€â”€â”€â”€â†—          â†“         â†“
xâ‚ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†—         â†“
                               yâ‚‚
                               yâ‚ƒ
```

**Example: Language Translation**

```
Input:  "I" "love" "pizza" (English - 3 words)
         â†“    â†“      â†“
       [Encoder RNN processes all]
                â†“
       [Decoder RNN generates output]
                â†“
Output: "à¤®à¥à¤à¥‡" "à¤ªà¤¿à¤œà¥à¤œà¤¾" "à¤ªà¤¸à¤‚à¤¦" "à¤¹à¥ˆ" (Hindi - 4 words)
```

**Example: Question Answering**

```
Input:  Question (variable length)
         â†“
       [Encoder processes question]
         â†“
       [Decoder generates answer]
         â†“
Output: Answer (variable length)
```

### Forward Propagation in RNN

#### Notation

**Variables:**
- $x_t$ = Input at time step t
- $h_t$ = Hidden state at time step t
- $y_t$ = Output at time step t
- $W$ = Weight matrix for input
- $W_h$ = Weight matrix for hidden state
- $b$ = Bias term

#### Example: Sentiment Analysis (Many-to-One)

**Sentence**: "The food is very good"

**Step 1: Tokenization**

```
Words: ["The", "food", "is", "very", "good"]
Mapped: [xâ‚, xâ‚‚, xâ‚ƒ, xâ‚„, xâ‚…]
```

**Step 2: Convert to Vectors**

Using Word2Vec (assume 300 dimensions):
```
xâ‚ = Word2Vec("The")   â†’ [0.12, 0.45, ..., 0.89]  (300 dims)
xâ‚‚ = Word2Vec("food")  â†’ [0.34, 0.67, ..., 0.23]  (300 dims)
xâ‚ƒ = Word2Vec("is")    â†’ [0.56, 0.89, ..., 0.45]  (300 dims)
xâ‚„ = Word2Vec("very")  â†’ [0.78, 0.12, ..., 0.67]  (300 dims)
xâ‚… = Word2Vec("good")  â†’ [0.23, 0.56, ..., 0.12]  (300 dims)
```

**Step 3: RNN Architecture**

```
    â”Œâ”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”
xâ‚ â†’â”‚RNN â”‚â†’ hâ‚ â†’â”‚RNN â”‚â†’ hâ‚‚ â†’â”‚RNN â”‚â†’ hâ‚ƒ â†’â”‚RNN â”‚â†’ hâ‚„ â†’â”‚RNN â”‚â†’ hâ‚…
    â””â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”˜      â””â”€â”¬â”€â”€â”˜
    t=1         t=2         t=3         t=4          t=5
                                                       â†“
                                                    [Sigmoid/Softmax]
                                                       â†“
                                                   Å· (Positive/Negative)
```

#### Forward Propagation Equations

**Time Step t=1:**

$$h_1 = f(W \cdot x_1 + b)$$

Where:
- $W$ = Weight matrix for input (initialized randomly)
- $x_1$ = Input vector (300 dims)
- $b$ = Bias
- $f$ = Activation function (tanh or ReLU)

**Time Step t=2:**

$$h_2 = f(W \cdot x_2 + W_h \cdot h_1 + b)$$

Where:
- $W \cdot x_2$ = Current input contribution
- $W_h \cdot h_1$ = Previous hidden state contribution
- $W_h$ = Weight matrix for hidden state

**Time Step t=3:**

$$h_3 = f(W \cdot x_3 + W_h \cdot h_2 + b)$$

**General Formula (Time Step t):**

$$h_t = f(W \cdot x_t + W_h \cdot h_{t-1} + b)$$

**Final Output (Many-to-One):**

$$\hat{y} = \sigma(W_y \cdot h_5 + b_y)$$

Where:
- $\sigma$ = Sigmoid (binary) or Softmax (multi-class)
- $W_y$ = Output weight matrix
- $h_5$ = Final hidden state
- $b_y$ = Output bias

#### Detailed Example with Dimensions

**Setup:**
- Input dimension: 300 (Word2Vec)
- Hidden dimension: 128 (neurons in RNN cell)
- Output: Binary (Positive=1, Negative=0)

**Weight Matrices:**

$$W: (128 \times 300) \text{ - Input to hidden}$$

$$W_h: (128 \times 128) \text{ - Hidden to hidden}$$

$$W_y: (1 \times 128) \text{ - Hidden to output}$$

**Time Step 1:**

$$h_1 = \tanh(W \cdot x_1 + b)$$

$$h_1 = \tanh([128 \times 300] \cdot [300 \times 1] + [128 \times 1])$$

$$h_1: [128 \times 1] \text{ (hidden state vector)}$$

**Time Step 2:**

$$h_2 = \tanh(W \cdot x_2 + W_h \cdot h_1 + b)$$

$$h_2 = \tanh([128 \times 300] \cdot [300 \times 1] + [128 \times 128] \cdot [128 \times 1] + [128 \times 1])$$

$$h_2: [128 \times 1]$$

**Time Step 5 (Final):**

$$h_5 = \tanh(W \cdot x_5 + W_h \cdot h_4 + b)$$

**Output:**

$$\hat{y} = \sigma(W_y \cdot h_5 + b_y)$$

$$\hat{y} = \sigma([1 \times 128] \cdot [128 \times 1] + [1 \times 1])$$

$$\hat{y}: \text{Scalar value between 0 and 1}$$

**Interpretation:**
- $\hat{y} \geq 0.5$ â†’ Positive sentiment
- $\hat{y} < 0.5$ â†’ Negative sentiment

#### Complete Forward Propagation Flow

```
Input: "The food is very good"
       â†“
   [Word2Vec]
       â†“
   xâ‚, xâ‚‚, xâ‚ƒ, xâ‚„, xâ‚… (each 300 dims)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Time Step 1:                 â”‚
â”‚ hâ‚ = tanh(WÂ·xâ‚ + b)         â”‚
â”‚ â†’ Hidden state: 128 dims     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Time Step 2:                 â”‚
â”‚ hâ‚‚ = tanh(WÂ·xâ‚‚ + Wâ‚•Â·hâ‚ + b)â”‚
â”‚ â†’ Hidden state: 128 dims     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Time Step 3:                 â”‚
â”‚ hâ‚ƒ = tanh(WÂ·xâ‚ƒ + Wâ‚•Â·hâ‚‚ + b)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Time Step 4:                 â”‚
â”‚ hâ‚„ = tanh(WÂ·xâ‚„ + Wâ‚•Â·hâ‚ƒ + b)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Time Step 5:                 â”‚
â”‚ hâ‚… = tanh(WÂ·xâ‚… + Wâ‚•Â·hâ‚„ + b)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
   [Output Layer]
   Å· = Ïƒ(Wáµ§Â·hâ‚… + báµ§)
       â†“
   0.87 â†’ Positive!
```

### Backward Propagation in RNN

**Process:**

1. **Calculate Loss:**

$$L = -(y \log(\hat{y}) + (1-y) \log(1-\hat{y}))$$

2. **Compute Gradients:**

$$\frac{\partial L}{\partial W_y}, \frac{\partial L}{\partial W_h}, \frac{\partial L}{\partial W}$$

3. **Backpropagate Through Time (BPTT):**
- Gradient flows backward: t=5 â†’ t=4 â†’ t=3 â†’ t=2 â†’ t=1
- Update weights at each time step

4. **Update Weights:**

$$W_{new} = W_{old} - \alpha \frac{\partial L}{\partial W}$$

Where $\alpha$ = Learning rate

**Key Issue**: Vanishing/Exploding Gradients (covered in LSTM chapter)

### Key Features of RNN

#### 1. Shared Weights

**Same weights** used across all time steps:
- $W$ (input weights)
- $W_h$ (hidden weights)
- $W_y$ (output weights)

**Advantage**: Fewer parameters, generalizes better

#### 2. Sequential Processing

**Order matters**:
```
"dog bites man" â‰  "man bites dog"

RNN processes:
t=1: "dog"  â†’ hâ‚
t=2: "bites" â†’ hâ‚‚ (remembers "dog")
t=3: "man"   â†’ hâ‚ƒ (remembers "dog bites")
```

#### 3. Memory (Hidden State)

**Hidden state carries information:**

$$h_t = f(x_t, h_{t-1})$$

- $h_t$ depends on current input AND previous state
- Information propagates forward through time

#### 4. Variable Length Input/Output

**Flexible architectures:**
- One-to-Many: 1 input â†’ N outputs
- Many-to-One: N inputs â†’ 1 output
- Many-to-Many: N inputs â†’ M outputs (N â‰  M possible)

## Advantages and Disadvantages

### Advantages

**1. Sequential Processing**
- Captures word order and context
- Understands sentence structure

**2. Memory Capability**
- Remembers previous inputs
- Hidden state carries information forward

**3. Variable Length Handling**
- Works with different input sizes
- No need for fixed-size padding (in theory)

**4. Shared Weights**
- Same weights across time steps
- Fewer parameters than separate networks

**5. Temporal Dependencies**
- Captures relationships over time
- Ideal for time series, text, speech

### Disadvantages

**1. Vanishing Gradient Problem**
- Gradients become very small over long sequences
- Difficult to learn long-term dependencies
- Solution: LSTM, GRU

**2. Exploding Gradient Problem**
- Gradients become very large
- Training becomes unstable
- Solution: Gradient clipping

**3. Sequential Processing (Slow)**
- Cannot parallelize across time steps
- Must process t=1, then t=2, then t=3...
- Slow for long sequences

**4. Short-Term Memory**
- Forgets information from early time steps
- Struggles with long sequences (> 100 tokens)
- Solution: LSTM, GRU, Attention

**5. Computational Cost**
- More expensive than feedforward networks
- Requires backpropagation through time (BPTT)

## â“ Interview Questions & Answers

**Q1: What is RNN and why is it needed?**

RNN (Recurrent Neural Network) is a neural network that processes sequential data by maintaining a hidden state that carries information across time steps. Needed because:
- Traditional ML loses word order ("dog bites man" = "man bites dog")
- Sequence matters in text, speech, time series
- RNN has memory and processes sequentially

**Q2: What is the key difference between RNN and feedforward neural networks?**

**Feedforward Network:**
- Processes entire input at once
- No memory of previous inputs
- Fixed input size

**RNN:**
- Processes input sequentially (one element at a time)
- Maintains hidden state (memory)
- Variable input size
- Feedback loop (output fed back to network)

**Q3: What are the types of RNN architectures?**

1. **One-to-One**: 1 input â†’ 1 output (Image classification)
2. **One-to-Many**: 1 input â†’ N outputs (Music generation, image captioning)
3. **Many-to-One**: N inputs â†’ 1 output (Sentiment analysis, document classification)
4. **Many-to-Many**: N inputs â†’ M outputs (Language translation, NER)

**Q4: What is the forward propagation equation in RNN?**

$$h_t = f(W \cdot x_t + W_h \cdot h_{t-1} + b)$$

Where:
- $h_t$ = Hidden state at time t
- $x_t$ = Input at time t
- $h_{t-1}$ = Previous hidden state
- $W$ = Input weight matrix
- $W_h$ = Hidden state weight matrix
- $f$ = Activation function (tanh or ReLU)

**Q5: Why does RNN have shared weights?**

Same weights ($W$, $W_h$, $W_y$) used across all time steps because:
- Reduces number of parameters
- Generalizes better (learns patterns, not specific positions)
- Makes network scalable to any sequence length

**Q6: What is the hidden state in RNN?**

Hidden state ($h_t$) is the memory of the RNN. It:
- Carries information from previous time steps
- Updated at each time step
- Combines current input with previous state
- Propagates information forward through time

**Q7: Give examples of Many-to-One RNN applications.**

1. **Sentiment Analysis**: Sentence (many words) â†’ Sentiment (positive/negative)
2. **Document Classification**: Document (many words) â†’ Category (sports/politics)
3. **Time Series Prediction**: Historical prices (many) â†’ Next day price (one)
4. **Video Classification**: Video frames (many) â†’ Category (one)

**Q8: Give examples of One-to-Many RNN applications.**

1. **Music Generation**: Starting note â†’ Sequence of notes
2. **Image Captioning**: Image â†’ Caption (sequence of words)
3. **Text Generation**: Seed word â†’ Generated paragraph
4. **Auto-completion**: Partial text â†’ Completed sentence

**Q9: What is the vanishing gradient problem in RNN?**

During backpropagation through time (BPTT):
- Gradients multiplied repeatedly over many time steps
- Gradients become exponentially small
- Early time steps get almost zero gradient
- Network cannot learn long-term dependencies

**Solution**: LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit)

**Q10: What is backpropagation through time (BPTT)?**

Backpropagation through time is the process of computing gradients in RNN:
1. Unfold RNN across all time steps
2. Calculate loss at output
3. Backpropagate gradients: t=T â†’ t=T-1 â†’ ... â†’ t=1
4. Update shared weights using accumulated gradients

**Q11: Why can't we parallelize RNN training?**

Because RNN processes sequentially:
- t=1 must complete before t=2 can start
- t=2 needs hidden state from t=1
- Cannot compute all time steps simultaneously
- This makes RNN slower than Transformers (which can parallelize)

**Q12: When should you use RNN vs Average Word2Vec?**

**Use Average Word2Vec:**
- Word order not critical
- Short sentences
- Fast inference needed
- Simple classification tasks

**Use RNN:**
- Word order critical ("not good" vs "good")
- Long sequences
- Context matters
- Complex tasks (translation, generation)

## ğŸ’¡ Key Takeaways

- **RNN** = Recurrent Neural Network with feedback loop
- **Purpose**: Process sequential data (text, speech, time series)
- **Key Feature**: Memory (hidden state) carries information forward
- **Feedback Loop**: Output fed back to same network
- **Sequential Processing**: t=1 â†’ t=2 â†’ t=3 â†’ ... â†’ t=T
- **Shared Weights**: Same W, W_h, W_y across all time steps
- **Types**: One-to-One, One-to-Many, Many-to-One, Many-to-Many
- **Forward Propagation**: $h_t = f(W \cdot x_t + W_h \cdot h_{t-1} + b)$
- **Applications**: Chatbots, translation, sentiment analysis, text generation
- **Advantages**: Sequence processing, memory, variable length
- **Disadvantages**: Vanishing gradients, slow training, short-term memory

## âš ï¸ Common Mistakes

**Mistake 1**: "RNN output at time t only depends on input at time t"
- **Reality**: Output at t depends on current input AND all previous inputs (through hidden state)

**Mistake 2**: "RNN has different weights at each time step"
- **Reality**: Same weights shared across ALL time steps

**Mistake 3**: "RNN can handle infinite sequence length"
- **Reality**: Vanishing gradients limit effective memory to ~10-20 time steps

**Mistake 4**: "Use RNN for all NLP tasks"
- **Reality**: Simple tasks (sentiment) can use simpler methods; complex tasks (translation) use LSTM/Transformers

**Mistake 5**: "RNN processes entire sentence at once"
- **Reality**: RNN processes one word at a time, sequentially

**Mistake 6**: "Hidden state is the output"
- **Reality**: Hidden state is internal memory; output comes from output layer applied to hidden state

## ğŸ“ Quick Revision Points

### RNN Architecture

**Basic Structure:**
```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
x â†’â”‚  RNN   â”‚â†’ h (hidden state)
    â”‚   âŸ²   â”‚â†’ y (output)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Unfolded:**
```
xâ‚â†’[RNN]â†’hâ‚â†’[RNN]â†’hâ‚‚â†’[RNN]â†’hâ‚ƒâ†’[RNN]â†’hâ‚„
   t=1      t=2      t=3      t=4
```

### Types of RNN

| Type | Input | Output | Example |
|------|-------|--------|---------|
| **One-to-One** | 1 | 1 | Image classification |
| **One-to-Many** | 1 | N | Music generation |
| **Many-to-One** | N | 1 | Sentiment analysis |
| **Many-to-Many** | N | M | Language translation |

### Forward Propagation Equations

**Time Step t:**

$$h_t = \tanh(W \cdot x_t + W_h \cdot h_{t-1} + b)$$

**Output (Many-to-One):**

$$\hat{y} = \sigma(W_y \cdot h_T + b_y)$$

**Output (Many-to-Many):**

$$\hat{y}_t = \sigma(W_y \cdot h_t + b_y)$$

### Weight Matrices

**For Hidden Dimension = 128, Input Dimension = 300:**

| Matrix | Dimension | Purpose |
|--------|-----------|---------|
| **W** | (128 Ã— 300) | Input to hidden |
| **W_h** | (128 Ã— 128) | Hidden to hidden |
| **W_y** | (1 Ã— 128) | Hidden to output (binary) |
| **b** | (128 Ã— 1) | Hidden bias |
| **b_y** | (1 Ã— 1) | Output bias |

### Applications

**One-to-Many:**
- Music generation
- Image captioning
- Text generation

**Many-to-One:**
- Sentiment analysis
- Document classification
- Stock price prediction

**Many-to-Many:**
- Language translation
- Named Entity Recognition (NER)
- Question answering

### Remember

- **RNN = Sequential processing + Memory**
- **Hidden state** = Memory that carries information
- **Shared weights** = Same W across all time steps
- **Forward**: $h_t = f(W \cdot x_t + W_h \cdot h_{t-1} + b)$
- **Vanishing gradient** = Main limitation of RNN
- **LSTM/GRU** = Better alternatives to vanilla RNN
- **Applications** = Text, speech, time series (sequential data)
- **Slower than Transformers** = Cannot parallelize
